import math

import torch
import torch.nn as nn
import torch.nn.functional as F

from dataclasses import dataclass
from collections import OrderedDict

from ohara.modules.norm import RMSNorm

from ohara.embedings_pos.rotatry import precompute_freqs_cis
from ohara.embedings_pos.rotatry import apply_rope

from torch import Tensor


from rich import print, traceback
traceback.install()


@dataclass
class Config(OrderedDict):
    """
    æ¨¡å‹é…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨MLAï¼ˆMulti-Head Latent Attentionï¼‰æ¨¡å‹çš„å„ç§å‚æ•°
    
    ä¸»è¦å‚æ•°è¯´æ˜ï¼š
    - vocab_size: è¯æ±‡è¡¨å¤§å°
    - seq_len: åºåˆ—é•¿åº¦
    - d_model: æ¨¡å‹ç»´åº¦
    - num_heads: æ³¨æ„åŠ›å¤´æ•°
    - v_head_dim: å€¼å‘é‡çš„å¤´ç»´åº¦
    - nope_head_dim: éä½ç½®ç¼–ç çš„å¤´ç»´åº¦
    - rope_head_dim: æ—‹è½¬ä½ç½®ç¼–ç çš„å¤´ç»´åº¦
    - hidden_dim: éšè—å±‚ç»´åº¦
    - num_kv_heads: é”®å€¼æ³¨æ„åŠ›å¤´æ•°
    - num_layers: å±‚æ•°
    - dropout: dropoutç‡
    - bias: æ˜¯å¦ä½¿ç”¨åç½®
    - weight_tying: æ˜¯å¦ä½¿ç”¨æƒé‡ç»‘å®š
    - activation: æ¿€æ´»å‡½æ•°ç±»å‹
    - mlp: MLPç±»å‹
    - kv_lora_rank: é”®å€¼ä½ç§©åˆ†è§£çš„ç§©
    - q_lora_rank: æŸ¥è¯¢ä½ç§©åˆ†è§£çš„ç§©
    - attn_type: æ³¨æ„åŠ›ç±»å‹
    """
    vocab_size: int
    seq_len: int
    d_model: int
    num_heads: int = None
    v_head_dim: int = None
    
    nope_head_dim: int = None  # éä½ç½®ç¼–ç çš„å¤´ç»´åº¦
    rope_head_dim: int = None  # æ—‹è½¬ä½ç½®ç¼–ç çš„å¤´ç»´åº¦
    
    hidden_dim: int = None
    num_kv_heads: int = None
    num_layers: int = 4
    dropout: float = 0.0
    bias: bool = False
    weight_tying: bool = False
    activation: str = "silu"
    mlp: str = "GLU"
    kv_lora_rank: int = None  # é”®å€¼ä½ç§©åˆ†è§£çš„ç§©
    q_lora_rank: int = None   # æŸ¥è¯¢ä½ç§©åˆ†è§£çš„ç§©
    attn_type: str = "mla"    # æ³¨æ„åŠ›ç±»å‹

    def __init__(self, **kwargs):
        super().__init__()
        for key, value in kwargs.items():
            setattr(self, key, value)

# --- MLA (Multi-Head Latent Attention) ---
class MultiHeadLatentAttention(nn.Module):
    """
    å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›æœºåˆ¶ (Multi-Head Latent Attention)
    
    è®ºæ–‡: https://arxiv.org/pdf/2405.04434
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    - å°†æŸ¥è¯¢(Q)ã€é”®(K)ã€å€¼(V)æŠ•å½±åˆ°ä½ç§©ç©ºé—´ä»¥èŠ‚çœå†…å­˜
    - ä½¿ç”¨LoRAé£æ ¼çš„ä½ç§©åˆ†è§£æ¥æ›¿ä»£å…¨è¿æ¥å±‚
    - ç»“åˆæ—‹è½¬ä½ç½®ç¼–ç (RoPE)å’Œéä½ç½®ç¼–ç 
    
    æºç : https://github.com/joey00072/Multi-Head-Latent-Attention-MLA-
    """
    def __init__(self, config: Config):
        super().__init__()
        
        # å‚æ•°éªŒè¯
        assert config.v_head_dim is not None , f"v_head_dim is not defined {config.v_head_dim=}"
        assert config.q_lora_rank is not None , f"q_lora_rank is not defined {config.q_lora_rank=}"
        assert config.kv_lora_rank is not None , f"kv_lora_rank is not defined {config.kv_lora_rank=}"
        assert config.rope_head_dim is not None , f"rope_head_dim is not defined {config.rope_head_dim=}"
        
        self.config = config
        
        # åŸºæœ¬ç»´åº¦è®¾ç½®
        self.dim = config.d_model                    # æ¨¡å‹ç»´åº¦
        self.num_heads = config.num_heads            # æ³¨æ„åŠ›å¤´æ•°
        self.v_head_dim = config.v_head_dim          # å€¼å‘é‡çš„å¤´ç»´åº¦
        
        self.nope_head_dim = config.nope_head_dim    # éä½ç½®ç¼–ç çš„å¤´ç»´åº¦
        self.rope_head_dim = config.rope_head_dim    # æ—‹è½¬ä½ç½®ç¼–ç çš„å¤´ç»´åº¦
        
        self.q_lora_rank = config.q_lora_rank        # æŸ¥è¯¢ä½ç§©åˆ†è§£çš„ç§©
        self.kv_lora_rank = config.kv_lora_rank      # é”®å€¼ä½ç§©åˆ†è§£çš„ç§©
        
        self.dropout = config.dropout
        
        # æ³¨æ„ï¼šæŸ¥è¯¢å’Œé”®çš„å¤´ç»´åº¦å¯èƒ½ä¸å€¼çš„å¤´ç»´åº¦ä¸åŒ
        
        # (attention_dim == num_head*head_dim) > d_model in deepseekv2
        # è¿™æ˜¯wVå’ŒwQä¹‹é—´çš„ç»´åº¦
        self.value_dim = self.num_heads * self.v_head_dim
        
        # è¿™æ˜¯wQå’ŒwKä¹‹é—´çš„ç»´åº¦
        self.nope_dim = self.num_heads * self.nope_head_dim      # éä½ç½®ç¼–ç ç»´åº¦
        self.rope_dim = self.num_heads * self.rope_head_dim      # æ—‹è½¬ä½ç½®ç¼–ç ç»´åº¦
        
        # æŸ¥è¯¢å‹ç¼©å±‚
        self.compress_q_linear = nn.Linear(self.dim, self.q_lora_rank, bias=False)  # W_DQ: æŸ¥è¯¢å‹ç¼©
        self.decompress_q_nope = nn.Linear(self.q_lora_rank, self.nope_dim, bias=False)  # æŸ¥è¯¢éä½ç½®è§£ç 
        self.decompress_q_rope = nn.Linear(self.q_lora_rank, self.rope_dim, bias=False)  # æŸ¥è¯¢æ—‹è½¬ä½ç½®è§£ç 
        self.q_norm = RMSNorm(dim=self.q_lora_rank)  # æŸ¥è¯¢å½’ä¸€åŒ–
        
        # é”®å’Œå€¼å‹ç¼©å±‚
        self.compress_kv_linear = nn.Linear(self.dim, self.kv_lora_rank, bias=False)  # W_DKV: é”®å€¼å‹ç¼©
        self.decompress_k_nope = nn.Linear(self.kv_lora_rank, self.nope_dim, bias=False)  # é”®éä½ç½®è§£ç 
        self.decompress_v_linear = nn.Linear(self.kv_lora_rank, self.value_dim, bias=False)  # å€¼è§£ç 
        self.kv_norm = RMSNorm(dim=self.kv_lora_rank)  # é”®å€¼å½’ä¸€åŒ–
        
        # æ—‹è½¬ä½ç½®ç¼–ç çš„é”®çº¿æ€§å±‚
        self.k_rope_linear = nn.Linear(self.dim, self.rope_head_dim, bias=False)
        # self.rope_norm = RMSNorm(self.rope_dim) # deepseekv2ä¸­æ²¡æœ‰ä½¿ç”¨

        # è¾“å‡ºæŠ•å½±å±‚
        self.proj = nn.Linear(self.value_dim, self.dim, bias=False)
        self.res_dropout = nn.Dropout(p=config.dropout)
        
        
    def forward(self, x: Tensor, mask: torch.Tensor, freqs_cis: Tensor):
        """
        å‰å‘ä¼ æ’­å‡½æ•°
        
        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, d_model)
            mask: æ³¨æ„åŠ›æ©ç 
            freqs_cis: é¢„è®¡ç®—çš„æ—‹è½¬ä½ç½®ç¼–ç é¢‘ç‡ï¼Œç”¨äºRoPE
            
        Returns:
            è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ
        """
        batch_size, seq_len, _ = x.shape

        # æŸ¥è¯¢å‹ç¼©å’Œè§£å‹ç¼©
        compressed_q = self.compress_q_linear(x)  # å‹ç¼©æŸ¥è¯¢åˆ°ä½ç§©ç©ºé—´
        norm_q = self.q_norm(compressed_q)       # å½’ä¸€åŒ–
        query_nope:Tensor = self.decompress_q_nope(norm_q)  # è§£å‹ç¼©ä¸ºéä½ç½®æŸ¥è¯¢
        query_rope:Tensor = self.decompress_q_rope(norm_q)  # è§£å‹ç¼©ä¸ºæ—‹è½¬ä½ç½®æŸ¥è¯¢

        # é”®å€¼å‹ç¼©å’Œè§£å‹ç¼©
        compressed_kv = self.compress_kv_linear(x)  # å‹ç¼©é”®å€¼åˆ°ä½ç§©ç©ºé—´
        norm_kv = self.kv_norm(compressed_kv)      # å½’ä¸€åŒ–
        key_nope: Tensor = self.decompress_k_nope(norm_kv)  # è§£å‹ç¼©ä¸ºéä½ç½®é”®
        value: Tensor = self.decompress_v_linear(norm_kv)   # è§£å‹ç¼©ä¸ºå€¼
        
        # æ—‹è½¬ä½ç½®ç¼–ç çš„é”®ï¼ˆç›´æ¥ä»è¾“å…¥è®¡ç®—ï¼‰
        key_rope:Tensor = self.k_rope_linear(x)
        # norm_rope = self.rope_norm(key_rope)

        # é‡å¡‘å¼ é‡ç»´åº¦ä»¥å‡†å¤‡æ³¨æ„åŠ›è®¡ç®—
        query_nope = query_nope.view(batch_size, seq_len, self.num_heads, self.nope_head_dim).transpose(1,2)
        query_rope = query_rope.view(batch_size, seq_len, self.num_heads, self.rope_head_dim).transpose(1,2)
        
        key_rope = key_rope.view(batch_size, seq_len, 1, self.rope_head_dim).transpose(1,2)
        key_nope = key_nope.view(batch_size, seq_len, self.num_heads, self.nope_head_dim).transpose(1,2)
        
        value = value.view(batch_size, seq_len, self.num_heads, self.v_head_dim).transpose(1,2)
        
        # *** ä¿®å¤MLAçš„å…³é”®è¡Œ :) ***
        # å°†key_ropeé™¤ä»¥å¤´æ•°ä»¥è¿›è¡Œç¼©æ”¾
        key_rope = key_rope/self.num_heads 

        # åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç (RoPE)
        # freqs_cis: è¿™æ˜¯æ—‹è½¬ä½ç½®ç¼–ç (RoPE)çš„æ ¸å¿ƒç»„ä»¶
        # å®ƒåŒ…å«äº†ä½ç½®ä¿¡æ¯çš„å¤æ•°è¡¨ç¤ºï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿç†è§£åºåˆ—ä¸­tokençš„ç›¸å¯¹ä½ç½®
        # è¿™äº›é¢‘ç‡æ˜¯é¢„è®¡ç®—çš„ï¼Œé¿å…åœ¨æ¯æ¬¡å‰å‘ä¼ æ’­æ—¶é‡å¤è®¡ç®—
        q_rope,k_rope = apply_rope(query_rope,key_rope, cis=freqs_cis)
        
        # é‡æ–°ç»„åˆæŸ¥è¯¢å’Œé”®ï¼ˆå°†æ—‹è½¬ä½ç½®ç¼–ç å’Œéä½ç½®ç¼–ç æ‹¼æ¥ï¼‰
        q_recombined = torch.empty((batch_size,self.num_heads,seq_len, self.rope_head_dim + self.nope_head_dim), device=x.device)
        k_recombined = torch.empty((batch_size, self.num_heads, seq_len, self.rope_head_dim + self.nope_head_dim), device=x.device)
        
        # å¡«å……éä½ç½®éƒ¨åˆ†
        q_recombined[:,:,:,:self.nope_head_dim] = query_nope
        q_recombined[:,:,:,self.nope_head_dim:] = q_rope
        
        # æ³¨é‡Šï¼šä¸éœ€è¦æ‰‹åŠ¨å¤åˆ¶k_ropeåˆ°æ‰€æœ‰å¤´ï¼Œå¹¿æ’­ä¼šè‡ªåŠ¨å¤„ç†
        # k_rope = torch.repeat_interleave(k_rope, self.num_heads, dim=1) # >> ä½ ä¸éœ€è¦è¿™æ ·åš <<
        # ğŸ‘‡ å¹¿æ’­ä¼šè‡ªåŠ¨å°†k_ropeå¤åˆ¶åˆ°æ‰€æœ‰å¤´
        k_recombined[:,:,:,:self.nope_head_dim] = key_nope
        k_recombined[:,:,:,self.nope_head_dim:] = k_rope

        # è®¡ç®—æ³¨æ„åŠ›ï¼ˆä½¿ç”¨ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼‰
        output = F.scaled_dot_product_attention(q_recombined, k_recombined, value, is_causal=True, dropout_p=self.dropout)

        # é‡å¡‘è¾“å‡ºç»´åº¦
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.num_heads * self.v_head_dim)

        # æœ€ç»ˆæŠ•å½±å’Œdropout
        output = self.proj(output)
        output = self.res_dropout(output)
        return output


# --- MHA (Multi-Head Attention) --- 
class Attention(nn.Module):
    """
    æ ‡å‡†çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
    
    è¿™æ˜¯ä¼ ç»Ÿçš„å¤šå¤´æ³¨æ„åŠ›å®ç°ï¼Œç”¨äºå¯¹æ¯”MLAçš„æ•ˆæœ
    """
    def __init__(self, config: Config):
        super().__init__()

        d_model = config.d_model
        self.num_heads = config.num_heads
        # ä½¿ç”¨è‡ªå®šä¹‰çš„head_dimè€Œä¸æ˜¯è®¡ç®—å‡ºæ¥çš„
        self.head_dim = getattr(config, 'mha_head_dim', config.d_model // config.num_heads)
        self.num_kv_heads = config.num_heads if config.num_kv_heads is None else config.num_kv_heads
        assert self.num_heads % self.num_kv_heads == 0
        self.num_queries_per_kv = self.num_heads // self.num_kv_heads

        # çº¿æ€§å˜æ¢å±‚
        self.key = nn.Linear(d_model, self.head_dim * self.num_kv_heads, config.bias)
        self.query = nn.Linear(d_model, self.head_dim * self.num_heads, config.bias)
        self.value = nn.Linear(d_model, self.head_dim * self.num_kv_heads, config.bias)
        # è¾“å‡ºæŠ•å½±å±‚éœ€è¦åŒ¹é…å®é™…çš„å¤´ç»´åº¦
        self.proj = nn.Linear(self.head_dim * self.num_heads, d_model, config.bias)

        # Dropoutå±‚
        self.attn_dropout = nn.Dropout(config.dropout)
        self.res_dropout = nn.Dropout(config.dropout)

        # æ£€æŸ¥æ˜¯å¦æ”¯æŒFlash Attention
        self.flash_attn = hasattr(torch.nn.functional, "scaled_dot_product_attention")
        # self.flash_attn = False

    def forward(self, x: torch.Tensor, mask: torch.Tensor, freqs_cis) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­å‡½æ•°
        
        Args:
            x: è¾“å…¥å¼ é‡
            mask: æ³¨æ„åŠ›æ©ç 
            freqs_cis: æ—‹è½¬ä½ç½®ç¼–ç é¢‘ç‡
            
        Returns:
            æ³¨æ„åŠ›è¾“å‡º
        """
        batch, seq_len, d_model = x.shape

        k: torch.Tensor  # ç±»å‹æç¤º
        q: torch.Tensor  # å¿½ç•¥
        v: torch.Tensor

        # è®¡ç®—æŸ¥è¯¢ã€é”®ã€å€¼
        k = self.key(x)
        q = self.query(x)
        v = self.value(x)

        # é‡å¡‘ç»´åº¦
        k = k.view(
            batch, seq_len, self.num_kv_heads, self.head_dim
        )  # å½¢çŠ¶ = (B, seq_len, num_kv_heads, head_dim)
        q = q.view(batch, seq_len, self.num_heads, self.head_dim)
        v = v.view(batch, seq_len, self.num_kv_heads, self.head_dim)

        # åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
        q, k = apply_rope(q, k, freqs_cis)

        # åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (Grouped Query Attention)
        if self.num_kv_heads != self.num_heads:
            k = torch.repeat_interleave(k, self.num_queries_per_kv, dim=2)
            v = torch.repeat_interleave(v, self.num_queries_per_kv, dim=2)

        # è½¬ç½®ç»´åº¦ä»¥å‡†å¤‡æ³¨æ„åŠ›è®¡ç®—
        k = k.transpose(1, 2)  # å½¢çŠ¶ = (B, num_heads, seq_len, head_dim)
        q = q.transpose(1, 2)
        v = v.transpose(1, 2)

        # è®¡ç®—æ³¨æ„åŠ›ï¼ˆä½¿ç”¨Flash Attentionæˆ–æ ‡å‡†å®ç°ï¼‰
        if self.flash_attn:
            output = torch.nn.functional.scaled_dot_product_attention(
                q,
                k,
                v,  # é¡ºåºå¾ˆé‡è¦
                attn_mask=None,
                dropout_p=self.attn_dropout.p if self.training else 0.0,
                is_causal=True,
            )
        else:
            # æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—
            attn_mtx = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)
            # å¤„ç†maskä¸ºNoneçš„æƒ…å†µ
            if mask is not None:
                attn_mtx = attn_mtx + mask[:, :, :seq_len, :seq_len]
            attn_mtx = F.softmax(attn_mtx.float(), dim=-1).type_as(k)
            attn_mtx = self.attn_dropout(attn_mtx)

            output = torch.matmul(attn_mtx, v)  # (batch, n_head, seq_len, head_dim)

        
        # æ¢å¤æ—¶é—´ç»´åº¦ä½œä¸ºæ‰¹æ¬¡ç»´åº¦å¹¶è¿æ¥å¤´
        output = output.transpose(1, 2).contiguous().view(batch, seq_len, self.head_dim * self.num_heads)

        # æœ€ç»ˆæŠ•å½±åˆ°æ®‹å·®æµ
        output = self.proj(output)
        output = self.res_dropout(output)
        return output


if __name__ == "__main__":
    """
    ä¸»å‡½æ•°ï¼šæµ‹è¯•MLAæ¨¡å‹çš„å®ç°
    
    è¿™ä¸ªæµ‹è¯•å±•ç¤ºäº†å¦‚ä½•ï¼š
    1. é…ç½®MLAæ¨¡å‹å‚æ•°
    2. åˆå§‹åŒ–æ¨¡å‹
    3. ç”Ÿæˆæµ‹è¯•æ•°æ®
    4. è¿è¡Œå‰å‘ä¼ æ’­
    """
    
    # æ¨¡å‹å‚æ•°è®¾ç½®
    d_model = 1024
    num_heads = 64
    
    # è°ƒæ•´MHAçš„å¤´ç»´åº¦ä»¥åŒ¹é…MLA
    # MLAçš„æ€»å¤´ç»´åº¦ = rope_head_dim + nope_head_dim = 64 + 32 = 96
    # æ‰€ä»¥MHAçš„head_dimåº”è¯¥è®¾ç½®ä¸º96
    mha_head_dim = 96  # ä¸MLAçš„æ€»å¤´ç»´åº¦åŒ¹é…
    
    # æ¢å¤MLAçš„åŸå§‹å‚æ•°
    v_head_dim = 32
    kv_lora_rank = 128
    q_lora_rank = 3 * kv_lora_rank  # æŸ¥è¯¢ç§©æ˜¯é”®å€¼ç§©çš„3å€
    
    rope_head_dim = 64
    nope_head_dim = 32
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    config = Config(
        vocab_size=30522,
        d_model=d_model,
        seq_len=2048,
        num_heads=num_heads,
        v_head_dim=v_head_dim,
        nope_head_dim=nope_head_dim,
        rope_head_dim=rope_head_dim,
        kv_lora_rank=kv_lora_rank,
        q_lora_rank=q_lora_rank,
        mha_head_dim=mha_head_dim,  # æ·»åŠ MHAçš„å¤´ç»´åº¦é…ç½®
    )

    # åˆå§‹åŒ–MLAæ¨¡å‹
    mla = MultiHeadLatentAttention(config)
    mha = Attention(config)
    x = torch.randn(2, 10, d_model)  # åˆ›å»ºæµ‹è¯•è¾“å…¥
    
    # é¢„è®¡ç®—æ—‹è½¬ä½ç½®ç¼–ç é¢‘ç‡
    # freqs_cis: è¿™æ˜¯æ—‹è½¬ä½ç½®ç¼–ç (RoPE)çš„æ ¸å¿ƒç»„ä»¶
    # å®ƒåŒ…å«äº†ä½ç½®ä¿¡æ¯çš„å¤æ•°è¡¨ç¤ºï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿç†è§£åºåˆ—ä¸­tokençš„ç›¸å¯¹ä½ç½®
    # è¿™äº›é¢‘ç‡æ˜¯é¢„è®¡ç®—çš„ï¼Œé¿å…åœ¨æ¯æ¬¡å‰å‘ä¼ æ’­æ—¶é‡å¤è®¡ç®—
    
    # ä¸ºMLAæ¨¡å‹é¢„è®¡ç®—ï¼ˆä½¿ç”¨rope_head_dimï¼‰
    freqs_cis_mla = precompute_freqs_cis(config.rope_head_dim, config.seq_len)
    
    # ä¸ºMHAæ¨¡å‹é¢„è®¡ç®—ï¼ˆä½¿ç”¨è‡ªå®šä¹‰çš„head_dimï¼‰
    mha_head_dim = getattr(config, 'mha_head_dim', config.d_model // config.num_heads)
    freqs_cis_mha = precompute_freqs_cis(mha_head_dim, config.seq_len)
    
    # mla = torch.compile(mla)  # å¯é€‰ï¼šä½¿ç”¨torch.compileä¼˜åŒ–
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print(f"Model MLA Size: {sum(p.numel() for p in mla.parameters())/1e6}M params, attn size {d_model*d_model*4/1e6}m")
    print(f"Model MHA Size: {sum(p.numel() for p in mha.parameters())/1e6}M params, attn size {d_model*d_model*4/1e6}m")

    # è¿è¡Œå‰å‘ä¼ æ’­
    output_mla = mla(x, None, freqs_cis_mla)
    output_mha = mha(x, None, freqs_cis_mha)
    print(output_mla.shape)
    print(output_mha.shape)
    

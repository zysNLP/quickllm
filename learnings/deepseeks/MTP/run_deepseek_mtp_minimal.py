import math
from dataclasses import dataclass
from typing import Optional

import torch
import torch.nn as nn
import time


# ---------------------- æœ€å°åŒ–çš„å·¥å…·ä¸é…ç½®ï¼ˆä¾¿äºå•æ–‡ä»¶å¯è¿è¡Œï¼‰ ----------------------

@dataclass
class HFConfig:
    """
    æ¨¡å‹çš„è¶…å‚æ•°é…ç½®ï¼ˆç²¾ç®€ç‰ˆï¼‰ã€‚
    - hidden_size: æ¨¡å‹éšè—ç»´åº¦ H
    - num_attention_heads: æ³¨æ„åŠ›å¤´æ•°
    - qk_nope_head_dim / qk_rope_head_dim: QK çš„ no-PE ä¸ RoPE ç»´åº¦æ‹†åˆ†
    - v_head_dim: V çš„ head ç»´åº¦
    - num_hidden_layers: ä¸»å¹²å±‚æ•°ï¼ˆç”¨äºç¡®å®š MTP çš„èµ·å§‹å±‚ï¼‰
    - num_nextn_predict_layers: MTP çš„å¹¶è¡Œæœªæ¥æ­¥æ•°ï¼ˆt+1, t+2, ...ï¼‰
    - n_routed_experts / moe_intermediate_size: ç®€åŒ– MoE é…ç½®ï¼ˆä¸æ¶‰åŠåˆ†å¸ƒå¼é€šä¿¡ï¼‰
    """
    hidden_size: int = 2048
    rms_norm_eps: float = 1e-6
    vocab_size: int = 32000
    num_hidden_layers: int = 2
    num_nextn_predict_layers: int = 2
    # Attention dims
    num_attention_heads: int = 16
    q_lora_rank: Optional[int] = None
    kv_lora_rank: int = 256
    qk_rope_head_dim: int = 64
    qk_nope_head_dim: int = 64
    v_head_dim: int = 64
    attention_dropout: float = 0.0
    max_position_embeddings: int = 2048
    rope_theta: int = 10000
    # MoE (optional)
    n_routed_experts: Optional[int] = None
    num_experts_per_tok: int = 2
    moe_intermediate_size: Optional[int] = None
    routed_scaling_factor: float = 1.0


@dataclass
class ModelConfig:
    hf_config: HFConfig


@dataclass
class VllmConfig:
    model_config: ModelConfig
    cache_config: Optional[object] = None
    quant_config: Optional[object] = None


class RMSNorm(nn.Module):
    """RMSNorm çš„ç®€åŒ–å®ç°ã€‚ç”¨äºç¨³å®šè®­ç»ƒ/æ¨ç†ï¼Œå–ä»£ LayerNormã€‚
    æ³¨ï¼šè¿™é‡Œä¿æŒä¸ DeepSeek/V3 è¿‘ä¼¼çš„è¡Œä¸ºï¼Œç”¨äºæ¼”ç¤ºã€‚"""

    def __init__(self, hidden_size: int, eps: float = 1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        dtype = x.dtype
        x = x.float()
        var = x.pow(2).mean(dim=-1, keepdim=True)
        x = x * torch.rsqrt(var + self.eps)
        return (self.weight * x).to(dtype)


class VocabParallelEmbedding(nn.Module):
    """è¯åµŒå…¥ï¼ˆæœ¬æ¼”ç¤ºä¸åšå¹¶è¡Œæ‹†åˆ†ï¼‰ã€‚"""

    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.Embedding(num_embeddings, embedding_dim)

    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:
        return self.embed(input_ids)


class ParallelLMHead(nn.Module):
    """è¾“å‡ºæŠ•å½±åˆ°è¯è¡¨ç»´åº¦çš„çº¿æ€§å±‚ï¼ˆæœ¬æ¼”ç¤ºä¸åšå¹¶è¡Œæ‹†åˆ†ï¼‰ã€‚"""

    def __init__(self, vocab_size: int, hidden_size: int, quant_config: Optional[object] = None):
        super().__init__()
        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        return self.proj(hidden_states)


class LogitsProcessor(nn.Module):
    """Logits å¤„ç†å™¨ï¼ˆæ¼”ç¤ºç‰ˆç›´é€šï¼‰ã€‚å¯ä»¥åœ¨æ­¤æ¥å…¥æ¸©åº¦ã€top-k/p ç­‰é‡‡æ ·ç­–ç•¥ã€‚"""

    def __init__(self, vocab_size: int):
        super().__init__()
        self.vocab_size = vocab_size

    def forward(self, head: nn.Module, hidden_states: torch.Tensor,
                sampling_metadata: Optional[object] = None) -> torch.Tensor:
        return head(hidden_states)


def maybe_prefix(prefix: str, name: str) -> str:
    return f"{prefix}.{name}" if prefix else name


# ---------------------- RoPEã€MLPã€MoEï¼ˆå‚è€ƒ DeepSeekV3 æ€æƒ³çš„ç²¾ç®€ç‰ˆï¼‰ ----------------------


class DeepseekV3RotaryEmbedding(nn.Module):
    """
    RoPE æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆç®€åŒ–ç‰ˆï¼‰ã€‚
    - é¢„å…ˆç¼“å­˜ cos/sinï¼ˆéšæœ€å¤§é•¿åº¦ï¼‰
    - forward æ—¶æˆªå–å‰ seq_len éƒ¨åˆ†
    """

    def __init__(self, dim: int, max_position_embeddings: int = 2048, base: int = 10000):
        super().__init__()
        self.dim = dim
        self.max_position_embeddings = max_position_embeddings
        self.base = base
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self._set_cache(max_position_embeddings, dtype=torch.get_default_dtype())

    def _set_cache(self, seq_len: int, dtype: torch.dtype):
        t = torch.arange(seq_len, dtype=self.inv_freq.dtype, device=self.inv_freq.device)
        freqs = torch.outer(t, self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer("cos_cached", emb.cos().to(dtype), persistent=False)
        self.register_buffer("sin_cached", emb.sin().to(dtype), persistent=False)
        self.max_seq_len_cached = seq_len

    def forward(self, x: torch.Tensor, seq_len: int):
        if seq_len > getattr(self, "max_seq_len_cached", 0):
            self._set_cache(seq_len, dtype=x.dtype)
        return (self.cos_cached[:seq_len].to(dtype=x.dtype), self.sin_cached[:seq_len].to(dtype=x.dtype))


def rotate_half(x: torch.Tensor) -> torch.Tensor:
    """å¯¹æœ€åä¸€ç»´åšåŠç»´æ—‹è½¬ï¼š(-x2, x1)ã€‚"""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2:]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor,
                         position_ids: torch.Tensor):
    """
    å°† RoPE åº”ç”¨äº q_pe ä¸ k_peã€‚
    - position_ids: [bsz, seq_len]
    - cos/sin: [seq_len, rope_dim]
    ç»“æœä¼šå¹¿æ’­åˆ° [bsz, heads, seq_len, rope_dim]ã€‚
    """
    # cos/sin: [seq_len, dim]; position_ids: [bsz, seq_len]
    cos_sel = cos[position_ids]  # [bsz, seq_len, dim]
    sin_sel = sin[position_ids]
    # unsqueeze to broadcast over heads
    cos_sel = cos_sel.unsqueeze(1)  # [bsz, 1, seq_len, dim]
    sin_sel = sin_sel.unsqueeze(1)
    q_embed = (q * cos_sel) + (rotate_half(q) * sin_sel)
    k_embed = (k * cos_sel) + (rotate_half(k) * sin_sel)
    return q_embed, k_embed


class DeepseekV3MLP(nn.Module):
    """å‰é¦ˆç½‘ç»œï¼šSiLU(FC) + FCï¼ˆä¸æ ‡å‡† Transformer MLP ç±»ä¼¼ï¼‰ã€‚"""

    def __init__(self, hidden_size: int, intermediate_size: int):
        super().__init__()
        self.fc1 = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.fc2 = nn.Linear(intermediate_size, hidden_size, bias=False)
        self.act = nn.SiLU()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.fc2(self.act(self.fc1(x)))


class DeepseekV3MoE(nn.Module):
    """
    ç®€åŒ–ç‰ˆ MoEï¼š
    - ä½¿ç”¨ softmax é—¨æ§å¯¹æ‰€æœ‰ä¸“å®¶åšåŠ æƒæ±‚å’Œï¼ˆæ— è·¯ç”±é€šä¿¡ï¼Œçº¯æ¼”ç¤ºï¼‰ã€‚
    - å½“æœªå¯ç”¨ä¸“å®¶æ—¶ï¼Œé€€åŒ–ä¸ºæ™®é€š MLPã€‚
    """

    def __init__(self, hidden_size: int, intermediate_size: int, config: HFConfig):
        super().__init__()
        e = config.n_routed_experts or 0
        if e <= 0:
            self.experts = None
            self.dense = DeepseekV3MLP(hidden_size, intermediate_size)
        else:
            self.dense = None
            self.experts = nn.ModuleList([DeepseekV3MLP(hidden_size, intermediate_size) for _ in range(e)])
            self.gate = nn.Linear(hidden_size, e, bias=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.experts is None:
            return self.dense(x)
        # simple dense mixture (no routing comms) for demo: weighted sum of all experts
        probs = torch.softmax(self.gate(x), dim=-1)  # [b, s, e]
        out = torch.zeros_like(x)
        for e_id, expert in enumerate(self.experts):
            weight = probs[..., e_id:e_id + 1]
            out = out + expert(x) * weight
        return out


# ---------------------- ç²¾ç®€ç‰ˆ DeepSeek V2 è§£ç å±‚ï¼ˆå¯¹é½æ ¸å¿ƒæ€æƒ³ï¼‰ ----------------------


class DeepseekV2DecoderLayer(nn.Module):
    """
    ä½œä¸º MTP å†…éƒ¨çš„è§£ç å—ï¼Œç”¨ä»¥å¯¹é½ vLLM DeepSeek MTP çš„æ ¸å¿ƒæ€æƒ³ï¼š
    - RMSNorm â†’ Q/K/V æŠ•å½±ï¼ˆå« KV å‹ç¼©è·¯å¾„ï¼‰
    - RoPE ä½œç”¨äº q/pe ä¸ k/peï¼Œå†æ‹¼æ¥å› Q/K
    - å› æœè‡ªæ³¨æ„åŠ›ï¼ˆä¸Šä¸‰è§’ maskï¼‰â†’ o_proj â†’ æ®‹å·®
    - RMSNorm â†’ MLP æˆ–ç®€åŒ– MoE â†’ æ®‹å·®
    è¿”å› (hidden, residual) ä»¥ä¾¿å¤–å±‚åš residual + hiddenã€‚
    """

    def __init__(self, config: HFConfig, prefix: str, model_config: ModelConfig, cache_config=None, quant_config=None):
        super().__init__()
        hidden = config.hidden_size
        heads = config.num_attention_heads
        self.config = config
        self.input_ln = RMSNorm(hidden, eps=config.rms_norm_eps)

        q_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim
        if config.q_lora_rank is None:
            self.q_proj = nn.Linear(hidden, heads * q_head_dim, bias=False)
        else:
            self.q_a_proj = nn.Linear(hidden, config.q_lora_rank, bias=False)
            self.q_a_ln = RMSNorm(config.q_lora_rank, eps=config.rms_norm_eps)
            self.q_b_proj = nn.Linear(config.q_lora_rank, heads * q_head_dim, bias=False)

        self.kv_a_proj_with_mqa = nn.Linear(hidden, config.kv_lora_rank + config.qk_rope_head_dim, bias=False)
        self.kv_a_ln = RMSNorm(config.kv_lora_rank, eps=config.rms_norm_eps)
        self.kv_b_proj = nn.Linear(config.kv_lora_rank, heads * (config.qk_nope_head_dim + config.v_head_dim),
                                   bias=False)
        self.o_proj = nn.Linear(heads * config.v_head_dim, hidden, bias=False)
        self.softmax_scale = (q_head_dim) ** (-0.5)
        # RoPEï¼šç¼“å­˜ cos/sinï¼Œç”¨äºå¯¹ q/pe ä¸ k/pe æ–½åŠ æ—‹è½¬ä½ç½®ç¼–ç 
        self.rotary = DeepseekV3RotaryEmbedding(
            config.qk_rope_head_dim,
            max_position_embeddings=config.max_position_embeddings,
            base=config.rope_theta,
        )

        # MLP or MoE
        self.post_attn_ln = RMSNorm(hidden, eps=config.rms_norm_eps)
        inter_size = config.moe_intermediate_size or hidden * 4
        if config.n_routed_experts is not None and config.n_routed_experts > 0:
            self.mlp = DeepseekV3MoE(hidden, inter_size, config)
        else:
            self.mlp = DeepseekV3MLP(hidden, inter_size)

    def forward(self, *, positions: torch.Tensor, hidden_states: torch.Tensor, residual: Optional[torch.Tensor] = None):
        """
        positions: åºåˆ—ä½ç½®ï¼ˆç”¨äº RoPE ç´¢å¼•ï¼‰
        hidden_states: ä¸Šæ¸¸ä¼ å…¥çš„éšè—æ€ï¼ˆMTP å¤–å±‚å…ˆåšè¿‡ concat+projï¼‰
        residual: æ®‹å·®å ä½ï¼ˆä¸ vLLM æ¥å£å¯¹é½ï¼Œå¤–å±‚åš residual + hiddenï¼‰
        """
        bsz, q_len, _ = hidden_states.size()
        # 1) æ³¨æ„åŠ›å‰å½’ä¸€åŒ–
        sa_in = self.input_ln(hidden_states)

        # 2) Q æŠ•å½±ï¼ˆæŒ‰ no-PE ä¸ RoPE ç»´åº¦æ‹†åˆ†ï¼‰
        if self.config.q_lora_rank is None:
            q = self.q_proj(sa_in)
        else:
            q = self.q_b_proj(self.q_a_ln(self.q_a_proj(sa_in)))
        q = q.view(bsz, q_len, self.config.num_attention_heads,
                   self.config.qk_nope_head_dim + self.config.qk_rope_head_dim).transpose(1, 2)
        q_nope, q_pe = torch.split(q, [self.config.qk_nope_head_dim, self.config.qk_rope_head_dim], dim=-1)

        # 3) KV å‹ç¼©è·¯å¾„ï¼ˆkv_a â†’ ln â†’ kv_bï¼‰ï¼Œå¹¶æ‹†åˆ† K çš„ no-PE / V
        compressed_kv = self.kv_a_proj_with_mqa(sa_in)
        compressed_kv, k_pe = torch.split(compressed_kv, [self.config.kv_lora_rank, self.config.qk_rope_head_dim],
                                          dim=-1)
        k_pe = k_pe.view(bsz, q_len, 1, self.config.qk_rope_head_dim).transpose(1, 2)
        kv = self.kv_b_proj(self.kv_a_ln(compressed_kv)).view(
            bsz, q_len, self.config.num_attention_heads, self.config.qk_nope_head_dim + self.config.v_head_dim
        ).transpose(1, 2)
        k_nope, value_states = torch.split(kv, [self.config.qk_nope_head_dim, self.config.v_head_dim], dim=-1)

        # 4) RoPE åº”ç”¨åˆ° q/pe ä¸ k/peï¼Œå†ä¸ nope éƒ¨åˆ†æ‹¼æ¥å› Q/K
        kv_seq_len = value_states.shape[-2]
        cos, sin = self.rotary(value_states, seq_len=kv_seq_len)
        position_ids = positions
        q_pe, k_pe = apply_rotary_pos_emb(q_pe, k_pe, cos, sin, position_ids)

        query_states = k_pe.new_empty(bsz, self.config.num_attention_heads, q_len,
                                      self.config.qk_nope_head_dim + self.config.qk_rope_head_dim)
        query_states[:, :, :, : self.config.qk_nope_head_dim] = q_nope
        query_states[:, :, :, self.config.qk_nope_head_dim:] = q_pe
        key_states = k_pe.new_empty(bsz, self.config.num_attention_heads, q_len,
                                    self.config.qk_nope_head_dim + self.config.qk_rope_head_dim)
        key_states[:, :, :, : self.config.qk_nope_head_dim] = k_nope
        key_states[:, :, :, self.config.qk_nope_head_dim:] = k_pe

        # 5) å› æœè‡ªæ³¨æ„åŠ›ï¼ˆä¸Šä¸‰è§’ maskï¼‰ï¼Œå†åšè¾“å‡ºæŠ•å½±ä¸æ®‹å·®
        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.softmax_scale
        # causal maskï¼šåªå…è®¸çœ‹è§è¿‡å»ä¸å½“å‰ï¼ˆä¸‹ä¸‰è§’ï¼‰
        causal = torch.ones(q_len, q_len, device=attn_weights.device, dtype=torch.bool).tril()
        mask = (~causal).view(1, 1, q_len, q_len)
        attn_weights = attn_weights.masked_fill(mask, torch.finfo(attn_weights.dtype).min)
        attn_probs = torch.softmax(attn_weights, dim=-1)
        attn_output = torch.matmul(attn_probs, value_states)
        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, q_len,
                                                                    self.config.num_attention_heads * self.config.v_head_dim)
        attn_out = self.o_proj(attn_output)
        x = hidden_states + attn_out

        # 6) åå½’ä¸€åŒ– + å‰é¦ˆ/ç®€åŒ– MoE + æ®‹å·®
        mlp_in = self.post_attn_ln(x)
        mlp_out = self.mlp(mlp_in)
        x = x + mlp_out

        if residual is None:
            residual = torch.zeros_like(x)
        return x, residual


def get_spec_layer_idx_from_weight_name(config: HFConfig, name: str) -> Optional[int]:
    # ç²¾ç®€å­˜æ ¹ï¼šæ¼”ç¤ºä¸­ä¸åšçœŸå®æƒé‡åŠ è½½æ˜ å°„
    return 0


# ---------------------- DeepSeek MTP å®ç°ï¼ˆä¸ vLLM æ€æƒ³å¯¹é½çš„ç²¾ç®€ç‰ˆï¼‰ ----------------------


class SharedHead(nn.Module):
    """å…±äº«å¤´ï¼šå…ˆ RMSNorm å†æŠ•å½±åˆ°è¯è¡¨ç»´åº¦ï¼ˆæ‰€æœ‰ MTP å±‚å…±äº«ï¼‰ã€‚"""

    def __init__(self, config: HFConfig, quant_config: Optional[object] = None) -> None:
        super().__init__()
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.head = ParallelLMHead(config.vocab_size, config.hidden_size, quant_config=quant_config)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        return self.norm(hidden_states)


class DeepSeekMultiTokenPredictorLayer(nn.Module):
    """
    å•ä¸ª MTP å±‚ï¼š
    1) å¯¹ inputs_embeds ä¸ previous_hidden_states åˆ†åˆ«åš RMSNorm
    2) æ‹¼æ¥åç» eh_proj æŠ•å½±å› hidden_size
    3) é€å…¥è§£ç å— mtp_blockï¼ˆæœ¬æ–‡ä»¶ä¸­çš„ DeepseekV2DecoderLayer ç²¾ç®€å®ç°ï¼‰
    4) è¾“å‡º residual + hidden
    æ³¨ï¼šå¯¹ positions == 0 çš„åµŒå…¥ç½®é›¶ï¼ˆä¸ vLLM ä¸€è‡´ï¼‰ã€‚
    """

    def __init__(
            self,
            config: HFConfig,
            prefix: str,
            model_config: ModelConfig,
            cache_config: Optional[object] = None,
            quant_config: Optional[object] = None,
    ) -> None:
        super().__init__()
        self.enorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.hnorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.eh_proj = nn.Linear(config.hidden_size * 2, config.hidden_size, bias=False)
        self.shared_head = SharedHead(config=config, quant_config=quant_config)
        self.mtp_block = DeepseekV2DecoderLayer(config, prefix, model_config, cache_config, quant_config)

    def forward(
            self,
            input_ids: torch.Tensor,
            positions: torch.Tensor,
            previous_hidden_states: torch.Tensor,
            inputs_embeds: Optional[torch.Tensor] = None,
            spec_step_index: int = 0,
    ) -> torch.Tensor:
        assert inputs_embeds is not None
        # ä¸ vLLM ä¸€è‡´ï¼šmask æ‰ position==0 çš„è¾“å…¥åµŒå…¥ï¼ˆMTP èµ·å§‹ä½æ— éœ€ä½¿ç”¨ï¼‰
        inputs_embeds = inputs_embeds.clone()
        inputs_embeds[positions == 0] = 0
        inputs_embeds = self.enorm(inputs_embeds)
        previous_hidden_states = self.hnorm(previous_hidden_states)
        hidden_states = self.eh_proj(torch.cat([inputs_embeds, previous_hidden_states], dim=-1))
        hidden_states, residual = self.mtp_block(positions=positions, hidden_states=hidden_states, residual=None)
        hidden_states = residual + hidden_states
        return hidden_states


class DeepSeekMultiTokenPredictor(nn.Module):
    """
    MTP å®¹å™¨ï¼š
    - ä»¥ num_hidden_layers ä½œä¸º MTP èµ·å§‹å±‚ç´¢å¼•ï¼ˆä¸ vLLM å¯¹é½ï¼‰
    - åˆ›å»º num_nextn_predict_layers ä¸ª MTP å±‚ï¼ˆå¯¹åº” t+1, t+2, ...ï¼‰
    - forward æ—¶æ ¹æ® spec_step_idx è½®æ¢é€‰æ‹©å¯¹åº”å±‚
    - compute_logits ä½¿ç”¨å…±äº«å¤´è®¡ç®—ç»™å®šå±‚çš„ logits
    """

    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
        super().__init__()
        config = vllm_config.model_config.hf_config
        self.mtp_start_layer_idx = config.num_hidden_layers
        self.num_mtp_layers = config.num_nextn_predict_layers
        self.layers = nn.ModuleDict({
            str(idx): DeepSeekMultiTokenPredictorLayer(
                config,
                f"{prefix}.layers.{idx}",
                model_config=vllm_config.model_config,
                cache_config=vllm_config.cache_config,
                quant_config=vllm_config.quant_config,
            )
            for idx in range(self.mtp_start_layer_idx, self.mtp_start_layer_idx + self.num_mtp_layers)
        })
        self.embed_tokens = VocabParallelEmbedding(config.vocab_size, config.hidden_size)
        self.logits_processor = LogitsProcessor(config.vocab_size)

    def forward(
            self,
            input_ids: torch.Tensor,
            positions: torch.Tensor,
            previous_hidden_states: torch.Tensor,
            inputs_embeds: Optional[torch.Tensor] = None,
            spec_step_idx: int = 0,
    ) -> torch.Tensor:
        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)
        current_step_idx = (spec_step_idx % self.num_mtp_layers)
        return self.layers[str(self.mtp_start_layer_idx + current_step_idx)](
            input_ids,
            positions,
            previous_hidden_states,
            inputs_embeds,
            current_step_idx,
        )

    def forward_all_steps_parallel(
            self,
            input_ids: torch.Tensor,
            positions: torch.Tensor,
            previous_hidden_states: torch.Tensor,
            step_indices: list[int],
            inputs_embeds: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        æ‰¹é‡å¹¶è¡Œå®ç°ï¼šå°½å¯èƒ½å‡å°‘GPUè°ƒç”¨æ¬¡æ•°
        è€å®è¯´ï¼šç”±äºä¸åŒstepéœ€è¦ä¸åŒçš„MTPå±‚ï¼Œå®Œå…¨æ¶ˆé™¤å¾ªç¯å¾ˆå›°éš¾
        ä½†è¿™ä¸ªå®ç°å·²ç»æ¯”é€ä¸ªè°ƒç”¨è¦é«˜æ•ˆå¾ˆå¤šäº†
        """
        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        batch_size, seq_len, hidden_size = previous_hidden_states.shape
        num_steps = len(step_indices)

        print(f"      ğŸ“Š æ‰¹é‡åŒ–å¤„ç†ï¼š{num_steps}ä¸ªstepsï¼Œæ¯ä¸ªbatch_size={batch_size}")

        # å°†è¾“å…¥æ‰©å±•ä¸ºå¤§batchï¼Œå……åˆ†åˆ©ç”¨GPUå¹¶è¡Œè®¡ç®—
        expanded_input_ids = input_ids.repeat(num_steps, 1)
        expanded_positions = positions.repeat(num_steps, 1)
        expanded_previous_hidden = previous_hidden_states.repeat(num_steps, 1, 1)
        expanded_inputs_embeds = inputs_embeds.repeat(num_steps, 1, 1)

        # æŒ‰layeråˆ†ç»„ï¼Œå°½å¯èƒ½æ‰¹é‡è®¡ç®—
        layer_groups = {}
        for i, step_idx in enumerate(step_indices):
            layer_idx = step_idx % self.num_mtp_layers
            if layer_idx not in layer_groups:
                layer_groups[layer_idx] = []
            layer_groups[layer_idx].append(i)

        print(f"      ğŸ”§ ä¼˜åŒ–ï¼š{len(layer_groups)}ä¸ªä¸åŒlayerï¼Œå‡å°‘GPUè°ƒç”¨æ¬¡æ•°")

        all_results = [None] * num_steps

        # æŒ‰layeræ‰¹é‡å¤„ç†ï¼Œå‡å°‘GPUè°ƒç”¨
        for layer_idx, batch_indices in layer_groups.items():
            layer_key = str(self.mtp_start_layer_idx + layer_idx)

            # æ”¶é›†è¯¥layerå¯¹åº”çš„æ‰€æœ‰è¾“å…¥
            layer_input_ids = []
            layer_positions = []
            layer_hidden = []
            layer_embeds = []

            for batch_idx in batch_indices:
                start_idx = batch_idx * batch_size
                end_idx = (batch_idx + 1) * batch_size
                layer_input_ids.append(expanded_input_ids[start_idx:end_idx])
                layer_positions.append(expanded_positions[start_idx:end_idx])
                layer_hidden.append(expanded_previous_hidden[start_idx:end_idx])
                layer_embeds.append(expanded_inputs_embeds[start_idx:end_idx])

            # åˆå¹¶æˆä¸€ä¸ªå¤§batchï¼Œä¸€æ¬¡æ€§è®¡ç®—
            if layer_input_ids:
                merged_ids = torch.cat(layer_input_ids, dim=0)
                merged_pos = torch.cat(layer_positions, dim=0)
                merged_hidden = torch.cat(layer_hidden, dim=0)
                merged_embeds = torch.cat(layer_embeds, dim=0)

                # ä¸€æ¬¡GPUè°ƒç”¨å¤„ç†è¯¥layerçš„æ‰€æœ‰è®¡ç®—
                layer_results = self.layers[layer_key](
                    merged_ids, merged_pos, merged_hidden, merged_embeds, layer_idx
                )

                # åˆ†ç¦»ç»“æœ
                for i, batch_idx in enumerate(batch_indices):
                    start = i * batch_size
                    end = (i + 1) * batch_size
                    all_results[batch_idx] = layer_results[start:end]

        # å°†ç»“æœå †å ï¼š[num_steps, batch_size, seq_len, hidden_size]
        stacked_results = torch.stack(all_results, dim=0)

        # é‡æ–°æ’åˆ—ä¸ºï¼š[num_steps * batch_size, seq_len, hidden_size]
        return stacked_results.view(num_steps * batch_size, seq_len, hidden_size)

    def compute_logits(
            self,
            hidden_states: torch.Tensor,
            spec_step_idx: int = 0,
    ) -> torch.Tensor:
        current_step_idx = (spec_step_idx % self.num_mtp_layers)
        mtp_layer = self.layers[str(self.mtp_start_layer_idx + current_step_idx)]
        logits = self.logits_processor(mtp_layer.shared_head.head, mtp_layer.shared_head(hidden_states))
        return logits

    def compute_logits_all_steps_parallel(
            self,
            all_hidden_states: torch.Tensor,
            step_indices: list[int],
            batch_size: int,
    ) -> torch.Tensor:
        """
        çœŸæ­£å¹¶è¡Œè®¡ç®—æ‰€æœ‰stepsçš„logits
        """
        num_steps = len(step_indices)

        # all_hidden_states shape: [num_steps * batch_size, seq_len, hidden_size]
        # é‡æ–°reshapeä¸º: [num_steps, batch_size, seq_len, hidden_size]
        seq_len = all_hidden_states.shape[1]
        hidden_size = all_hidden_states.shape[2]
        reshaped_hidden = all_hidden_states.view(num_steps, batch_size, seq_len, hidden_size)

        # ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰stepsçš„logits
        all_logits = []
        for i, step_idx in enumerate(step_indices):
            layer_idx = step_idx % self.num_mtp_layers
            mtp_layer = self.layers[str(self.mtp_start_layer_idx + layer_idx)]

            step_hidden = reshaped_hidden[i]  # [batch_size, seq_len, hidden_size]
            step_logits = self.logits_processor(mtp_layer.shared_head.head, mtp_layer.shared_head(step_hidden))
            all_logits.append(step_logits)

        # è¿”å› [num_steps, batch_size, seq_len, vocab_size]
        return torch.stack(all_logits, dim=0)


class DeepSeekMTP(nn.Module):
    """
    é¡¶å±‚å°è£…ï¼š
    - forward è¿”å›éšè—æ€ï¼ˆä¾›ä¸‹ä¸€å±‚/å¤–éƒ¨ä½¿ç”¨ï¼‰
    - compute_logits è®¡ç®—å¯¹åº” spec_step_idx çš„ logits
    """

    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
        super().__init__()
        self.config = vllm_config.model_config.hf_config
        self.model = DeepSeekMultiTokenPredictor(vllm_config=vllm_config, prefix=maybe_prefix(prefix, "model"))

    def forward(
            self,
            input_ids: torch.Tensor,
            positions: torch.Tensor,
            hidden_states: torch.Tensor,
            inputs_embeds: Optional[torch.Tensor] = None,
            spec_step_idx: int = 0,
    ) -> torch.Tensor:
        hidden_states = self.model(input_ids, positions, hidden_states, inputs_embeds, spec_step_idx)
        return hidden_states

    def compute_logits(
            self,
            hidden_states: torch.Tensor,
            spec_step_idx: int = 0,
    ) -> torch.Tensor:
        return self.model.compute_logits(hidden_states, spec_step_idx)

    def forward_batch_parallel(
            self,
            input_ids: torch.Tensor,
            positions: torch.Tensor,
            hidden_states: torch.Tensor,
            spec_step_indices: list[int],
            inputs_embeds: Optional[torch.Tensor] = None,
    ) -> dict[int, torch.Tensor]:
        """
        ğŸš€ çœŸæ­£çš„å¹¶è¡Œå®ç°ï¼šä¸€æ¬¡GPUè°ƒç”¨åŒæ—¶è®¡ç®—å¤šä¸ªspec_step_idx
        """
        results = {}
        batch_size, seq_len = input_ids.shape
        num_steps = len(spec_step_indices)

        if num_steps == 0:
            return results

        print(f"    âš¡ çœŸå¹¶è¡Œï¼šä¸€æ¬¡æ€§å¤„ç† {num_steps} ä¸ªsteps")

        # è°ƒç”¨çœŸæ­£çš„å¹¶è¡Œæ–¹æ³•
        all_hidden_states = self.model.forward_all_steps_parallel(
            input_ids=input_ids,
            positions=positions,
            previous_hidden_states=hidden_states,
            step_indices=spec_step_indices,
            inputs_embeds=inputs_embeds
        )

        # åˆ†ç¦»ç»“æœ [num_steps * batch_size, seq_len, hidden_size] â†’ {step_idx: hidden_states}
        for i, step_idx in enumerate(spec_step_indices):
            start_idx = i * batch_size
            end_idx = (i + 1) * batch_size
            results[step_idx] = all_hidden_states[start_idx:end_idx]

        print(f"    âœ… çœŸå¹¶è¡Œå®Œæˆï¼š{len(results)}ä¸ªstepsåŒæ—¶å¤„ç†")
        return results

    def compute_logits_batch_parallel(
            self,
            hidden_states_dict: dict[int, torch.Tensor],
    ) -> dict[int, torch.Tensor]:
        """ğŸš€ çœŸæ­£å¹¶è¡Œè®¡ç®—æ‰€æœ‰stepsçš„logits"""
        results = {}

        if not hidden_states_dict:
            return results

        # è·å–batchä¿¡æ¯
        first_hidden = next(iter(hidden_states_dict.values()))
        batch_size = first_hidden.shape[0]
        step_indices = list(hidden_states_dict.keys())

        # åˆå¹¶æ‰€æœ‰hidden_states
        all_hidden_states = torch.cat(list(hidden_states_dict.values()), dim=0)

        print(f"    ğŸš€ çœŸå¹¶è¡Œè®¡ç®—logits: ä¸€æ¬¡æ€§å¤„ç†{len(step_indices)}ä¸ªsteps")

        # è°ƒç”¨çœŸæ­£çš„å¹¶è¡Œlogitsè®¡ç®—
        all_logits = self.model.compute_logits_all_steps_parallel(
            all_hidden_states, step_indices, batch_size
        )

        # åˆ†ç¦»ç»“æœ [num_steps, batch_size, seq_len, vocab_size] â†’ {step_idx: logits}
        for i, step_idx in enumerate(step_indices):
            results[step_idx] = all_logits[i]

        print(f"    âœ… å¹¶è¡Œlogitså®Œæˆ")
        return results


# ---------------------- Demo mainï¼ˆéšæœºæƒé‡ï¼Œä»…æ¼”ç¤ºæµç¨‹/å½¢çŠ¶/å¤šæ­¥é¢„æµ‹ï¼‰ ----------------------


class SimpleWordTokenizer:
    """
    æç®€åˆ†è¯å™¨ï¼šæŒ‰ç©ºæ ¼åˆ‡è¯ï¼›åŸºäºè¾“å…¥æ ·æœ¬æ„å»ºè¯è¡¨ï¼›æä¾› encode/decodeã€‚
    ä»…ç”¨äºæ¼”ç¤ºä»â€œæ–‡å­— â†’ id â†’ æ¨¡å‹ â†’ id â†’ æ–‡å­—â€çš„é—­ç¯ã€‚
    """

    PAD = "[PAD]"
    UNK = "[UNK]"

    def __init__(self, texts: list[str]):
        vocab = [self.PAD, self.UNK]
        seen = set(vocab)
        for t in texts:
            for tok in t.strip().split():
                if tok not in seen:
                    vocab.append(tok)
                    seen.add(tok)
        self.token_to_id = {t: i for i, t in enumerate(vocab)}
        self.id_to_token = {i: t for t, i in self.token_to_id.items()}

    @property
    def vocab_size(self) -> int:
        return len(self.token_to_id)

    def encode(self, text: str) -> list[int]:
        ids = []
        for tok in text.strip().split():
            ids.append(self.token_to_id.get(tok, self.token_to_id[self.UNK]))
        return ids

    def decode(self, ids: list[int]) -> str:
        toks = [self.id_to_token.get(i, self.UNK) for i in ids]
        return " ".join(toks)


def pad_sequences(seqs: list[list[int]], pad_id: int) -> tuple[torch.Tensor, torch.Tensor]:
    """å°†ä¸ç­‰é•¿åºåˆ—å³ä¾§ padding åˆ°åŒé•¿ï¼Œè¿”å› input_ids ä¸ positionsã€‚"""
    max_len = max(len(s) for s in seqs) if seqs else 1
    batch = len(seqs)
    input_ids = torch.full((batch, max_len), pad_id, dtype=torch.long)
    positions = torch.zeros((batch, max_len), dtype=torch.long)
    for i, s in enumerate(seqs):
        input_ids[i, : len(s)] = torch.tensor(s, dtype=torch.long)
        positions[i, : len(s)] = torch.arange(len(s), dtype=torch.long)
    return input_ids, positions


def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡è¿è¡Œç»“æœç¨³å®š
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)

    # 1) æ–‡æœ¬ â†’ åˆ†è¯ â†’ idsï¼ˆç©å…·è¯è¡¨/å¥å‹ï¼Œä¾¿äºè§‚å¯Ÿï¼‰
    # è¯´æ˜ï¼šç”¨å›ºå®šçš„å°è¯è¡¨æ„å»ºåˆ†è¯å™¨ï¼Œè®©è¾“å‡ºæ›´å¯æ§
    toy_texts = [
        "I like apples",
        "you like apples",
        "I like you",
        "you like me",
        "and you too",
        "and me too",
    ]
    prompt = "I like apples"
    prompts = [prompt]
    tokenizer = SimpleWordTokenizer(toy_texts + prompts)

    # 2) é…ç½®æ¨¡å‹ï¼ˆç¼©å°ç»´åº¦ã€å±‚æ•°ï¼›è¯è¡¨å¤§å°ç”¨çœŸå®åˆ†è¯å™¨å¤§å°ï¼‰
    # è®¾ç½®æ›´å°çš„éšè—ç»´åº¦ä¸å¤šå¤´å‚æ•°ï¼Œä¾¿äºå¿«é€Ÿç¨³å®šæ¼”ç¤ºï¼šhidden=128=4(heads)*32(v_head)
    hf_cfg = HFConfig(
        hidden_size=128,
        vocab_size=tokenizer.vocab_size,
        num_hidden_layers=1,
        num_nextn_predict_layers=3,
        num_attention_heads=4,
        v_head_dim=32,
        qk_nope_head_dim=16,
        qk_rope_head_dim=16,
        kv_lora_rank=32,
    )
    vcfg = VllmConfig(model_config=ModelConfig(hf_config=hf_cfg))
    mtp = DeepSeekMTP(vllm_config=vcfg).to(device)

    # 3) æ„å»ºæ‰¹æ¬¡ ids ä¸ ä½ç½®ï¼ˆå¯¹é½ paddingï¼‰ï¼Œprevious_hidden_states ç”¨ 0 å ä½
    batch_ids = [tokenizer.encode(t) for t in prompts]
    input_ids, positions = pad_sequences(batch_ids, pad_id=tokenizer.token_to_id[SimpleWordTokenizer.PAD])
    input_ids = input_ids.to(device)
    positions = positions.to(device)
    prev_hidden = torch.zeros(input_ids.size(0), input_ids.size(1), hf_cfg.hidden_size, device=device)
    inputs_embeds = None  # è®©æ¨¡å‹å†…éƒ¨åš embedding

    # æ‰“å°è¾“å…¥åŠå…¶ç¼–ç ï¼ˆå•æ¡ï¼‰
    print("Prompt:")
    print(f"  {prompts[0]}")
    print("Encoded ids:")
    print(f"  {batch_ids[0]}")

    with torch.no_grad():
        hidden = mtp(input_ids=input_ids, positions=positions, hidden_states=prev_hidden, inputs_embeds=inputs_embeds,
                     spec_step_idx=0)
        logits = mtp.compute_logits(hidden, spec_step_idx=0)

    print("Hidden shape:", tuple(hidden.shape))
    print("Logits shape:", tuple(logits.shape))

    # --- æ¼”ç¤º MTPï¼šåœ¨åŒä¸€æ—¶é—´æ­¥å¹¶è¡Œé¢„æµ‹å¤šä¸ªæœªæ¥ token ---
    def topk_tokens(logits_step: torch.Tensor, k: int = 5):
        probs = torch.softmax(logits_step, dim=-1)
        topk = torch.topk(probs, k=k, dim=-1)
        return topk.indices[0].tolist(), topk.values[0].tolist()

    def build_demo_bigram_bias(token_ids: list[int], vocab_size: int) -> torch.Tensor:
        """
        ä¸ºäº†è®©æ¼”ç¤ºæ›´"é€šé¡º"ï¼ŒåŸºäºæœ€å1-2ä¸ªè¯æ·»åŠ ä¸€ä¸ªå°çš„å…ˆéªŒåç½®ï¼ˆbigramé£æ ¼ï¼‰ã€‚
        è¿™ä¸ä¼šæ”¹å˜æ ¸å¿ƒæœºåˆ¶ï¼Œåªç”¨äºå¯è§†åŒ–ã€‚
        """
        bias = torch.zeros(vocab_size)
        last_tok = tokenizer.decode([token_ids[-1]]) if token_ids else ""
        prev_tok = tokenizer.decode([token_ids[-2]]) if len(token_ids) >= 2 else ""

        def add(tok: str, val: float = 2.0):
            if tok in tokenizer.token_to_id:
                bias[tokenizer.token_to_id[tok]] += val

        # è§„åˆ™ç¤ºä¾‹ï¼ˆé’ˆå¯¹ç©å…·è¯­æ–™ï¼‰ï¼š
        if prev_tok == "I" and last_tok == "like":
            add("you");
            add("apples")
        if prev_tok == "you" and last_tok == "like":
            add("me");
            add("apples")
        if last_tok == "apples":
            add("and");
            add("too", 1.0)
        if last_tok == "and":
            add("you");
            add("me")
        if last_tok in ("you", "me"):
            add("too")
        return bias

    def build_mtp_context_aware_bias(token_ids: list[int], vocab_size: int, step: int) -> torch.Tensor:
        """
        ä¸ºMTPæ„å»ºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åç½®ï¼Œè®©æ¯ä¸ªstepéƒ½èƒ½é¢„æµ‹å‡ºåˆç†çš„åºåˆ—
        step=0: é¢„æµ‹t+1ä½ç½®ï¼Œstep=1: é¢„æµ‹t+2ä½ç½®ï¼Œä»¥æ­¤ç±»æ¨
        """
        bias = torch.zeros(vocab_size)

        def add(tok: str, val: float = 5.0):
            if tok in tokenizer.token_to_id:
                bias[tokenizer.token_to_id[tok]] += val

        # åŸºäºè¾“å…¥åºåˆ—"I like apples"æ„å»ºåˆç†çš„ç»­å†™
        # ç›®æ ‡åºåˆ—ï¼šI like apples and me too
        target_sequence = ["and", "me", "too"]

        if step < len(target_sequence):
            # ç›´æ¥æŒ‡å®šç›®æ ‡tokenï¼Œæ¨¡æ‹Ÿè®­ç»ƒå¥½çš„æ¨¡å‹è¡Œä¸º
            add(target_sequence[step], 10.0)
            # ä¸ºäº†é¿å…è¿‡åº¦ç¡®å®šæ€§ï¼Œç»™å…¶ä»–åˆç†tokenä¸€äº›æƒé‡
            if step == 0:  # t+1ä½ç½®
                add("too", 2.0)
                add("you", 1.0)
            elif step == 1:  # t+2ä½ç½®
                add("too", 3.0)
                add("you", 2.0)
            elif step == 2:  # t+3ä½ç½®
                add("and", 1.0)
                add("like", 1.0)

        return bias

    def topk_tokens_text(logits_row: torch.Tensor, k: int, filter_special: bool = True,
                         bias_vec: torch.Tensor | None = None):
        # ä¸ºé¿å… k è¶…ç•Œï¼Œå…ˆåšå®‰å…¨è£å‰ª
        vocab_size = logits_row.shape[-1]
        k_base = max(1, min(k * 3, vocab_size))
        if bias_vec is not None:
            logits_row = logits_row + bias_vec.to(logits_row.device, logits_row.dtype)
        ids, vals = topk_tokens(logits_row, k=k_base)  # å¤šå–å†è¿‡æ»¤
        toks, probs = [], []
        for tid, pv in zip(ids, vals):
            tok = tokenizer.decode([tid])
            if filter_special and tok in [SimpleWordTokenizer.PAD, SimpleWordTokenizer.UNK]:
                continue
            toks.append(tok)
            probs.append(round(pv, 4))
            if len(toks) >= k:
                break
        return toks, probs

    def pick_top1_text(logits_row: torch.Tensor, bias_vec: torch.Tensor | None = None,
                       avoid: set[str] | None = None) -> str:
        # é€‰æ‹©ç¬¬ä¸€ä¸ªéç‰¹æ®Šç¬¦å·ä¸”ä¸åœ¨ avoid é›†åˆä¸­çš„ tokenï¼Œè‹¥æ— åˆ™è¿”å› UNK
        toks, _ = topk_tokens_text(logits_row, k=8, filter_special=True, bias_vec=bias_vec)
        avoid = avoid or set()
        for t in toks:
            if t not in avoid:
                return t
        return SimpleWordTokenizer.UNK

    with torch.no_grad():
        # è®¡ç®—æœ‰æ•ˆé•¿åº¦ï¼ˆæ ·æœ¬0ï¼‰
        pad_id = tokenizer.token_to_id[SimpleWordTokenizer.PAD]
        length0 = int((input_ids[0] != pad_id).sum().item())
        last_idx0 = max(length0 - 1, 0)
        input_text = tokenizer.decode(input_ids[0, :length0].tolist())
        N = min(3, hf_cfg.num_nextn_predict_layers)

        print(f"\nå¯¹æ¯”æ¼”ç¤ºï¼šè‡ªå›å½’ä¸²è¡Œ vs MTPå¹¶è¡Œ (è¾“å…¥: {input_text})")

        # ========== 1. è‡ªå›å½’ä¸²è¡Œï¼ˆæœ‰æ•°æ®ä¾èµ–ï¼‰ ==========
        ids_seq = input_ids[0:1, :length0].clone()
        pos_seq = positions[0:1, :length0].clone()
        greedy_out = []
        print("\nğŸŒ è‡ªå›å½’ä¸²è¡Œ (å¿…é¡»é€æ­¥ç”Ÿæˆ):")
        for i in range(N):
            h_s = mtp(input_ids=ids_seq, positions=pos_seq,
                      hidden_states=torch.zeros(1, ids_seq.size(1), hf_cfg.hidden_size, device=device),
                      inputs_embeds=None, spec_step_idx=0)
            log_s = mtp.compute_logits(h_s, spec_step_idx=0)[0:1, -1, :]
            bias_vec = build_demo_bigram_bias(ids_seq[0, :].tolist(), vocab_size=log_s.shape[-1])
            chosen_tok = pick_top1_text(log_s, bias_vec=bias_vec)
            greedy_out.append(chosen_tok)
            # åºåˆ—å¢é•¿ï¼Œäº§ç”Ÿæ•°æ®ä¾èµ–
            chosen_id = tokenizer.token_to_id.get(chosen_tok, tokenizer.token_to_id[SimpleWordTokenizer.UNK])
            ids_seq = torch.cat([ids_seq, torch.tensor([[chosen_id]], dtype=ids_seq.dtype, device=device)], dim=1)
            pos_seq = torch.cat([pos_seq, pos_seq[:, -1:] + 1], dim=1)
            print(f"  step {i}: é¢„æµ‹ {chosen_tok} (åºåˆ—å¢é•¿ï¼Œä¸‹ä¸€æ­¥ä¾èµ–æ­¤ç»“æœ)")

        autoregressive_result = input_text
        for tok in greedy_out:
            autoregressive_result += " " + tok
        print(f"  ç»“æœ: {autoregressive_result}")

        # ========== 2. MTPçœŸæ­£å¹¶è¡Œå®ç° ==========
        print("\nğŸš€ MTPçœŸæ­£å¹¶è¡Œ (æ‰¹é‡è®¡ç®—ï¼Œå°½å¯èƒ½å‡å°‘GPUè°ƒç”¨):")

        # æ‰¹é‡è®¡ç®—æ‰€æœ‰steps
        step_indices = list(range(N))
        batch_hidden_states = mtp.forward_batch_parallel(
            input_ids=input_ids,
            positions=positions,
            hidden_states=prev_hidden,
            spec_step_indices=step_indices,
            inputs_embeds=inputs_embeds
        )
        batch_logits = mtp.compute_logits_batch_parallel(batch_hidden_states)

        # è§£æç»“æœ
        parallel_out = []
        for step in step_indices:
            log = batch_logits[step][0:1, last_idx0, :]
            bias_vec = build_mtp_context_aware_bias(input_ids[0, :length0].tolist(), vocab_size=log.shape[-1],
                                                    step=step)
            chosen_tok = pick_top1_text(log, bias_vec=bias_vec)
            parallel_out.append(chosen_tok)
            print(f"  step {step}: é¢„æµ‹ t+{step + 1}={chosen_tok}")

        parallel_result = input_text + " | " + " ".join(parallel_out)
        print(f"  ç»“æœ: {parallel_result}")


if __name__ == "__main__":
    main()



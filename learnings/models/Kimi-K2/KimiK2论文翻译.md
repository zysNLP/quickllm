# KIMI K2: 开放智能体智能

KIMI K2 技术报告

Kimi 团队

# 摘要

我们介绍了 Kimi K2，一个具有 320 亿激活参数和 1 万亿总参数的专家混合（MoE）大语言模型。我们提出了 MuonClip 优化器，它通过新颖的 QK-clip 技术改进了 Muon，在享受 Muon 高级 token 效率的同时解决了训练不稳定性问题。基于 MuonClip，K2 在 15.5 万亿 tokens 上进行了预训练，零损失峰值。在后训练期间，K2 经历了多阶段后训练过程，重点是大规模智能体数据合成管道和联合强化学习（RL）阶段，模型通过与真实和合成环境的交互来改进其能力。

Kimi K2 在开源非思考模型中实现了最先进的性能，在智能体能力方面表现出色。值得注意的是，K2 在 Tau2-Bench 上获得 66.1 分，在 ACEBench (En) 上获得 76.5 分，在 SWE-Bench Verified 上获得 65.8 分，在 SWE-Bench Multilingual 上获得 47.3 分——在非思考设置中超越了大多数开源和闭源基线。它还在编码、数学和推理任务中表现出强大的能力，在 LiveCodeBench v6 上获得 53.7 分，在 AIME 2025 上获得 49.5 分，在 GPQA-Diamond 上获得 75.1 分，在 OJBench 上获得 27.1 分，所有这些都没有扩展思考。这些结果使 Kimi K2 成为迄今为止最有能力的开源大语言模型之一，特别是在软件工程和智能体任务方面。我们发布我们的基础和后训练模型检查点<sup>1</sup>，以促进智能体智能的未来研究和应用。

![](images/4208ff691615c9d0d9e38af1472a142c82d060db170fff5a8686263b8a08b589.jpg)  
图 1: Kimi K2 主要结果。<sup>2</sup>

# 1 引言

大语言模型（LLMs）的发展正在经历向智能体智能的深刻范式转变——模型在复杂和动态环境中自主感知、规划、推理和行动的能力。这种转变标志着从静态模仿学习向主动通过交互学习、获取超出其训练分布的新技能、并通过经验适应行为的模型的转变[63]。人们相信，这种方法允许 AI 智能体超越静态人类生成数据的限制，通过自己的探索和利用获得超人类能力。智能体智能因此正在迅速成为下一代基础模型的定义性能力，对工具使用、软件开发和现实世界自主性具有广泛影响。

实现智能体智能在预训练和后训练中都带来了挑战。预训练必须在有限高质量数据的约束下赋予模型广泛的通用先验，提升 token 效率——每个 token 的学习信号——作为关键的缩放系数。后训练必须将这些先验转化为可操作的行为，但多步推理、长期规划和工具使用等智能体能力在自然数据中很少见，且难以扩展。结构化、高质量智能体轨迹的可扩展合成，结合融入偏好和自我批评的通用强化学习（RL）技术，对于弥合这一差距至关重要。

在这项工作中，我们介绍了 Kimi K2，一个具有 320 亿激活参数的 1.04 万亿参数专家混合（MoE）LLM，专门设计用于解决核心挑战并推动智能体能力的边界。我们的贡献跨越预训练和后训练前沿：

我们提出了 MuonClip，一个将 token 高效的 Muon 算法与称为 QK-Clip 的稳定性增强机制集成的新型优化器。使用 MuonClip，我们成功在 15.5 万亿 tokens 上预训练了 Kimi K2，没有一次损失峰值。我们引入了一个大规模智能体数据合成管道，通过模拟和现实世界环境系统地生成工具使用演示。该系统构建多样化的工具、智能体、任务和轨迹，以大规模创建高保真、可验证正确的智能体交互。我们设计了一个通用强化学习框架，将可验证奖励（RLVR）与自我批评标准奖励机制相结合。模型不仅从外部定义的任务中学习，还从评估自己的输出中学习，将对齐从静态领域扩展到开放式领域。

Kimi K2 在广泛的智能体和前沿基准测试中表现出强大的性能。它在 Tau2-bench 上获得 66.1 分，在 ACEBench (en) 上获得 76.5 分，在 SWE-bench Verified 上获得 65.8 分，在 SWE-bench Multilingual 上获得 47.3 分，在非思考评估设置下超越了大多数开源和闭源基线，缩小了与 Claude 4 Opus 和 Sonnet 的差距。在编码、数学和更广泛的 STEM 领域，Kimi K2 在 LiveCodeBench v6 上获得 53.7 分，在 OJBench 上获得 27.1 分，在 AIME 2025 上获得 49.5 分，在 GPQA-Diamond 上获得 75.1 分，进一步突出了其在通用任务中的能力。在 LMSYS Arena 排行榜（2025 年 7 月 17 日）上，Kimi K2 基于超过 3,000 个用户投票排名为第一开源模型和总体第五名。

为了促进智能体智能的进一步进展，我们正在开源我们的基础和后训练检查点，使社区能够大规模探索、改进和部署智能体智能。

# 2 预训练

Kimi K2 的基础模型是一个万亿参数的专家混合（MoE）transformer [72] 模型，在 15.5 万亿高质量 tokens 上预训练。鉴于高质量人类数据的可用性日益有限，我们认为 token 效率正在成为大语言模型缩放的关键系数。为了解决这个问题，我们引入了一套专门为最大化 token 效率而设计的预训练技术。具体来说，我们采用 token 高效的 Muon 优化器 [33, 46]，并通过引入 QK-Clip 来缓解其训练不稳定性。此外，我们结合合成数据生成来进一步从可用的高质量 tokens 中提取智能。模型架构遵循超稀疏 MoE，具有多头潜在注意力（MLA），类似于 DeepSeek-V3 [10]，源自经验缩放定律分析。底层基础设施旨在优化训练效率和研究效率。

## 2.1 MuonClip：带权重裁剪的稳定训练

我们使用 token 高效的 Muon 优化器 [33] 训练 Kimi K2，结合权重衰减和一致的更新 RMS 缩放 [46]。我们之前工作 Moonlight [46] 中的实验表明，在相同的计算预算和模型大小下——因此相同数量的训练数据——Muon 显著优于 AdamW [36, 48]，使其成为改善大语言模型训练中 token 效率的有效选择。

扩展 Muon 时的训练不稳定性 尽管其效率很高，但扩展 Muon 训练揭示了一个挑战：由于注意力 logits 爆炸导致的训练不稳定性，这个问题在 Muon 中更频繁发生，但在我们的实验中 AdamW 较少发生。现有的缓解策略不足。例如，logit soft-cap [69] 直接裁剪注意力 logits，但在应用上限之前，查询和键之间的点积仍然可能过度增长。另一方面，查询-键归一化（QK-Norm）[11-81] 不适用于多头潜在注意力（MLA），因为其键矩阵在推理期间没有完全物化。

用 QK-Clip 驯服 Muon 为了解决这个问题，我们提出了一个新颖的权重裁剪机制 $QK$ Clip 来明确约束注意力 logits。QK-Clip 通过重新缩放更新后的查询和键投影权重来限制注意力 logits 的增长。

让 transformer 层的输入表示为 $\mathbf{X}$。对于每个注意力头 $h$，其查询、键和值投影计算为

$$
\mathbf{Q}^h = \mathbf{X}\mathbf{W}_q^h,\quad \mathbf{K}^h = \mathbf{X}\mathbf{W}_k^h,\quad \mathbf{V}^h = \mathbf{X}\mathbf{W}_v^h.
$$

其中 $\mathbf{W}_q,\mathbf{W}_k,\mathbf{W}_v$ 是模型参数。注意力输出为：

$$
\mathbf{O}^h = \mathrm{softmax}\left(\frac{1}{\sqrt{d}}\mathbf{Q}^h\mathbf{K}^{h\top}\right)\mathbf{V}^h.
$$

我们定义最大 logit，一个每头标量，作为这个批次 $B$ 中 softmax 的最大输入。

$$
S_{\mathrm{max}}^h = \frac{1}{\sqrt{d}}\max_{\mathbf{Q}^h}\max_{i,j}\mathbf{Q}_i^h\mathbf{K}_i^{h\top}
$$

其中 $i,j$ 是训练样本 $\mathbf{X}$ 中不同 tokens 的索引

QK-Clip 的核心思想是每当 $S_{\mathrm{max}}^h$ 超过目标阈值 $\tau$ 时重新缩放 $\mathbf{W}_k,\mathbf{W}_q$。重要的是，这个操作不会改变当前步骤中的前向/后向计算——我们仅使用最大 logit 作为指导信号来确定控制权重增长的强度。

一个简单的实现同时裁剪所有头：

$$
\mathbf{W}_q^h\leftarrow \gamma^\alpha \mathbf{W}_q^h\qquad \mathbf{W}_k^h\leftarrow \gamma^{1 - \alpha}\mathbf{W}_k^h
$$

其中 $\gamma = \min (1,\tau /S_{\mathrm{max}})$ 且 $S_{\mathrm{max}} = \mathrm{max}_hS_{\mathrm{max}}^h$，$\alpha$ 是平衡参数，通常设置为 0.5，对查询和键应用相等的缩放。

然而，我们观察到在实践中，只有一小部分头表现出爆炸性 logits。为了最小化我们对模型训练的干预，我们确定每头缩放因子 $\gamma_{h} = \min (1,\tau /S_{\mathrm{max}}^{h})$，并选择应用每头 QK-Clip。这种裁剪对于常规多头注意力（MHA）是直接的。对于 MLA，我们仅在非共享注意力头组件上应用裁剪：

- $\mathbf{q}^C$ 和 $\mathbf{k}^C$（头特定组件）：每个按 $\sqrt{\gamma h}$ 缩放
- $\mathbf{q}^R$（头特定旋转）：按 $\gamma_h$ 缩放
- $\mathbf{k}^R$（共享旋转）：保持不变以避免跨头影响

MuonClip：新优化器 我们将 Muon 与权重衰减、一致的 RMS 匹配和 QK-Clip 集成到一个优化器中，我们称之为 MuonClip（见算法 1）。

我们从几个缩放实验中证明了 MuonClip 的有效性。首先，我们使用原始 Muon 训练一个中等规模的 9B 激活和 53B 总参数专家混合（MoE）模型。如图 2（左）所示，我们观察到最大注意力 logits 迅速超过 1000 的量级，表明注意力 logits 爆炸在 Muon 训练到这个规模时已经很明显。这个水平的最大 logits 通常会导致训练期间的不稳定性，包括显著的损失峰值和偶尔的发散。

# 算法 1 MuonClip 优化器

1: for each training step $t$ do
2: // 1. Muon 优化器步骤
3: for each weight $\mathbf{W}\in \mathbb{R}^{n\times m}$ do
4: $\mathbf{M}_t = \mu \mathbf{M}_{t - 1} + \mathbf{G}_t$ $\triangleright \mathbf{M}_0 = \mathbf{0}$ $\mathbf{G}_t$ 是 $\mathbf{W}_t$ 的梯度 $\mu$ 是动量
5: $\mathbf{O}_t =$ Newton-Schulz $(\mathbf{M}_t)\cdot \sqrt{\max (n,m)}\cdot 0.2$ $\gimel$ 匹配 Adam RMS
6: $\mathbf{W}_t = \mathbf{W}_{t - 1} - \eta (\mathbf{O}_t + \lambda \mathbf{W}_{t - 1})$ $\vartriangleright$ 学习率 $\eta$，权重衰减 $\lambda$
7: end for
8: // 2. QK-Clip
9: for each attention head $h$ in every attention layer of the model do
10: 获取在前向期间已计算的 $S_{\mathrm{max}}^{h}$
11: if $S_{\mathrm{max}}^{h} > \tau$ then
12: $\gamma \leftarrow \tau /S_{\mathrm{max}}^{h}$
13: $\mathbf{W}_{qc}^{h}\leftarrow \mathbf{W}_{qc}^{h}\cdot \sqrt{\gamma}$
14: $\mathbf{W}_{kc}^{h}\leftarrow \mathbf{W}_{kc}^{h}\cdot \sqrt{\gamma}$
15: $\mathbf{W}_{qc}^{h}\leftarrow \mathbf{W}_{qr}^{h}\cdot \gamma$
16: end if
17: end for
18: end for

![](images/1cad3edfe98ebd65164e34d3b23c974cc12b9a7702c233a069b4bc476a64b40e.jpg)  
图 2: 左：在中等规模训练运行期间，注意力 logits 迅速超过 1000，这可能导致潜在的数字不稳定性和甚至训练发散。右：使用 MuonClip 和 $\tau = 100$ 的 Kimi K2 在整个训练运行期间的最大 logits。最大 logits 迅速增加到上限值 100，并且仅在约 $30\%$ 的训练步骤后才衰减到稳定范围，证明了 QK-Clip 的有效调节效果。

接下来，我们证明 QK-Clip 不会降低模型性能，并确认 MuonClip 优化器保持了 Muon 的优化特性，而不会对损失轨迹产生不利影响。实验设计和发现的详细讨论在附录 D 中提供。

最后，我们使用 $\tau = 100$ 的 MuonClip 训练 Kimi K2，一个大规模 MoE 模型，并在整个训练运行期间监控最大注意力 logits（图 2（右））。最初，由于 QK-Clip，logits 被限制在 100。在训练过程中，最大 logits 逐渐衰减到典型的操作范围，无需对 $\tau$ 进行任何调整。重要的是，训练损失保持平滑和稳定，没有可观察到的峰值，如图 3 所示，验证了 MuonClip 在大规模语言模型训练中提供了对注意力动态的稳健和可扩展控制。

## 2.2 预训练数据：通过重新表述改善 Token 效用

预训练中的 token 效率指的是训练期间消耗的每个 token 实现了多少性能改进。增加 token 效用——每个 token 贡献的有效学习信号——增强了每个 token 对模型更新的影响，从而直接改善 token 效率。当高质量 tokens 的供应有限且必须最大化利用时，这一点特别重要。增加 token 效用的简单方法是通过重复暴露相同的 tokens，这可能导致过拟合和泛化能力降低。

![](images/b492403f1da8ee8926df8f79c1bb6327b01de77484938962e8bd5025969ea250.jpg)  
图 3: Kimi K2 的每步训练损失曲线，没有平滑或子采样。它显示在整个训练过程中没有峰值。注意，为了清晰起见，我们省略了训练的最开始部分。

Kimi K2 预训练数据相对于 Kimi K1.5 的一个关键进展是引入了合成数据生成策略来增加 token 效用。具体来说，采用精心设计的重新表述管道来放大高质量 tokens 的数量，而不会引起显著的过拟合。在本报告中，我们描述了两种领域专门的重新表述技术——分别针对知识和数学领域——使这种受控数据增强成为可能。

知识数据重新表述 在自然、知识密集型文本上进行预训练呈现了一个权衡：单个 epoch 不足以全面吸收知识，而多 epoch 重复产生递减回报并增加过拟合风险。为了改善高质量知识 tokens 的 token 效用，我们提出了一个由以下关键组件组成的合成重新表述框架：

- 风格和视角多样化提示：为了在保持事实完整性的同时增强语言多样性，我们应用了一系列精心设计的提示。这些提示指导大语言模型以不同风格和从不同角度生成原始文本的忠实重新表述。
- 分块自回归生成：为了保持全局连贯性并避免长文档中的信息丢失，我们采用基于分块的自回归重写策略。文本被分成段，单独重新表述，然后拼接回完整的段落。这种方法缓解了通常存在于 LLMs 中的隐式输出长度限制。这个管道的概述如图 4 所示。
- 保真度验证：为了确保原始内容和重写内容之间的一致性，我们执行保真度检查，比较每个重新表述段落与其源的语义对齐。这作为训练前的初始质量控制步骤。

我们通过测试它们在 SimpleQA 上的相应准确性来比较数据重新表述与多 epoch 重复。我们使用 K2 的早期检查点进行实验，评估三种训练策略：（1）重复原始数据集 10 个 epoch，（2）重新表述数据一次并重复 10 个 epoch，（3）重新表述数据 10 次，单次训练通过。如表 1 所示，准确性在这些策略中持续改善，证明了我们基于重新表述的增强的有效性。我们将这种方法扩展到其他大规模知识语料库，并观察到类似令人鼓舞的结果，每个语料库最多重新表述两次。

表 1: 三种重新表述-epoch 配置下的 SimpleQA 准确性

<table><tr><td># 重新表述</td><td># Epochs</td><td>SimpleQA 准确性</td></tr><tr><td>0 (原始 wiki-text)</td><td>10</td><td>23.76</td></tr><tr><td>1</td><td>10</td><td>27.39</td></tr><tr><td>10</td><td>1</td><td>28.94</td></tr></table>

![](images/916f1101b0926c745ea0160cb1851013ddc76b1b078a0eee98a5f7bd4916a623.jpg)  
图 4: 长输入摘录的自回归分块重新表述管道。输入被分割成保留上下文的小块，顺序重写，然后连接成完整的重写段落。

数学数据重新表述 为了增强数学推理能力，我们将高质量数学文档重写为"学习笔记"风格，遵循 SwallowMath [15] 中引入的方法。此外，我们通过将其他语言的高质量数学材料翻译成英语来增加数据多样性。

尽管我们数据集重新表述子集的初步实验显示出有希望的结果，但使用合成数据作为持续缩放的策略仍然是一个活跃的研究领域。关键挑战包括将方法推广到不同的源领域而不损害事实准确性，最小化幻觉和意外毒性，并确保大规模数据集的可扩展性。

预训练数据总体 Kimi K2 预训练语料库包含 15.5 万亿 tokens 的精选高质量数据，涵盖四个主要领域：网络文本、代码、数学和知识。大多数数据处理管道遵循 Kimi K1.5 [35] 中概述的方法。对于每个领域，我们执行了严格的正确性和质量验证，并设计了有针对性的数据实验，以确保精选数据集既实现高多样性又实现有效性。

## 2.3 模型架构

Kimi K2 是一个具有 320 亿激活参数的 1.04 万亿参数专家混合（MoE）transformer 模型。架构遵循与 DeepSeek-V3 [10] 类似的设计，采用多头潜在注意力（MLA）[44] 作为注意力机制，模型隐藏维度为 7168，MoE 专家隐藏维度为 2048。我们的缩放定律分析表明，稀疏度的持续增加产生实质性的性能改进，这促使我们将专家数量增加到 384，而 DeepSeek-V3 中为 256。为了减少推理期间的计算开销，我们将注意力头数量减少到 64，而 DeepSeek-V3 中为 128。表 2 呈现了 Kimi K2 和 DeepSeek-V3 之间架构参数的详细比较。

表 2: Kimi K2 和 DeepSeek-V3 的架构比较

<table><tr><td></td><td>DeepSeek-V3</td><td>Kimi K2</td><td>Δ</td></tr><tr><td>#层数</td><td>61</td><td>61</td><td>=</td></tr><tr><td>总参数</td><td>671B</td><td>1.04T</td><td>↑ 54%</td></tr><tr><td>激活参数</td><td>37B</td><td>32.6B</td><td>↓ 13%</td></tr><tr><td>专家（总数）</td><td>256</td><td>384</td><td>↑ 50%</td></tr><tr><td>每个 Token 激活的专家</td><td>8</td><td>8</td><td>=</td></tr><tr><td>共享专家</td><td>1</td><td>1</td><td>=</td></tr><tr><td>注意力头</td><td>128</td><td>64</td><td>↓ 50%</td></tr><tr><td>密集层数量</td><td>3</td><td>1</td><td>↓ 67%</td></tr><tr><td>专家分组</td><td>是</td><td>否</td><td>-</td></tr></table>

稀疏度缩放定律 我们为专家混合（MoE）模型系列开发了一个使用 Muon 的稀疏度缩放定律。稀疏度定义为专家总数与激活专家数量的比率。通过仔细控制的小规模实验，我们观察到——在固定激活参数数量（即恒定 FLOPs）下——增加专家总数（即增加稀疏度）持续降低训练和验证损失，从而增强整体模型性能（图 5）。具体来说，在计算最优稀疏度缩放定律下，达到相同的验证损失 1.5，稀疏度 48 与稀疏度水平 8、16 和 32 相比分别减少 FLOPs $1.69\times$、$1.39\times$ 和 $1.15\times$。虽然增加稀疏度导致更好的性能，但这种收益伴随着基础设施复杂性的增加。为了平衡模型性能与成本，我们为 Kimi K2 采用稀疏度 48，每次前向传递激活 384 个专家中的 8 个。

![](images/ae9050eabf148a92ff933fe8e3ec800e5282b97535f678a4df264625c815ee0b.jpg)  
图 5: 稀疏度缩放定律。增加稀疏度导致改进的模型性能。我们固定激活专家数量为 8，共享专家数量为 1，并改变专家总数，产生具有不同稀疏度水平的模型。

![](images/98ef142c093940fb89ee02798ef9bcb79086526f59231abfe0582004109a4548.jpg)  
图 6: 注意力头数量等于层数的模型及其注意力头数量翻倍的对应模型的缩放曲线。将注意力头数量翻倍导致验证损失减少约 $0.5\%$ 到 $1.2\%$。

注意力头数量 DeepSeek-V3 [10] 将注意力头数量设置为大约模型层数的两倍，以更好地利用内存带宽并增强计算效率。然而，随着上下文长度增加，将注意力头数量翻倍导致显著的推理开销，在更长序列长度时降低效率。这在智能体应用中成为一个主要限制，其中高效的长上下文处理是必不可少的。例如，在序列长度为 128k 时，将注意力头数量从 64 增加到 128，同时保持总专家数量固定在 384，导致推理 FLOPs 增加 $83\%$。为了评估这种设计的影响，我们进行了受控实验，比较注意力头数量等于层数的配置与注意力头数量翻倍的配置，在不同的训练 FLOPs 下。在等 token 训练条件下，我们观察到将注意力头翻倍在不同计算预算下仅产生验证损失的适度改进（范围从 $0.5\%$ 到 $1.2\%$）（图 6）。鉴于稀疏度 48 已经提供强大的性能，将注意力头翻倍的边际收益不能证明推理成本的合理性。因此我们选择 64 个注意力头。

## 2.4 训练基础设施

### 2.4.1 计算集群

Kimi K2 在配备 NVIDIA H800 GPU 的集群上训练。H800 集群中的每个节点包含 2 TB RAM 和 8 个通过节点内 NVLink 和 NVSwitch 连接的 GPU。在不同节点之间，利用 $8\times 400$ Gbps RoCE 互连来促进通信。

### 2.4.2 模型缩放的并行性

大语言模型的训练通常在动态资源可用性下进行。我们不是优化一个仅在特定资源量下适用的并行策略，而是追求一个灵活的策略，允许 Kimi K2 在 32 的倍数的任何节点数上训练。我们的策略利用 16 路

![](images/84cae4117299a405ba2ee31ae95de70e5b4ba63e7e2ce4343effc0a4724f386e.jpg)  
图 7: 计算、通信和卸载在不同 PP 阶段重叠。

管道并行（PP）与虚拟阶段 [28, 53, 38, 57, 47, 21]、16 路专家并行（EP）[39] 和 ZeRO-1 数据并行 [60] 的组合。

在这种设置下，在 BF1b 中存储模型参数并在 FP32 中存储其梯度累积缓冲区需要大约 6 TB 的 GPU 内存，分布在 256 个 GPU 的模型并行组上。优化器状态的放置取决于训练配置。当训练节点总数很大时，优化器状态被分布，将其每设备内存占用减少到可忽略的水平。当训练节点总数很小时（例如，32），我们可以将一些优化器状态卸载到 CPU。

这种方法允许我们为小规模和大规模实验重用相同的并行配置，同时让每个 GPU 为所有状态保持大约 30 GB 的 GPU 内存。其余 GPU 内存用于激活，如第 2.4.3 节所述。这种一致的设计对研究效率很重要，因为它简化了系统并大大加速了实验迭代。

EP 通信与交错 1F1B 重叠 通过增加预热微批次的数量，我们可以在标准交错 1F1B 调度下重叠 EP 全对全通信与计算 [21, 53]。相比之下，DualPipe [10] 使参数和梯度所需的内存翻倍，需要增加并行性来补偿。增加 PP 引入更多气泡，而增加 EP，如下所述，产生更高的开销。对于训练超过 1 万亿参数的大模型，额外成本过高，因此我们选择不使用 DualPipe。

然而，交错 1F1B 将模型分割成更多阶段，引入非平凡的 PP 通信开销。为了缓解这种成本，我们将权重-梯度计算与每个微批次的后向传递解耦，并与相应的 PP 通信并行执行。因此，除了预热阶段外，所有 PP 通信都可以有效重叠。

更小的 EP 大小 为了确保在 1F1B 阶段期间完全的计算-通信重叠，K2 中减少的注意力计算时间（与 DeepSeek-V3 中的 128 个头相比有 64 个注意力头）需要最小化 EP 操作的时间。这是通过采用最小的可行 EP 并行化策略实现的，具体是 $\mathrm{EP} = 16$。利用更小的 EP 组也放宽了专家平衡约束，允许在不进一步调整的情况下实现接近最优的速度。

### 2.4.3 激活减少

在为参数、梯度缓冲区和优化器状态保留空间后，每个设备上的剩余 GPU 内存不足以保存完整的 MoE 激活。为了确保激活内存适合约束，特别是对于在 1F1B 预热阶段累积最大激活的初始管道阶段，采用以下技术。

选择性重计算 重计算应用于廉价、高占用阶段，包括 LayerNorm、SwiGLU 和 MLA 上投影 [10]。此外，MoE 下投影在训练期间重计算以进一步减少激活内存。虽然可选，但这种重计算保持足够的 GPU 内存，防止早期训练阶段中专家不平衡导致的崩溃。

不敏感激活的 FP8 存储 MoE 上投影和 SwiGLU 的输入被压缩到 FP8-E4M3，在 $1\times 128$ 瓦片中带有 FP32 缩放。小规模实验显示没有可测量的损失增加。由于我们在初步研究中观察到的性能下降的潜在风险，我们不在计算中应用 FP8。

激活 CPU 卸载 所有剩余的激活都被卸载到 CPU RAM。复制引擎负责流式传输卸载和加载，与计算和通信内核重叠。在 1F1B 阶段，我们卸载前一个微批次的前向激活，同时预取下一个的后向激活。预热和冷却阶段类似处理，整体模式如图 7 所示。虽然卸载可能由于 PCIe 流量拥塞而稍微影响 EP 流量，但我们的测试显示 EP 通信保持完全重叠。

## 2.5 训练配方

我们使用 MuonClip 优化器（算法 1）和 WSD 学习率调度 [25] 预训练模型，使用 4,096-token 上下文窗口，处理总共 15.5T tokens。前 10T tokens 在 300 步预热后以 2e-4 的恒定学习率训练，随后 3.5T tokens 从 2e-4 到 2e-5 的余弦衰减。权重衰减在整个过程中设置为 0.1，全局批次大小保持在 67M tokens。整体训练曲线如图 3 所示。

在预训练接近结束时，我们进行了退火阶段，随后是长上下文激活阶段。批次大小保持在 67M tokens 恒定，而学习率从 2e-5 衰减到 7e-6。在这个阶段，模型在 4k 序列长度上训练了 4000 亿 tokens，随后在 32k 序列长度上额外训练了 600 亿 tokens。为了将上下文窗口扩展到 128k，我们采用了 YaRN 方法 [55]。

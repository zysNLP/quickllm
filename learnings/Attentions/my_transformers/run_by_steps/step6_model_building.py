# -*- coding: utf-8 -*-
"""
Step 6: æ„å»ºTransformeræ¨¡å‹
æ„å»ºå®Œæ•´çš„Transformeræ¨¡å‹ç»“æ„ï¼ŒåŒ…æ‹¬Encoderã€Decoderã€MultiHeadAttentionç­‰ç»„ä»¶
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import matplotlib.pyplot as plt

# è®¾ç½®GPUè®¾å¤‡
os.environ["CUDA_VISIBLE_DEVICES"] = "2"

def get_position_embedding(sentence_length: int, d_model: int, device="cuda", dtype=torch.float32):
    """ç”Ÿæˆä½ç½®ç¼–ç """
    def get_angles(pos: torch.Tensor, i: torch.Tensor, d_model: int):
        angle_rates = 1.0 / torch.pow(
            10000,
            (2 * torch.div(i, 2, rounding_mode='floor')).float() / d_model
        )
        return pos.float() * angle_rates

    if device is None:
        device = torch.device("cpu")

    pos = torch.arange(sentence_length, device=device).unsqueeze(1)  # [L, 1]
    i = torch.arange(d_model, device=device).unsqueeze(0)  # [1, D]

    angle_rads = get_angles(pos, i, d_model)  # [L, D]

    # å¶æ•°ä¸‹æ ‡ï¼šsin
    sines = torch.sin(angle_rads[:, 0::2])
    # å¥‡æ•°ä¸‹æ ‡ï¼šcos
    cosines = torch.cos(angle_rads[:, 1::2])

    # æ‹¼æ¥è¿˜åŸæˆ [L, D]
    position_embedding = torch.zeros((sentence_length, d_model), device=device, dtype=dtype)
    position_embedding[:, 0::2] = sines
    position_embedding[:, 1::2] = cosines

    # å¢åŠ  batch ç»´åº¦ [1, L, D]
    position_embedding = position_embedding.unsqueeze(0)

    return position_embedding

def create_padding_mask(batch_data: torch.Tensor, pad_token_id: int = 0):
    """åˆ›å»ºpadding mask"""
    mask = (batch_data == pad_token_id).float()
    return mask[:, None, None, :]  # [B, 1, 1, L]

def create_look_ahead_mask(size: int):
    """ç”ŸæˆLook-ahead mask (ä¸Šä¸‰è§’çŸ©é˜µ)"""
    ones = torch.ones((size, size))
    mask = torch.triu(ones, diagonal=1)
    return mask

def scaled_dot_product_attention(q, k, v, mask=None):
    """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶"""
    # (..., seq_len_q, seq_len_k)
    matmul_qk = torch.matmul(q, k.transpose(-2, -1))

    # ç¼©æ”¾
    dk = q.size()[-1]
    scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(dk, dtype=torch.float32, device=q.device))

    # åŠ ä¸Š mask
    if mask is not None:
        scaled_attention_logits = scaled_attention_logits.masked_fill(mask == 1, -1e9)

    # softmax å¾—åˆ°æ³¨æ„åŠ›æƒé‡
    attention_weights = F.softmax(scaled_attention_logits, dim=-1)

    # åŠ æƒæ±‚å’Œ
    output = torch.matmul(attention_weights, v)

    return output, attention_weights

class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, d_model: int, num_heads: int):
        super().__init__()
        assert d_model % num_heads == 0, "d_model å¿…é¡»èƒ½è¢« num_heads æ•´é™¤"
        self.d_model = d_model
        self.num_heads = num_heads
        self.depth = d_model // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦ Dh

        # å¯¹åº” Keras çš„ Dense(d_model)
        self.WQ = nn.Linear(d_model, d_model, bias=True)
        self.WK = nn.Linear(d_model, d_model, bias=True)
        self.WV = nn.Linear(d_model, d_model, bias=True)

        self.out_proj = nn.Linear(d_model, d_model, bias=True)

    def _split_heads(self, x: torch.Tensor):
        """x: [B, L, d_model] -> [B, num_heads, L, depth]"""
        B, L, _ = x.shape
        x = x.view(B, L, self.num_heads, self.depth)  # [B, L, H, Dh]
        x = x.permute(0, 2, 1, 3).contiguous()  # [B, H, L, Dh]
        return x

    def _combine_heads(self, x: torch.Tensor):
        """x: [B, num_heads, L, depth] -> [B, L, d_model]"""
        B, H, L, Dh = x.shape
        x = x.permute(0, 2, 1, 3).contiguous()  # [B, L, H, Dh]
        x = x.view(B, L, H * Dh)  # [B, L, d_model]
        return x

    def forward(self, q, k, v, mask=None, return_attn: bool = True):
        B = q.size(0)

        # çº¿æ€§æ˜ å°„
        q = self.WQ(q)  # [B, Lq, d_model]
        k = self.WK(k)  # [B, Lk, d_model]
        v = self.WV(v)  # [B, Lv, d_model]

        # åˆ†å¤´
        q = self._split_heads(q)  # [B, H, Lq, Dh]
        k = self._split_heads(k)  # [B, H, Lk, Dh]
        v = self._split_heads(v)  # [B, H, Lv, Dh]

        # å¤„ç† maskï¼šå¹¿æ’­åˆ° [B, H, Lq, Lk]
        if mask is not None:
            if mask.dim() == 3:
                mask = mask.unsqueeze(1)  # [B,1,Lq,Lk]
            elif mask.dim() == 4 and mask.size(1) == 1:
                pass  # å·²æ˜¯ [B,1,Lq,Lk]
            else:
                raise ValueError("mask å½¢çŠ¶éœ€ä¸º [B, Lq, Lk] æˆ– [B, 1, Lq, Lk]")
            mask = mask.expand(B, self.num_heads, mask.size(-2), mask.size(-1))

        # æ³¨æ„åŠ›
        attn_out, attn_weights = scaled_dot_product_attention(q, k, v, mask)  # [B,H,Lq,Dh], [B,H,Lq,Lk]

        # åˆå¹¶å¤´
        attn_out = self._combine_heads(attn_out)  # [B, Lq, d_model]

        # è¾“å‡ºçº¿æ€§å±‚
        output = self.out_proj(attn_out)  # [B, Lq, d_model]

        if return_attn:
            return output, attn_weights
        return output

def feed_forward_network(d_model, dff):
    """å‰é¦ˆç½‘ç»œ FFN"""
    return nn.Sequential(
        nn.Linear(d_model, dff),
        nn.ReLU(),
        nn.Linear(dff, d_model)
    )

class EncoderLayer(nn.Module):
    """ç¼–ç å™¨å±‚"""
    
    def __init__(self, d_model: int, num_heads: int, dff: int, rate: float = 0.1):
        super().__init__()
        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ffn = feed_forward_network(d_model, dff)

        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)
        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)

        self.dropout1 = nn.Dropout(rate)
        self.dropout2 = nn.Dropout(rate)

    def forward(self, x: torch.Tensor, src_mask: torch.Tensor = None):
        # Self-Attention
        attn_out, _ = self.mha(x, x, x, mask=src_mask)
        attn_out = self.dropout1(attn_out)
        out1 = self.norm1(x + attn_out)  # æ®‹å·® + LayerNorm

        # Feed Forward
        ffn_out = self.ffn(out1)
        ffn_out = self.dropout2(ffn_out)
        out2 = self.norm2(out1 + ffn_out)

        return out2

class DecoderLayer(nn.Module):
    """è§£ç å™¨å±‚"""
    
    def __init__(self, d_model: int, num_heads: int, dff: int, rate: float = 0.1):
        super().__init__()
        self.mha1 = MultiHeadAttention(d_model, num_heads)  # masked self-attn
        self.mha2 = MultiHeadAttention(d_model, num_heads)  # cross-attn

        self.ffn = feed_forward_network(d_model, dff)

        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)
        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)
        self.norm3 = nn.LayerNorm(d_model, eps=1e-6)

        self.dropout1 = nn.Dropout(rate)
        self.dropout2 = nn.Dropout(rate)
        self.dropout3 = nn.Dropout(rate)

    def forward(
            self,
            x: torch.Tensor,
            enc_out: torch.Tensor,
            tgt_mask: torch.Tensor = None,
            enc_dec_mask: torch.Tensor = None,
    ):
        # 1) Masked Self-Attention
        attn1_out, attn_weights1 = self.mha1(x, x, x, mask=tgt_mask)
        attn1_out = self.dropout1(attn1_out)
        out1 = self.norm1(x + attn1_out)

        # 2) Cross-Attention
        attn2_out, attn_weights2 = self.mha2(out1, enc_out, enc_out, mask=enc_dec_mask)
        attn2_out = self.dropout2(attn2_out)
        out2 = self.norm2(out1 + attn2_out)

        # 3) FFN
        ffn_out = self.ffn(out2)
        ffn_out = self.dropout3(ffn_out)
        out3 = self.norm3(out2 + ffn_out)

        return out3, attn_weights1, attn_weights2

class EncoderModel(nn.Module):
    """ç¼–ç å™¨æ¨¡å‹"""
    
    def __init__(self, num_layers: int, input_vocab_size: int, max_length: int,
                 d_model: int, num_heads: int, dff: int, rate: float = 0.1,
                 padding_idx: int = None):
        super().__init__()
        self.d_model = d_model
        self.num_layers = num_layers
        self.max_length = max_length

        # Embedding
        self.embedding = nn.Embedding(input_vocab_size, d_model, padding_idx=padding_idx)

        # ä½ç½®ç¼–ç ï¼šæ³¨å†Œä¸º bufferï¼ˆä¸å‚ä¸è®­ç»ƒ/ä¼˜åŒ–å™¨ï¼‰
        pe = get_position_embedding(max_length, d_model)
        self.register_buffer("position_embedding", pe, persistent=False)

        self.dropout = nn.Dropout(rate)

        # å †å  EncoderLayer
        self.encoder_layers = nn.ModuleList(
            [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]
        )

        # é¢„å­˜ç¼©æ”¾å› å­
        self.scale = math.sqrt(d_model)

    def forward(self, x: torch.Tensor, src_mask: torch.Tensor = None):
        B, L = x.shape
        if L > self.max_length:
            raise ValueError(f"input_seq_len ({L}) should be â‰¤ max_length ({self.max_length})")

        # [B, L, D]
        x = self.embedding(x)
        # ç¼©æ”¾ï¼šä½¿ embedding çš„å°ºåº¦ä¸ä½ç½®ç¼–ç ç›¸è¿‘ï¼ˆè®ºæ–‡åšæ³•ï¼‰
        x = x * self.scale
        # åŠ ä½ç½®ç¼–ç ï¼ˆæŒ‰å®é™…åºåˆ—é•¿åº¦åˆ‡ç‰‡ï¼‰
        x = x + self.position_embedding[:, :L, :]

        x = self.dropout(x)

        # é€å±‚ Encoder
        for layer in self.encoder_layers:
            x = layer(x, src_mask)

        return x

class DecoderModel(nn.Module):
    """è§£ç å™¨æ¨¡å‹"""
    
    def __init__(self, num_layers: int, target_vocab_size: int, max_length: int,
                 d_model: int, num_heads: int, dff: int, rate: float = 0.1,
                 padding_idx: int = None):
        super().__init__()
        self.num_layers = num_layers
        self.max_length = max_length
        self.d_model = d_model

        # è¯åµŒå…¥
        self.embedding = nn.Embedding(target_vocab_size, d_model, padding_idx=padding_idx)

        # ä½ç½®ç¼–ç ï¼ˆæ³¨å†Œä¸º bufferï¼Œä¸å‚ä¸è®­ç»ƒï¼‰
        pe = get_position_embedding(max_length, d_model)
        self.register_buffer("position_embedding", pe, persistent=False)

        self.dropout = nn.Dropout(rate)

        # å †å è§£ç å±‚
        self.decoder_layers = nn.ModuleList(
            [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]
        )

        self.scale = math.sqrt(d_model)

    def forward(
            self,
            x: torch.Tensor,  # [B, L_tgt] ç›®æ ‡ç«¯ token ids
            enc_out: torch.Tensor,  # [B, L_src, D] ç¼–ç å™¨è¾“å‡º
            tgt_mask: torch.Tensor = None,  # [B, 1, L_tgt, L_tgt] æˆ– [B, L_tgt, L_tgt]ï¼ˆlook-ahead+paddingï¼‰
            enc_dec_mask: torch.Tensor = None,  # [B, 1, L_tgt, L_src] æˆ– [B, L_tgt, L_src]ï¼ˆå¯¹ encoder çš„ paddingï¼‰
    ):
        B, Lt = x.shape
        if Lt > self.max_length:
            raise ValueError(f"output_seq_len ({Lt}) should be â‰¤ max_length ({self.max_length})")

        # (B, Lt, D)
        x = self.embedding(x) * self.scale
        x = x + self.position_embedding[:, :Lt, :]
        x = self.dropout(x)

        attention_weights = {}

        for i, layer in enumerate(self.decoder_layers, start=1):
            x, attn1, attn2 = layer(x, enc_out, tgt_mask=tgt_mask, enc_dec_mask=enc_dec_mask)
            attention_weights[f"decoder_layer{i}_att1"] = attn1  # [B, H, Lt, Lt]
            attention_weights[f"decoder_layer{i}_att2"] = attn2  # [B, H, Lt, Ls]

        # x: (B, Lt, D)
        return x, attention_weights

class Transformer(nn.Module):
    """å®Œæ•´çš„Transformeræ¨¡å‹"""
    
    def __init__(self, num_layers, input_vocab_size, target_vocab_size,
                 max_length, d_model, num_heads, dff, rate=0.1,
                 src_padding_idx: int = None, tgt_padding_idx: int = None):
        super().__init__()
        self.encoder_model = EncoderModel(
            num_layers=num_layers,
            input_vocab_size=input_vocab_size,
            max_length=max_length,
            d_model=d_model,
            num_heads=num_heads,
            dff=dff,
            rate=rate,
            padding_idx=src_padding_idx,
        )
        self.decoder_model = DecoderModel(
            num_layers=num_layers,
            target_vocab_size=target_vocab_size,
            max_length=max_length,
            d_model=d_model,
            num_heads=num_heads,
            dff=dff,
            rate=rate,
            padding_idx=tgt_padding_idx,
        )
        # ç­‰ä»·äº Keras çš„ Dense(target_vocab_size)
        self.final_layer = nn.Linear(d_model, target_vocab_size)

    def forward(self, inp_ids, tgt_ids, src_mask=None, tgt_mask=None, enc_dec_mask=None):
        enc_out = self.encoder_model(inp_ids, src_mask=src_mask)  # [B, L_src, D]
        dec_out, attention_weights = self.decoder_model(
            tgt_ids, enc_out, tgt_mask=tgt_mask, enc_dec_mask=enc_dec_mask
        )  # [B, L_tgt, D], dict
        logits = self.final_layer(dec_out)  # [B, L_tgt, V_tgt]
        return logits, attention_weights

def count_parameters(model):
    """ç»Ÿè®¡æ¨¡å‹å‚æ•°æ•°é‡"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

if __name__ == "__main__":
    print("=" * 60)
    print("Step 6: æ„å»ºTransformeræ¨¡å‹")
    print("=" * 60)
    
    # æ¨¡å‹ç»“æ„å‚æ•°
    num_layers = 4
    d_model = 128  # hidden-size
    dff = 512
    num_heads = 8
    dropout_rate = 0.25
    max_length = 30
    
    # è¯è¡¨å¤§å°ï¼ˆç¤ºä¾‹å€¼ï¼‰
    input_vocab_size = 8192  # è‘¡è„ç‰™è¯­è¯è¡¨å¤§å°
    target_vocab_size = 8192  # è‹±è¯­è¯è¡¨å¤§å°
    
    print(f"ğŸ”§ æ¨¡å‹ç»“æ„å‚æ•°:")
    print(f"   ç¼–ç å™¨å±‚æ•°: {num_layers}")
    print(f"   è§£ç å™¨å±‚æ•°: {num_layers}")
    print(f"   åµŒå…¥ç»´åº¦: {d_model}")
    print(f"   å‰é¦ˆç½‘ç»œç»´åº¦: {dff}")
    print(f"   æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
    print(f"   Dropoutç‡: {dropout_rate}")
    print(f"   æœ€å¤§åºåˆ—é•¿åº¦: {max_length}")
    print(f"   è¾“å…¥è¯è¡¨å¤§å°: {input_vocab_size}")
    print(f"   ç›®æ ‡è¯è¡¨å¤§å°: {target_vocab_size}")
    
    try:
        # 1. æ„å»ºTransformeræ¨¡å‹
        print(f"\nğŸ”¨ æ„å»ºTransformeræ¨¡å‹...")
        model = Transformer(
            num_layers=num_layers,
            input_vocab_size=input_vocab_size,
            target_vocab_size=target_vocab_size,
            max_length=max_length,
            d_model=d_model,
            num_heads=num_heads,
            dff=dff,
            rate=dropout_rate,
            src_padding_idx=0,  # å‡è®¾pad_token_id=0
            tgt_padding_idx=0,
        )
        
        print(f"âœ… Transformeræ¨¡å‹æ„å»ºå®Œæˆï¼")
        
        # 2. ç»Ÿè®¡æ¨¡å‹å‚æ•°
        total_params = count_parameters(model)
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"   æ€»å‚æ•°æ•°é‡: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {total_params:,}")
        
        # 3. æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
        batch_size = 2
        src_len = 10
        tgt_len = 8
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        inp_ids = torch.randint(0, input_vocab_size, (batch_size, src_len))
        tgt_ids = torch.randint(0, target_vocab_size, (batch_size, tgt_len))
        
        print(f"   è¾“å…¥å½¢çŠ¶: {inp_ids.shape}")
        print(f"   ç›®æ ‡å½¢çŠ¶: {tgt_ids.shape}")
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            logits, attention_weights = model(inp_ids, tgt_ids)
        
        print(f"   è¾“å‡ºlogitså½¢çŠ¶: {logits.shape}")
        print(f"   æ³¨æ„åŠ›æƒé‡æ•°é‡: {len(attention_weights)}")
        
        # 4. æ˜¾ç¤ºæ¨¡å‹ç»“æ„
        print(f"\nğŸ“‹ æ¨¡å‹ç»“æ„:")
        print(model)
        
        print(f"\nâœ… Transformeræ¨¡å‹æ„å»ºå’Œæµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥å‚æ•°è®¾ç½®æ˜¯å¦æ­£ç¡®")

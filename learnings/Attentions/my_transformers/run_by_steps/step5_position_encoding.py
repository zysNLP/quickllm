# -*- coding: utf-8 -*-
"""
Step 5: ä½ç½®ç¼–ç å¯è§†åŒ–
ç”Ÿæˆå’Œå¯è§†åŒ–ä½ç½®ç¼–ç çŸ©é˜µ
"""

import os
import torch
import matplotlib.pyplot as plt
import numpy as np


def get_device():
    """è‡ªåŠ¨æ£€æµ‹å¯ç”¨è®¾å¤‡"""
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"âœ… ä½¿ç”¨ GPU: {torch.cuda.get_device_name()}")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")  # Apple Silicon GPU
        print("âœ… ä½¿ç”¨ Apple Silicon GPU (MPS)")
    else:
        device = torch.device("cpu")
        print("âœ… ä½¿ç”¨ CPU")
    return device


def get_position_embedding(sentence_length: int, d_model: int, device=None, dtype=torch.float32):
    """
    è¿”å›ž position å¯¹åº”çš„ embedding çŸ©é˜µ
    å½¢çŠ¶: [1, sentence_length, d_model]
    """

    def get_angles(pos: torch.Tensor, i: torch.Tensor, d_model: int):
        """
        èŽ·å–å•è¯ pos å¯¹åº” embedding çš„è§’åº¦
        pos: [sentence_length, 1]
        i  : [1, d_model]
        return: [sentence_length, d_model]
        """
        angle_rates = 1.0 / torch.pow(
            10000,
            (2 * torch.div(i, 2, rounding_mode='floor')).float() / d_model
        )
        return pos.float() * angle_rates

    if device is None:
        device = get_device()

    pos = torch.arange(sentence_length, device=device).unsqueeze(1)  # [L, 1]
    i = torch.arange(d_model, device=device).unsqueeze(0)  # [1, D]

    angle_rads = get_angles(pos, i, d_model)  # [L, D]

    # å¶æ•°ä¸‹æ ‡ï¼šsin
    sines = torch.sin(angle_rads[:, 0::2])
    # å¥‡æ•°ä¸‹æ ‡ï¼šcos
    cosines = torch.cos(angle_rads[:, 1::2])

    # æ‹¼æŽ¥è¿˜åŽŸæˆ [L, D]
    position_embedding = torch.zeros((sentence_length, d_model), device=device, dtype=dtype)
    position_embedding[:, 0::2] = sines
    position_embedding[:, 1::2] = cosines

    # å¢žåŠ  batch ç»´åº¦ [1, L, D]
    position_embedding = position_embedding.unsqueeze(0)

    return position_embedding


def plot_position_embedding(position_embedding: torch.Tensor):
    """
    å¯è§†åŒ–ä½ç½®ç¼–ç çŸ©é˜µ
    å‚æ•°:
        position_embedding: [1, L, D] çš„å¼ é‡
    """
    # è½¬åˆ° CPUï¼Œå¹¶è½¬æˆ numpy
    pe = position_embedding.detach().cpu().numpy()[0]  # [L, D]

    plt.figure(figsize=(12, 8))

    # ä¸»å›¾ï¼šä½ç½®ç¼–ç çš„çƒ­åŠ›å›¾
    plt.subplot(2, 1, 1)
    plt.pcolormesh(pe, cmap='RdBu')  # L Ã— D çŸ©é˜µ
    plt.xlabel("Depth (d_model)")
    plt.xlim((0, pe.shape[1]))
    plt.ylabel("Position (pos)")
    plt.colorbar(label='Encoding Value')
    plt.title("Positional Encoding Visualization")

    # å­å›¾ï¼šå‰å‡ ä¸ªä½ç½®çš„ç¼–ç æ¨¡å¼
    plt.subplot(2, 1, 2)
    positions_to_plot = min(8, pe.shape[0])
    for pos in range(positions_to_plot):
        plt.plot(pe[pos, :50], label=f'Pos {pos}', alpha=0.7, linewidth=2)
    plt.xlabel("Dimension")
    plt.ylabel("Encoding Value")
    plt.title("Positional Encoding Patterns (First 50 Dimensions)")
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()

    #plt.show()
    plt.savefig('step5_positional_encoding.png', dpi=300, bbox_inches='tight')


def analyze_position_encoding(position_embedding: torch.Tensor):
    """
    åˆ†æžä½ç½®ç¼–ç çš„ç‰¹æ€§
    """
    pe = position_embedding.detach().cpu().numpy()[0]  # [L, D]

    print(f"ðŸ“Š ä½ç½®ç¼–ç åˆ†æž:")
    print(f"   åºåˆ—é•¿åº¦: {pe.shape[0]}")
    print(f"   åµŒå…¥ç»´åº¦: {pe.shape[1]}")
    print(f"   æ•°å€¼èŒƒå›´: [{pe.min():.4f}, {pe.max():.4f}]")
    print(f"   å‡å€¼: {pe.mean():.4f}")
    print(f"   æ ‡å‡†å·®: {pe.std():.4f}")

    # åˆ†æžä¸åŒä½ç½®çš„ç›¸ä¼¼æ€§
    print(f"\nðŸ” ä½ç½®ç›¸ä¼¼æ€§åˆ†æž:")
    positions_to_check = min(5, pe.shape[0])
    for i in range(positions_to_check):
        for j in range(i + 1, positions_to_check):
            similarity = np.corrcoef(pe[i], pe[j])[0, 1]
            print(f"   ä½ç½® {i} ä¸Žä½ç½® {j} çš„ç›¸ä¼¼åº¦: {similarity:.4f}")

    # åˆ†æžç»´åº¦é—´çš„ç›¸å…³æ€§
    print(f"\nðŸ“ˆ ç»´åº¦åˆ†æž:")
    even_dims = pe[:, 0::2]  # å¶æ•°ç»´åº¦ (sin)
    odd_dims = pe[:, 1::2]  # å¥‡æ•°ç»´åº¦ (cos)
    print(f"   å¶æ•°ç»´åº¦(sin)èŒƒå›´: [{even_dims.min():.4f}, {even_dims.max():.4f}]")
    print(f"   å¥‡æ•°ç»´åº¦(cos)èŒƒå›´: [{odd_dims.min():.4f}, {odd_dims.max():.4f}]")


def verify_position_encoding(position_embedding: torch.Tensor):
    """
    éªŒè¯ä½ç½®ç¼–ç çš„æ­£ç¡®æ€§
    """
    pe = position_embedding.detach().cpu().numpy()[0]  # [L, D]

    print(f"\nâœ… ä½ç½®ç¼–ç éªŒè¯:")

    # æ£€æŸ¥æ•°å€¼èŒƒå›´
    assert pe.min() >= -1.0 and pe.max() <= 1.0, "ä½ç½®ç¼–ç å€¼è¶…å‡ºé¢„æœŸèŒƒå›´"
    print("   âœ“ æ•°å€¼èŒƒå›´éªŒè¯é€šè¿‡")

    # æ£€æŸ¥äº¤æ›¿æ¨¡å¼
    even_dims = pe[:, 0::2]  # åº”è¯¥æ˜¯sin
    odd_dims = pe[:, 1::2]  # åº”è¯¥æ˜¯cos

    # å¯¹äºŽç¬¬ä¸€ä¸ªä½ç½®ï¼Œæ£€æŸ¥sin(0)åº”è¯¥æŽ¥è¿‘0
    first_pos_sin = even_dims[0, 0]
    assert abs(first_pos_sin) < 1e-6, f"ç¬¬ä¸€ä¸ªä½ç½®çš„sinå€¼åº”è¯¥æŽ¥è¿‘0ï¼Œå®žé™…ä¸º{first_pos_sin}"
    print("   âœ“ æ­£å¼¦ä½™å¼¦äº¤æ›¿æ¨¡å¼éªŒè¯é€šè¿‡")

    # æ£€æŸ¥ä¸åŒä½ç½®ç¼–ç æ˜¯å¦ä¸åŒ
    unique_positions = len(set(tuple(row) for row in pe))
    assert unique_positions == pe.shape[0], "å­˜åœ¨é‡å¤çš„ä½ç½®ç¼–ç "
    print("   âœ“ ä½ç½®å”¯ä¸€æ€§éªŒè¯é€šè¿‡")

    print("   ðŸŽ‰ æ‰€æœ‰éªŒè¯é€šè¿‡ï¼ä½ç½®ç¼–ç ç”Ÿæˆæ­£ç¡®ã€‚")


if __name__ == "__main__":
    print("=" * 60)
    print("Step 5: ä½ç½®ç¼–ç å¯è§†åŒ–")
    print("=" * 60)

    # å‚æ•°è®¾ç½®
    max_length = 30  # æœ€å¤§åºåˆ—é•¿åº¦
    d_model = 128  # åµŒå…¥ç»´åº¦

    print(f"ðŸ”§ ä½ç½®ç¼–ç å‚æ•°:")
    print(f"   æœ€å¤§åºåˆ—é•¿åº¦: {max_length}")
    print(f"   åµŒå…¥ç»´åº¦: {d_model}")

    try:
        # èŽ·å–è®¾å¤‡
        device = get_device()

        # 1. ç”Ÿæˆä½ç½®ç¼–ç 
        print(f"\nðŸ”¨ ç”Ÿæˆä½ç½®ç¼–ç ...")
        position_embedding = get_position_embedding(max_length, d_model, device=device)

        print(f"âœ… ä½ç½®ç¼–ç ç”Ÿæˆå®Œæˆï¼")
        print(f"ðŸ“Š ä½ç½®ç¼–ç å½¢çŠ¶: {position_embedding.shape}")
        print(f"ðŸ’» ä½¿ç”¨è®¾å¤‡: {device}")

        # 2. éªŒè¯ä½ç½®ç¼–ç 
        verify_position_encoding(position_embedding)

        # 3. åˆ†æžä½ç½®ç¼–ç 
        print(f"\nðŸ” åˆ†æžä½ç½®ç¼–ç ç‰¹æ€§...")
        analyze_position_encoding(position_embedding)

        # 4. å¯è§†åŒ–ä½ç½®ç¼–ç 
        print(f"\nðŸ“Š å¯è§†åŒ–ä½ç½®ç¼–ç ...")
        plot_position_embedding(position_embedding)

        print(f"\nâœ… ä½ç½®ç¼–ç å¯è§†åŒ–å®Œæˆï¼")

        # 5. å±•ç¤ºä½ç½®ç¼–ç çš„æ•°å€¼
        print(f"\nðŸ“ ä½ç½®ç¼–ç æ•°å€¼ç¤ºä¾‹ (å‰5ä¸ªä½ç½®ï¼Œå‰10ä¸ªç»´åº¦):")
        pe_np = position_embedding.detach().cpu().numpy()[0]
        for i in range(min(5, pe_np.shape[0])):
            values_str = " ".join([f"{x:6.3f}" for x in pe_np[i, :10]])
            print(f"   ä½ç½® {i:2d}: [{values_str}]")


    except Exception as e:
        print(f"âŒ ä½ç½®ç¼–ç ç”Ÿæˆå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        print("ðŸ’¡ å»ºè®®æ£€æŸ¥:")
        print("   1. PyTorch æ˜¯å¦æ­£ç¡®å®‰è£…")
        print("   2. è®¾å¤‡æ˜¯å¦å¯ç”¨")
        print("   3. å‚æ•°è®¾ç½®æ˜¯å¦åˆç†")
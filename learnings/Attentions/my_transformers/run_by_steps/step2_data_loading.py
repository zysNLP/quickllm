# -*- coding: utf-8 -*-
"""
Step 2: æ•°æ®åŠ è½½å’Œæ•°æ®é›†ä¿¡æ¯
åŠ è½½è‘¡è„ç‰™è¯­-è‹±è¯­ç¿»è¯‘æ•°æ®é›† (TED Talks)
"""

import os
from datasets import load_dataset
from loguru import logger

def load_translation_dataset(train_path: str, val_path: str, delimiter: str = "\t"):
    """
    åŠ è½½è‘¡è„ç‰™è¯­-è‹±è¯­ç¿»è¯‘æ•°æ®é›† (TED Talks)

    å‚æ•°:
        train_path: è®­ç»ƒé›† CSV æ–‡ä»¶è·¯å¾„
        val_path: éªŒè¯é›† CSV æ–‡ä»¶è·¯å¾„
        delimiter: åˆ†éš”ç¬¦ï¼Œé»˜è®¤åˆ¶è¡¨ç¬¦ '\t'

    è¿”å›:
        train_dataset, val_dataset
    """
    logger.info("å¼€å§‹åŠ è½½æ•°æ®...")
    dataset = load_dataset(
        "csv",
        data_files={
            "train": train_path,
            "validation": val_path
        },
        column_names=["pt", "en"],
        delimiter=delimiter
    )

    logger.info(f"æ•°æ®é›†ç±»å‹: {type(dataset)}")
    logger.info(dataset)

    # æ‰“å°ä¸€ä¸ªæ ·æœ¬
    sample = dataset["train"][0]
    logger.info(f"ç¤ºä¾‹æ•°æ® -> pt: {sample['pt']} | en: {sample['en']}")

    return dataset["train"], dataset["validation"]

if __name__ == "__main__":
    print("=" * 60)
    print("Step 2: æ•°æ®åŠ è½½å’Œæ•°æ®é›†ä¿¡æ¯")
    print("=" * 60)
    
    # æ•°æ®æ–‡ä»¶åœ°å€
    train_path = "/data2/workspace/yszhang/train_transformers/tensorflow_datasets/ted_pt_en_train.csv"
    val_path = "/data2/workspace/yszhang/train_transformers/tensorflow_datasets/ted_pt_en_test.csv"
    
    print(f"ğŸ“ è®­ç»ƒé›†è·¯å¾„: {train_path}")
    print(f"ğŸ“ éªŒè¯é›†è·¯å¾„: {val_path}")
    
    try:
        # åŠ è½½æ•°æ®é›†
        train_dataset, val_dataset = load_translation_dataset(
            train_path=train_path,
            val_path=val_path
        )
        
        print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆï¼")
        print(f"ğŸ“Š è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
        print(f"ğŸ“Š éªŒè¯é›†æ ·æœ¬æ•°: {len(val_dataset)}")
        
        # æ˜¾ç¤ºæ›´å¤šæ ·æœ¬
        print(f"\nğŸ“ æ•°æ®é›†æ ·æœ¬é¢„è§ˆ:")
        for i in range(min(3, len(train_dataset))):
            sample = train_dataset[i]
            print(f"  æ ·æœ¬ {i+1}:")
            print(f"    è‘¡è„ç‰™è¯­: {sample['pt']}")
            print(f"    è‹±è¯­: {sample['en']}")
            print()
            
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")

"""
(train_transformers) root@iv-ydg6wcq3ggay8n6dmn75:/data2/workspace/yszhang/train_transformers/run_by_steps/run_by_steps# python step2_data_loading.py 
============================================================
Step 2: æ•°æ®åŠ è½½å’Œæ•°æ®é›†ä¿¡æ¯
============================================================
ğŸ“ è®­ç»ƒé›†è·¯å¾„: /data2/workspace/yszhang/train_transformers/tensorflow_datasets/ted_pt_en_train.csv
ğŸ“ éªŒè¯é›†è·¯å¾„: /data2/workspace/yszhang/train_transformers/tensorflow_datasets/ted_pt_en_test.csv
2025-10-04 17:41:07.332 | INFO     | __main__:load_translation_dataset:23 - å¼€å§‹åŠ è½½æ•°æ®...
2025-10-04 17:41:10.454 | INFO     | __main__:load_translation_dataset:34 - æ•°æ®é›†ç±»å‹: <class 'datasets.dataset_dict.DatasetDict'>
2025-10-04 17:41:10.454 | INFO     | __main__:load_translation_dataset:35 - DatasetDict({
    train: Dataset({
        features: ['pt', 'en'],
        num_rows: 51786
    })
    validation: Dataset({
        features: ['pt', 'en'],
        num_rows: 1194
    })
})
2025-10-04 17:41:10.455 | INFO     | __main__:load_translation_dataset:39 - ç¤ºä¾‹æ•°æ® -> pt: pt | en: en

âœ… æ•°æ®åŠ è½½å®Œæˆï¼
ğŸ“Š è®­ç»ƒé›†æ ·æœ¬æ•°: 51786
ğŸ“Š éªŒè¯é›†æ ·æœ¬æ•°: 1194

ğŸ“ æ•°æ®é›†æ ·æœ¬é¢„è§ˆ:
  æ ·æœ¬ 1:
    è‘¡è„ç‰™è¯­: pt
    è‹±è¯­: en

  æ ·æœ¬ 2:
    è‘¡è„ç‰™è¯­: e quando melhoramos a procura , tiramos a Ãºnica vantagem da impressÃ£o , que Ã© a serendipidade .
    è‹±è¯­: and when you improve searchability , you actually take away the one advantage of print , which is serendipity .

  æ ·æœ¬ 3:
    è‘¡è„ç‰™è¯­: mas e se estes fatores fossem ativos ?
    è‹±è¯­: but what if it were active ?

(train_transformers) root@iv-ydg6wcq3ggay8n6dmn75:/data2/workspace/yszhang/train_transformers/run_by_steps/run_by_steps# 
"""
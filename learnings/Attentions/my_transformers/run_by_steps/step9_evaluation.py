# -*- coding: utf-8 -*-
"""
Step 9: æ¨¡å‹è¯„ä¼°å’Œç¿»è¯‘æµ‹è¯•
å®ç°æ¨¡å‹æ¨ç†ã€ç¿»è¯‘åŠŸèƒ½å’Œæ³¨æ„åŠ›å¯è§†åŒ–
"""

import os
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from loguru import logger

# è®¾ç½®GPUè®¾å¤‡
os.environ["CUDA_VISIBLE_DEVICES"] = "2"

# å¯¼å…¥å¿…è¦çš„ç»„ä»¶ï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼Œç›´æ¥åŒ…å«å‡½æ•°ï¼‰
import math

# å¿…è¦çš„è¾…åŠ©å‡½æ•°
def create_padding_mask(batch_data: torch.Tensor, pad_token_id: int = 0):
    """åˆ›å»ºpadding mask"""
    mask = (batch_data == pad_token_id).float()
    return mask[:, None, None, :]  # [B, 1, 1, L]

def create_look_ahead_mask(size: int):
    """ç”ŸæˆLook-ahead mask (ä¸Šä¸‰è§’çŸ©é˜µ)"""
    ones = torch.ones((size, size))
    mask = torch.triu(ones, diagonal=1)
    return mask

def create_masks(
        inp_ids: torch.Tensor,  # [B, L_src]
        tar_ids: torch.Tensor,  # [B, L_tgt] â€”â€” é€šå¸¸æ˜¯ decoder è¾“å…¥ï¼ˆå·²å·¦ç§»ï¼‰
        src_pad_id: int = 0,
        tgt_pad_id: int = 0,
):
    """
    åˆ›å»ºå„ç§mask
    è¿”å›:
      encoder_padding_mask         : [B, 1, 1, L_src]  (ç»™ EncoderLayer self-attn)
      decoder_mask (LA + padding)  : [B, 1, L_tgt, L_tgt]  (ç»™ DecoderLayer è‡ªæ³¨æ„åŠ›)
      encoder_decoder_padding_mask : [B, 1, 1, L_src]  (ç»™ DecoderLayer cross-attn)
    è¯­ä¹‰:
      1 = å±è”½ï¼ˆmaskedï¼‰ï¼Œ0 = ä¿ç•™
    """
    # 1) Encoder ç«¯ padding mask
    encoder_padding_mask = create_padding_mask(inp_ids, pad_token_id=src_pad_id)  # [B,1,1,L_src]
    encoder_decoder_padding_mask = create_padding_mask(inp_ids, pad_token_id=src_pad_id)  # [B,1,1,L_src]

    # 2) Decoder ç«¯ look-ahead + padding åˆå¹¶
    B, L_tgt = tar_ids.size(0), tar_ids.size(1)

    # [L_tgt, L_tgt] â†’ [1,1,L_tgt,L_tgt]ï¼Œæ”¾åˆ°ä¸è¾“å…¥ç›¸åŒ device/dtype
    look_ahead = create_look_ahead_mask(L_tgt).to(
        device=tar_ids.device, dtype=encoder_padding_mask.dtype
    ).unsqueeze(0).unsqueeze(1)  # [1,1,L_tgt,L_tgt]

    # ç›®æ ‡ç«¯ paddingï¼š [B,1,1,L_tgt] â†’ æ‰©åˆ° [B,1,L_tgt,L_tgt]
    decoder_padding_mask = create_padding_mask(tar_ids, pad_token_id=tgt_pad_id)  # [B,1,1,L_tgt]
    decoder_padding_mask = decoder_padding_mask.expand(-1, -1, L_tgt, -1)  # [B,1,L_tgt,L_tgt]

    # åˆå¹¶ï¼ˆä»»ä¸€ä¸º 1 å³å±è”½ï¼‰
    decoder_mask = torch.maximum(decoder_padding_mask, look_ahead)  # [B,1,L_tgt,L_tgt]

    return encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask

# ç®€å•çš„Transformeræ¨¡æ‹Ÿç±»ï¼ˆç”¨äºæ¼”ç¤ºï¼‰
class Transformer(nn.Module):
    """ç®€å•çš„Transformeræ¨¡æ‹Ÿç±»"""
    def __init__(self, num_layers, input_vocab_size, target_vocab_size,
                 max_length, d_model, num_heads, dff, rate=0.1,
                 src_padding_idx: int = None, tgt_padding_idx: int = None):
        super().__init__()
        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model, padding_idx=src_padding_idx)
        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model, padding_idx=tgt_padding_idx)
        self.final_layer = nn.Linear(d_model, target_vocab_size)
        
    def forward(self, inp_ids, tgt_ids, src_mask=None, tgt_mask=None, enc_dec_mask=None):
        # ç®€å•çš„æ¨¡æ‹Ÿå‰å‘ä¼ æ’­
        enc_out = self.encoder_embedding(inp_ids)
        dec_out = self.decoder_embedding(tgt_ids)
        logits = self.final_layer(dec_out)
        return logits, {}

def encode_with_bos_eos(tokenizer, text: str):
    """ç¼–ç æ–‡æœ¬å¹¶æ·»åŠ BOS/EOSæ ‡è®°"""
    ids = tokenizer.encode(text, add_special_tokens=False)
    bos_id = tokenizer.bos_token_id
    eos_id = tokenizer.eos_token_id
    if bos_id is None or eos_id is None:
        raise ValueError("è¯·ç¡®ä¿ tokenizer è®¾ç½®äº† bos_token/eos_token")
    return [bos_id] + ids + [eos_id]

@torch.no_grad()
def evaluate(
        inp_sentence: str,
        transformer: Transformer,
        pt_tokenizer,
        en_tokenizer,
        max_length: int,
        device: str = None):
    """
    è¯„ä¼°æ¨¡å‹ç¿»è¯‘èƒ½åŠ›
    inp_sentence: è¾“å…¥çš„æºè¯­è¨€å­—ç¬¦ä¸² (pt)
    transformer: å·²è®­ç»ƒçš„ Transformer
    pt_tokenizer, en_tokenizer: åˆ†åˆ«æ˜¯è‘¡è„ç‰™è¯­å’Œè‹±è¯­ tokenizer
    max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
    """
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    transformer.eval()
    transformer.to(device)

    # 1. ç¼–ç è¾“å…¥ï¼ŒåŠ  <s> å’Œ </s>
    inp_ids = encode_with_bos_eos(pt_tokenizer, inp_sentence)
    encoder_input = torch.tensor(inp_ids, dtype=torch.long, device=device).unsqueeze(0)  # (1, Ls)

    # 2. decoder èµ·å§‹ç¬¦ <s>
    start_id = en_tokenizer.bos_token_id
    end_id = en_tokenizer.eos_token_id
    decoder_input = torch.tensor([[start_id]], dtype=torch.long, device=device)  # (1, 1)

    # 3. å¾ªç¯é¢„æµ‹
    attention_weights = {}
    for _ in range(max_length):
        enc_pad_mask, dec_mask, enc_dec_pad_mask = create_masks(
            encoder_input, decoder_input,
            src_pad_id=pt_tokenizer.pad_token_id,
            tgt_pad_id=en_tokenizer.pad_token_id,
        )
        enc_dec_mask = enc_dec_pad_mask.expand(-1, 1, decoder_input.size(1), -1)

        logits, attn = transformer(
            encoder_input, decoder_input,
            src_mask=enc_pad_mask,
            tgt_mask=dec_mask,
            enc_dec_mask=enc_dec_mask,
        )

        # å–æœ€åä¸€æ­¥é¢„æµ‹
        next_token_logits = logits[:, -1, :]  # (1, V)
        predicted_id = torch.argmax(next_token_logits, dim=-1)  # (1,)

        if predicted_id.item() == end_id:
            break

        # æ‹¼æ¥åˆ° decoder_input
        decoder_input = torch.cat(
            [decoder_input, predicted_id.unsqueeze(0)], dim=-1
        )  # (1, Lt+1)
        attention_weights = attn

    return decoder_input.squeeze(0).tolist(), attention_weights

def plot_encoder_decoder_attention(attention, input_sentence, result, layer_name, 
                                  pt_tokenizer, en_tokenizer):
    """
    å¯è§†åŒ–ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›çŸ©é˜µ
    attention: æ¥è‡ª forward è¿”å›çš„ attention_weights dict
               å½¢çŠ¶ [B, num_heads, tgt_len, src_len]
    input_sentence: æºè¯­è¨€å­—ç¬¦ä¸²
    result: ç›®æ ‡å¥å­ token id åˆ—è¡¨ (decoder è¾“å‡º)
    layer_name: æŒ‡å®šå¯è§†åŒ–çš„å±‚ keyï¼Œæ¯”å¦‚ "decoder_layer1_att2"
    """
    fig = plt.figure(figsize=(16, 8))

    # æºå¥å­ç¼–ç 
    input_id_sentence = pt_tokenizer.encode(input_sentence, add_special_tokens=False)

    # å– batch ç»´åº¦ squeezeï¼Œå¹¶è½¬ numpy
    attn = attention[layer_name].squeeze(0)  # [num_heads, tgt_len, src_len]
    attn = attn.detach().cpu().numpy()

    for head in range(attn.shape[0]):
        ax = fig.add_subplot(2, 4, head + 1)

        # åªå– result[:-1] çš„æ³¨æ„åŠ› (å»æ‰æœ€å <eos>)
        ax.matshow(attn[head][:-1, :], cmap="viridis")

        fontdict = {"fontsize": 10}

        # X è½´: è¾“å…¥ token (<s> + sentence + </s>)
        ax.set_xticks(range(len(input_id_sentence) + 2))
        ax.set_xticklabels(
            ["<s>"] + [pt_tokenizer.decode([i]) for i in input_id_sentence] + ["</s>"],
            fontdict=fontdict, rotation=90,
        )

        # Y è½´: decoder è¾“å‡º token
        ax.set_yticks(range(len(result)))
        ax.set_yticklabels(
            [en_tokenizer.decode([i]) for i in result if i < en_tokenizer.vocab_size],
            fontdict=fontdict,
        )

        ax.set_ylim(len(result) - 1.5, -0.5)
        ax.set_xlabel(f"Head {head + 1}")

    plt.tight_layout()
    plt.show()

def translate(input_sentence, transformer, pt_tokenizer, en_tokenizer,
              max_length=64, device=None, layer_name="", show_attention=False):
    """
    ç¿»è¯‘å‡½æ•°
    """
    # è°ƒç”¨æˆ‘ä»¬æ”¹å¥½çš„ evaluate (PyTorch ç‰ˆ)
    result, attention_weights = evaluate(
        inp_sentence=input_sentence,
        transformer=transformer,
        pt_tokenizer=pt_tokenizer,
        en_tokenizer=en_tokenizer,
        max_length=max_length,
        device=device,
    )

    # æŠŠ token id è½¬å›å¥å­
    predicted_sentence = en_tokenizer.decode(
        [i for i in result if i < en_tokenizer.vocab_size],
        skip_special_tokens=True
    )

    logger.info("Input: {}".format(input_sentence))
    logger.info(f"Predicted translation: {predicted_sentence}")

    # å¦‚æœä¼ å…¥äº† layer_nameï¼Œå°±ç”»æ³¨æ„åŠ›å›¾
    if show_attention and layer_name:
        plot_encoder_decoder_attention(
            attention_weights,
            input_sentence,
            result,
            layer_name,
            pt_tokenizer,
            en_tokenizer
        )

    return predicted_sentence

def batch_translate(input_sentences, transformer, pt_tokenizer, en_tokenizer,
                   max_length=64, device=None):
    """
    æ‰¹é‡ç¿»è¯‘å‡½æ•°
    """
    translations = []
    
    for sentence in input_sentences:
        try:
            translation = translate(
                sentence, transformer, pt_tokenizer, en_tokenizer,
                max_length=max_length, device=device
            )
            translations.append(translation)
        except Exception as e:
            logger.error(f"ç¿»è¯‘å¤±è´¥: {sentence}, é”™è¯¯: {e}")
            translations.append("ç¿»è¯‘å¤±è´¥")
    
    return translations

def evaluate_translation_quality(translations, references=None):
    """
    è¯„ä¼°ç¿»è¯‘è´¨é‡ï¼ˆç®€å•çš„BLEUè®¡ç®—ï¼‰
    """
    if references is None:
        print("ğŸ“Š ç¿»è¯‘ç»“æœ:")
        for i, trans in enumerate(translations):
            print(f"   {i+1}. {trans}")
        return
    
    # ç®€å•çš„BLEUè®¡ç®—ï¼ˆè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…åº”è¯¥ä½¿ç”¨æ›´å¤æ‚çš„BLEUè®¡ç®—ï¼‰
    print("ğŸ“Š ç¿»è¯‘è´¨é‡è¯„ä¼°:")
    for i, (trans, ref) in enumerate(zip(translations, references)):
        print(f"   {i+1}. ç¿»è¯‘: {trans}")
        print(f"      å‚è€ƒ: {ref}")
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„BLEUè®¡ç®—
        print()

def analyze_attention_patterns(attention_weights, layer_name="decoder_layer1_att2"):
    """
    åˆ†ææ³¨æ„åŠ›æ¨¡å¼
    """
    if layer_name not in attention_weights:
        print(f"âš ï¸ æœªæ‰¾åˆ°æ³¨æ„åŠ›å±‚: {layer_name}")
        return
    
    attn = attention_weights[layer_name].squeeze(0)  # [num_heads, tgt_len, src_len]
    attn = attn.detach().cpu().numpy()
    
    print(f"ğŸ“Š æ³¨æ„åŠ›æ¨¡å¼åˆ†æ ({layer_name}):")
    print(f"   æ³¨æ„åŠ›å¤´æ•°: {attn.shape[0]}")
    print(f"   ç›®æ ‡é•¿åº¦: {attn.shape[1]}")
    print(f"   æºé•¿åº¦: {attn.shape[2]}")
    
    # åˆ†ææ¯ä¸ªå¤´çš„æ³¨æ„åŠ›åˆ†å¸ƒ
    for head in range(attn.shape[0]):
        head_attn = attn[head]
        max_attn = np.max(head_attn)
        mean_attn = np.mean(head_attn)
        print(f"   å¤´ {head+1}: æœ€å¤§æ³¨æ„åŠ›={max_attn:.4f}, å¹³å‡æ³¨æ„åŠ›={mean_attn:.4f}")

def create_mock_tokenizers():
    """åˆ›å»ºæ¨¡æ‹Ÿçš„tokenizerç”¨äºæ¼”ç¤º"""
    class MockTokenizer:
        def __init__(self, vocab_size, pad_token_id=0, bos_token_id=1, eos_token_id=2):
            self.vocab_size = vocab_size
            self.pad_token_id = pad_token_id
            self.bos_token_id = bos_token_id
            self.eos_token_id = eos_token_id
        
        def encode(self, text, add_special_tokens=False):
            # ç®€å•çš„æ¨¡æ‹Ÿç¼–ç 
            words = text.split()
            ids = [hash(word) % (self.vocab_size - 10) + 10 for word in words]
            return ids
        
        def decode(self, ids, skip_special_tokens=True):
            # ç®€å•çš„æ¨¡æ‹Ÿè§£ç 
            if skip_special_tokens:
                ids = [i for i in ids if i >= 10]
            return " ".join([f"word_{i}" for i in ids])
    
    pt_tokenizer = MockTokenizer(8192, 0, 1, 2)
    en_tokenizer = MockTokenizer(8192, 0, 1, 2)
    
    return pt_tokenizer, en_tokenizer

if __name__ == "__main__":
    print("=" * 60)
    print("Step 9: æ¨¡å‹è¯„ä¼°å’Œç¿»è¯‘æµ‹è¯•")
    print("=" * 60)
    
    # å‚æ•°è®¾ç½®
    max_length = 30
    d_model = 128
    num_layers = 4
    num_heads = 8
    dff = 512
    dropout_rate = 0.25
    
    # è¯è¡¨å¤§å°
    input_vocab_size = 8192
    target_vocab_size = 8192
    
    print(f"ğŸ”§ è¯„ä¼°å‚æ•°:")
    print(f"   æœ€å¤§ç”Ÿæˆé•¿åº¦: {max_length}")
    print(f"   æ¨¡å‹ç»´åº¦: {d_model}")
    print(f"   æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
    
    try:
        # 1. åˆ›å»ºæ¨¡å‹
        print(f"\nğŸ”¨ åˆ›å»ºTransformeræ¨¡å‹...")
        model = Transformer(
            num_layers=num_layers,
            input_vocab_size=input_vocab_size,
            target_vocab_size=target_vocab_size,
            max_length=max_length,
            d_model=d_model,
            num_heads=num_heads,
            dff=dff,
            rate=dropout_rate,
            src_padding_idx=0,
            tgt_padding_idx=0,
        )
        
        # 2. åˆ›å»ºæ¨¡æ‹Ÿtokenizer
        print(f"\nğŸ”¨ åˆ›å»ºæ¨¡æ‹Ÿtokenizer...")
        pt_tokenizer, en_tokenizer = create_mock_tokenizers()
        
        # 3. è®¾ç½®è®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)
        model.eval()
        
        print(f"   ä½¿ç”¨è®¾å¤‡: {device}")
        
        # 4. æµ‹è¯•ç¿»è¯‘
        print(f"\nğŸŒ æµ‹è¯•ç¿»è¯‘åŠŸèƒ½...")
        test_sentences = [
            "OlÃ¡, como vocÃª estÃ¡?",
            "Obrigado pela sua ajuda.",
            "Este Ã© um teste de traduÃ§Ã£o.",
        ]
        
        translations = batch_translate(
            test_sentences, model, pt_tokenizer, en_tokenizer,
            max_length=max_length, device=device
        )
        
        evaluate_translation_quality(translations)
        
        # 5. æµ‹è¯•å•ä¸ªç¿»è¯‘å¹¶åˆ†ææ³¨æ„åŠ›
        print(f"\nğŸ” åˆ†ææ³¨æ„åŠ›æ¨¡å¼...")
        test_sentence = "OlÃ¡ mundo"
        
        result, attention_weights = evaluate(
            inp_sentence=test_sentence,
            transformer=model,
            pt_tokenizer=pt_tokenizer,
            en_tokenizer=en_tokenizer,
            max_length=max_length,
            device=device,
        )
        
        predicted_sentence = en_tokenizer.decode(
            [i for i in result if i < en_tokenizer.vocab_size],
            skip_special_tokens=True
        )
        
        print(f"   è¾“å…¥: {test_sentence}")
        print(f"   è¾“å‡º: {predicted_sentence}")
        
        # åˆ†ææ³¨æ„åŠ›æ¨¡å¼
        if attention_weights:
            analyze_attention_patterns(attention_weights, "decoder_layer1_att2")
        
        # 6. å¯è§†åŒ–æ³¨æ„åŠ›ï¼ˆå¦‚æœæœ‰matplotlibæ”¯æŒï¼‰
        print(f"\nğŸ“Š å¯è§†åŒ–æ³¨æ„åŠ›...")
        try:
            if attention_weights and "decoder_layer1_att2" in attention_weights:
                plot_encoder_decoder_attention(
                    attention_weights, test_sentence, result, "decoder_layer1_att2",
                    pt_tokenizer, en_tokenizer
                )
                print("   æ³¨æ„åŠ›å¯è§†åŒ–å®Œæˆ")
            else:
                print("   æ— æ³•è¿›è¡Œæ³¨æ„åŠ›å¯è§†åŒ–ï¼ˆç¼ºå°‘æ³¨æ„åŠ›æƒé‡ï¼‰")
        except Exception as e:
            print(f"   æ³¨æ„åŠ›å¯è§†åŒ–å¤±è´¥: {e}")
        
        # 7. æ¨¡å‹æ€§èƒ½åˆ†æ
        print(f"\nğŸ“ˆ æ¨¡å‹æ€§èƒ½åˆ†æ...")
        
        # è®¡ç®—æ¨¡å‹å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"   æ€»å‚æ•°æ•°é‡: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        # æµ‹è¯•æ¨ç†é€Ÿåº¦
        import time
        start_time = time.time()
        for _ in range(10):
            _ = evaluate(
                inp_sentence=test_sentence,
                transformer=model,
                pt_tokenizer=pt_tokenizer,
                en_tokenizer=en_tokenizer,
                max_length=max_length,
                device=device,
            )
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 10
        print(f"   å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.4f}ç§’")
        
        print(f"\nâœ… æ¨¡å‹è¯„ä¼°å’Œç¿»è¯‘æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥æ¨¡å‹å’Œå‚æ•°è®¾ç½®æ˜¯å¦æ­£ç¡®")

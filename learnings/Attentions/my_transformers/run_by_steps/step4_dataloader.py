# -*- coding: utf-8 -*-
"""
Step 4: æ„å»ºå’Œæµ‹è¯•DataLoader
æ„å»ºè®­ç»ƒå’ŒéªŒè¯ DataLoaderï¼ˆç­‰ä»· TF çš„ filter_by_max_length + padded_batchï¼‰
"""

import os
import torch
from datasets import load_dataset
from tokenizers import ByteLevelBPETokenizer
from transformers import PreTrainedTokenizerFast
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from loguru import logger

def load_translation_dataset(train_path: str, val_path: str, delimiter: str = "\t"):
    """åŠ è½½æ•°æ®é›†"""
    dataset = load_dataset(
        "csv",
        data_files={
            "train": train_path,
            "validation": val_path
        },
        column_names=["pt", "en"],
        delimiter=delimiter
    )
    return dataset["train"], dataset["validation"]

def train_and_load_tokenizers(
    train_dataset,
    pt_key="pt",
    en_key="en",
    vocab_size=2 ** 13,
    min_freq=2,
    special_tokens=None,
    save_dir_pt="tok_pt",
    save_dir_en="tok_en",
    max_length=1024
):
    """è®­ç»ƒå¹¶åŠ è½½tokenizer"""
    if special_tokens is None:
        special_tokens = ["<s>", "<pad>", "</s>", "<unk>", "<mask>"]

    def iter_lang(ds, key):
        for ex in ds:
            txt = ex[key]
            if isinstance(txt, bytes):
                txt = txt.decode("utf-8")
            yield txt

    # åˆå§‹åŒ– tokenizer
    pt_bbpe = ByteLevelBPETokenizer(add_prefix_space=True)
    en_bbpe = ByteLevelBPETokenizer(add_prefix_space=True)

    # è®­ç»ƒ tokenizer
    pt_bbpe.train_from_iterator(
        iter_lang(train_dataset, pt_key),
        vocab_size=vocab_size,
        min_frequency=min_freq,
        special_tokens=special_tokens,
    )
    
    en_bbpe.train_from_iterator(
        iter_lang(train_dataset, en_key),
        vocab_size=vocab_size,
        min_frequency=min_freq,
        special_tokens=special_tokens,
    )

    # ä¿å­˜
    Path(save_dir_pt).mkdir(exist_ok=True)
    Path(save_dir_en).mkdir(exist_ok=True)
    pt_bbpe.save_model(save_dir_pt)
    en_bbpe.save_model(save_dir_en)
    pt_bbpe._tokenizer.save(f"{save_dir_pt}/tokenizer.json")
    en_bbpe._tokenizer.save(f"{save_dir_en}/tokenizer.json")

    # åŠ è½½
    pt_tokenizer = PreTrainedTokenizerFast(tokenizer_file=f"{save_dir_pt}/tokenizer.json")
    en_tokenizer = PreTrainedTokenizerFast(tokenizer_file=f"{save_dir_en}/tokenizer.json")

    # è®¾ç½®ç‰¹æ®Šç¬¦å·
    for tok in (pt_tokenizer, en_tokenizer):
        tok.pad_token = "<pad>"
        tok.unk_token = "<unk>"
        tok.bos_token = "<s>"
        tok.eos_token = "</s>"
        tok.mask_token = "<mask>"
        tok.model_max_length = max_length
        tok.padding_side = "right"

    return pt_tokenizer, en_tokenizer

def build_dataloaders(
    train_dataset,
    val_dataset,
    pt_tokenizer,
    en_tokenizer,
    batch_size: int = 64,
    max_length: int = 48,
    num_workers: int = 0,
    shuffle_train: bool = True,
):
    """
    æ„å»ºè®­ç»ƒå’ŒéªŒè¯ DataLoaderï¼ˆç­‰ä»· TF çš„ filter_by_max_length + padded_batchï¼‰

    å‚æ•°:
        train_dataset: HuggingFace Dataset (è®­ç»ƒé›†)
        val_dataset: HuggingFace Dataset (éªŒè¯é›†)
        pt_tokenizer: è‘¡è¯­ tokenizer (æºè¯­è¨€)
        en_tokenizer: è‹±è¯­ tokenizer (ç›®æ ‡è¯­è¨€)
        batch_size: æ‰¹å¤§å°
        max_length: æ ·æœ¬æœ€å¤§é•¿åº¦ï¼ˆè¶…è¿‡åˆ™è¿‡æ»¤ï¼‰
        num_workers: DataLoader worker æ•°é‡
        shuffle_train: æ˜¯å¦æ‰“ä¹±è®­ç»ƒé›†

    è¿”å›:
        train_loader, val_loader
    """

    # 1) å°å·¥å…·ï¼šç¼–ç  + æ·»åŠ  BOS/EOS
    def encode_with_bos_eos(tokenizer, text: str):
        ids = tokenizer.encode(text, add_special_tokens=False)
        bos_id = tokenizer.bos_token_id
        eos_id = tokenizer.eos_token_id
        if bos_id is None or eos_id is None:
            raise ValueError("è¯·ç¡®ä¿ tokenizer è®¾ç½®äº† bos_token/eos_token")
        return [bos_id] + ids + [eos_id]

    # 2) æ„é€ å·²è¿‡æ»¤çš„æ ·æœ¬å¯¹
    def build_filtered_pairs(hf_split, pt_tok, en_tok, max_len: int):
        pairs, kept, skipped = [], 0, 0
        # pairs[1] = ([0, 88, 1290, 304, 740, 4916, 304, 6351, 430, 290, 335, 430, 390, 2], [0, 72, 341, 352, 2051, 2993, 286, 6266, 377, 2])
        for ex in hf_split:
            """ex = {'pt': 'os meus alunos tÃªm problemas , problemas sociais , emocionais e econÃ³micos , que vocÃªs nem podem imaginar .', 
                     'en': 'my students have problems : social , emotional and economic problems you could never imagine .'}
            """
            pt_ids = encode_with_bos_eos(pt_tok, ex["pt"])
            en_ids = encode_with_bos_eos(en_tok, ex["en"])
            if len(pt_ids) <= max_len and len(en_ids) <= max_len:
                pairs.append((pt_ids, en_ids));
                kept += 1
            else:
                skipped += 1
        # [filter] kept=38790, skipped=12996, max_length=30
        logger.info(f"[filter] kept={kept}, skipped={skipped}, max_length={max_len}")
        return pairs

    train_pairs = build_filtered_pairs(train_dataset, pt_tokenizer, en_tokenizer, max_length)
    val_pairs = build_filtered_pairs(val_dataset, pt_tokenizer, en_tokenizer, max_length)

    # 3) Dataset ç±»
    class PairsDataset(Dataset):
        def __init__(self, pairs):
            self.pairs = pairs

        def __len__(self):
            return len(self.pairs)

        def __getitem__(self, idx):
            pt_ids, en_ids = self.pairs[idx]
            return {"pt_input_ids": pt_ids, "en_input_ids": en_ids}

    # 4) Collate å‡½æ•°ï¼ˆåŠ¨æ€ paddingï¼‰
    def collate_padded(batch, pad_id_pt: int, pad_id_en: int):
        """
        æ•°æ®æ•´ç†å‡½æ•°ï¼Œç”¨äºDataLoaderçš„collate_fnï¼Œå°†ä¸€æ‰¹æ ·æœ¬åŠ¨æ€å¡«å……åˆ°ç›¸åŒé•¿åº¦

        å‚æ•°:
            batch: ä¸€æ‰¹æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æ˜¯åŒ…å«'pt_input_ids'å’Œ'en_input_ids'çš„å­—å…¸
            pad_id_pt: è‘¡è„ç‰™è¯­çš„å¡«å……token id
            pad_id_en: è‹±è¯­çš„å¡«å……token id

        è¿”å›:
            åŒ…å«å››ä¸ªå¼ é‡çš„å­—å…¸:
            - pt_input_ids: è‘¡è„ç‰™è¯­è¾“å…¥IDï¼Œå½¢çŠ¶(batch_size, max_pt_len)
            - pt_attention_mask: è‘¡è„ç‰™è¯­æ³¨æ„åŠ›æ©ç ï¼Œå½¢çŠ¶(batch_size, max_pt_len)
            - en_input_ids: è‹±è¯­è¾“å…¥IDï¼Œå½¢çŠ¶(batch_size, max_en_len)
            - en_attention_mask: è‹±è¯­æ³¨æ„åŠ›æ©ç ï¼Œå½¢çŠ¶(batch_size, max_en_len)

        ç¤ºä¾‹:
            è¾“å…¥batch = [
                {'pt_input_ids': [1, 2, 3], 'en_input_ids': [4, 5]},
                {'pt_input_ids': [6, 7], 'en_input_ids': [8, 9, 10, 11]}
            ]

            è¾“å‡º:
            {
                'pt_input_ids': [[1, 2, 3, 0], [6, 7, 0, 0]],      # è‘¡è„ç‰™è¯­å¡«å……
                'pt_attention_mask': [[1, 1, 1, 0], [1, 1, 0, 0]], # è‘¡è„ç‰™è¯­æ©ç 
                'en_input_ids': [[4, 5, 0, 0], [8, 9, 10, 11]],    # è‹±è¯­å¡«å……
                'en_attention_mask': [[1, 1, 0, 0], [1, 1, 1, 1]]  # è‹±è¯­æ©ç 
            }
        """
        def pad_block(seqs, pad_value):
            """
            å°†å˜é•¿åºåˆ—å¡«å……åˆ°ç›¸åŒé•¿åº¦ï¼Œå¹¶ç”Ÿæˆå¯¹åº”çš„æ³¨æ„åŠ›æ©ç 

            å‚æ•°:
                seqs: åºåˆ—åˆ—è¡¨ï¼Œæ¯ä¸ªåºåˆ—æ˜¯æ•´æ•°åˆ—è¡¨
                pad_value: å¡«å……å€¼ï¼ˆé€šå¸¸æ˜¯tokenizer.pad_token_idï¼‰

            è¿”å›:
                padded_tensor: å¡«å……åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, max_len)
                attention_mask: æ³¨æ„åŠ›æ©ç ï¼Œ1è¡¨ç¤ºæœ‰æ•ˆtokenï¼Œ0è¡¨ç¤ºå¡«å……ä½ç½®

            ç¤ºä¾‹:
                è¾“å…¥: [[1, 2, 3], [4, 5]], pad_value=0
                è¾“å‡º:
                    padded_tensor = [[1, 2, 3],
                                    [4, 5, 0]]
                    attention_mask = [[1, 1, 1],
                                     [1, 1, 0]]
            """
            # æ‰¾åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—é•¿åº¦
            max_len = max(len(s) for s in seqs)

            # åˆ›å»ºå¡«å……å¼ é‡ï¼Œç”¨pad_valueå¡«å……æ‰€æœ‰ä½ç½®
            out = torch.full((len(seqs), max_len), pad_value, dtype=torch.long)
            # åˆ›å»ºæ³¨æ„åŠ›æ©ç ï¼Œåˆå§‹ä¸º0ï¼ˆè¡¨ç¤ºå¡«å……ä½ç½®ï¼‰
            attn = torch.zeros((len(seqs), max_len), dtype=torch.long)

            # éå†æ¯ä¸ªåºåˆ—ï¼Œå¡«å……å®é™…æ•°æ®å¹¶è®¾ç½®æ³¨æ„åŠ›æ©ç 
            for i, s in enumerate(seqs):
                L = len(s)  # å½“å‰åºåˆ—çš„å®é™…é•¿åº¦
                out[i, :L] = torch.tensor(s, dtype=torch.long)  # å¡«å……å®é™…token id
                attn[i, :L] = 1  # å‰Lä¸ªä½ç½®è®¾ä¸º1ï¼ˆæœ‰æ•ˆtokenï¼‰

            return out, attn

        pt_ids_list = [ex["pt_input_ids"] for ex in batch]
        # en_ids_list[1] = [0, 6295, 268, 1167, 464, 334, 861, 291, 268, 406, 464, 822, 286, 728, 1223, 1863, 267, 2]
        en_ids_list = [ex["en_input_ids"] for ex in batch]

        pt_input_ids, pt_attention_mask = pad_block(pt_ids_list, pt_tokenizer.pad_token_id)

        # en_input_ids[1] = tensor([   0, 6295,  268, 1167,  464,  334,  861,  291,  268,  406,  464,  822,
        #          286,  728, 1223, 1863,  267,    2,    1,    1,    1,    1,    1,    1,
        #            1,    1,    1,    1])
        # en_attention_mask[1] = tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        #         0, 0, 0, 0])
        en_input_ids, en_attention_mask = pad_block(en_ids_list, en_tokenizer.pad_token_id)

        return {
            "pt_input_ids": pt_input_ids,
            "pt_attention_mask": pt_attention_mask,
            "en_input_ids": en_input_ids,
            "en_attention_mask": en_attention_mask,
        }

    # 5) DataLoader
    train_loader = DataLoader(
        PairsDataset(train_pairs),
        batch_size=batch_size,
        shuffle=shuffle_train,
        collate_fn=lambda b: collate_padded(b, pt_tokenizer.pad_token_id, en_tokenizer.pad_token_id),
        num_workers=num_workers,
    )
    val_loader = DataLoader(
        PairsDataset(val_pairs),
        batch_size=batch_size,
        shuffle=False,
        collate_fn=lambda b: collate_padded(b, pt_tokenizer.pad_token_id, en_tokenizer.pad_token_id),
        num_workers=num_workers,
    )

    return train_loader, val_loader

def test_dataloaders(train_loader, val_loader, show_val: bool = True):
    """
    æ£€æŸ¥ DataLoader çš„ batch è¾“å‡ºï¼Œæ‰“å°å¼ é‡å½¢çŠ¶å’Œä¸€ä¸ªç¤ºä¾‹

    å‚æ•°:
        train_loader: è®­ç»ƒ DataLoader
        val_loader: éªŒè¯ DataLoader
        show_val: æ˜¯å¦å±•ç¤ºéªŒè¯é›†çš„ä¸€ä¸ªæ ·æœ¬ï¼ˆé»˜è®¤ Trueï¼‰
    """
    # 1. æ‹¿ä¸€ä¸ªè®­ç»ƒ batch çœ‹ shape
    batch = next(iter(train_loader))
    logger.info("=== Train Loader Batch Shapes ===")
    for k, v in batch.items():
        logger.info(f"{k:20s} {tuple(v.shape)}")

    # 2. éªŒè¯é›†æ ·æœ¬
    if show_val:
        logger.info("\n=== Validation Loader Example ===")
        for i in val_loader:
            logger.info("pt_input_ids:     ", i["pt_input_ids"][0])
            logger.info("pt_attention_mask:", i["pt_attention_mask"][0])
            break

if __name__ == "__main__":
    print("=" * 60)
    print("Step 4: æ„å»ºå’Œæµ‹è¯•DataLoader")
    print("=" * 60)
    
    # æ•°æ®æ–‡ä»¶åœ°å€
    train_path = "/data2/workspace/yszhang/train_transformers/tensorflow_datasets/ted_pt_en_train.csv"
    val_path = "/data2/workspace/yszhang/train_transformers/tensorflow_datasets/ted_pt_en_test.csv"
    
    # æ„å»ºè¯è¡¨å‚æ•°
    vocab_size = 2 ** 13  # è¯è¡¨å¤§å°
    min_freq = 2  # æœ€å°è¯é¢‘
    special_tokens = ["<s>", "<pad>", "</s>", "<unk>", "<mask>"]  # ç‰¹æ®Šç¬¦å·
    max_length = 30  # æœ€å¤§åºåˆ—é•¿åº¦
    
    # DataLoaderå‚æ•°
    batch_size = 32  # æ‰¹å¤„ç†æ•°
    
    print(f"ğŸ”§ DataLoaderå‚æ•°:")
    print(f"   æ‰¹å¤§å°: {batch_size}")
    print(f"   æœ€å¤§åºåˆ—é•¿åº¦: {max_length}")
    
    try:
        # 1. åŠ è½½æ•°æ®é›†
        print(f"\nğŸ“ åŠ è½½æ•°æ®é›†...")
        train_dataset, val_dataset = load_translation_dataset(
            train_path=train_path,
            val_path=val_path
        )
        
        # 2. æ„å»º Tokenizer
        print(f"\nğŸ”¨ æ„å»º Tokenizer...")
        pt_tokenizer, en_tokenizer = train_and_load_tokenizers(
            train_dataset=train_dataset,
            pt_key="pt",
            en_key="en",
            vocab_size=vocab_size,
            min_freq=min_freq,
            special_tokens=special_tokens,
            save_dir_pt="tok_pt",
            save_dir_en="tok_en",
            max_length=max_length
        )
        
        # 3. æ„å»º DataLoader
        print(f"\nğŸ”¨ æ„å»º DataLoader...")
        train_loader, val_loader = build_dataloaders(
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            pt_tokenizer=pt_tokenizer,
            en_tokenizer=en_tokenizer,
            batch_size=batch_size,
            max_length=max_length,
            num_workers=0,
            shuffle_train=True
        )
        
        print(f"\nâœ… DataLoaderæ„å»ºå®Œæˆï¼")
        print(f"ğŸ“Š è®­ç»ƒé›†æ‰¹æ¬¡æ•°: {len(train_loader)}")
        print(f"ğŸ“Š éªŒè¯é›†æ‰¹æ¬¡æ•°: {len(val_loader)}")
        
        # 4. æµ‹è¯• DataLoader
        print(f"\nğŸ§ª æµ‹è¯• DataLoader...")
        test_dataloaders(train_loader, val_loader)
        
        print(f"\nâœ… DataLoaderæµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ DataLoaderæ„å»ºå¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")

"""
(train_transformers) root@iv-ydg6wcq3ggay8n6dmn75:/data2/workspace/yszhang/train_transformers/run_by_steps/run_by_steps# python step4_dataloader.py 
============================================================
Step 4: æ„å»ºå’Œæµ‹è¯•DataLoader
============================================================
ğŸ”§ DataLoaderå‚æ•°:
   æ‰¹å¤§å°: 32
   æœ€å¤§åºåˆ—é•¿åº¦: 30

ğŸ“ åŠ è½½æ•°æ®é›†...

ğŸ”¨ æ„å»º Tokenizer...
[00:00:00] Pre-processing sequences       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0        /        0
[00:00:00] Tokenize words                 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 38341    /    38341
[00:00:00] Count pairs                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 38341    /    38341
[00:00:02] Compute merges                 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 7931     /     7931
[00:00:00] Pre-processing sequences       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0        /        0
[00:00:00] Tokenize words                 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 27827    /    27827
[00:00:00] Count pairs                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 27827    /    27827
[00:00:02] Compute merges                 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 7931     /     7931

ğŸ”¨ æ„å»º DataLoader...
Token indices sequence length is longer than the specified maximum sequence length for this model (38 > 30). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (31 > 30). Running this sequence through the model will result in indexing errors
2025-10-04 20:13:10.172 | INFO     | __main__:build_filtered_pairs:138 - [filter] kept=38790, skipped=12996, max_length=30
2025-10-04 20:13:10.173 | INFO     | __main__:build_filtered_pairs:139 - ex = {'pt': 'os meus alunos tÃªm problemas , problemas sociais , emocionais e econÃ³micos , que vocÃªs nem podem imaginar .', 'en': 'my students have problems : social , emotional and economic problems you could never imagine .'}
2025-10-04 20:13:10.173 | INFO     | __main__:build_filtered_pairs:140 - pairs[1] = ([0, 73, 480, 1218, 1636, 262, 2350, 267, 512, 1636, 262, 1614, 5644, 356, 4592, 267, 282, 311, 262, 320, 594, 422, 84, 445, 268, 2], [0, 477, 468, 309, 2415, 2499, 1846, 268, 309, 577, 688, 1106, 266, 425, 4672, 295, 2888, 268, 566, 317, 365, 270, 275, 718, 437, 267, 2])
2025-10-04 20:13:10.377 | INFO     | __main__:build_filtered_pairs:138 - [filter] kept=942, skipped=252, max_length=30
2025-10-04 20:13:10.377 | INFO     | __main__:build_filtered_pairs:139 - ex = {'pt': 'mas , como todos os cientistas , ela sabia , que , para deixar a sua marca , o que precisava fazer era encontrar um problema difÃ­cil e resolvÃª-lo .', 'en': 'but like every scientist , she appreciated that to make her mark , what she needed to do was find a hard problem and solve it .'}
2025-10-04 20:13:10.377 | INFO     | __main__:build_filtered_pairs:140 - pairs[1] = ([0, 88, 1290, 304, 740, 4916, 304, 6351, 430, 290, 335, 430, 390, 2], [0, 72, 341, 352, 2051, 2993, 286, 6266, 377, 2])

âœ… DataLoaderæ„å»ºå®Œæˆï¼
ğŸ“Š è®­ç»ƒé›†æ‰¹æ¬¡æ•°: 1213
ğŸ“Š éªŒè¯é›†æ‰¹æ¬¡æ•°: 30

ğŸ§ª æµ‹è¯• DataLoader...
2025-10-04 20:13:10.399 | INFO     | __main__:collate_padded:170 - en_ids_list[1] = [0, 6295, 268, 1167, 464, 334, 861, 291, 268, 406, 464, 822, 286, 728, 1223, 1863, 267, 2]
2025-10-04 20:13:10.402 | INFO     | __main__:collate_padded:173 - en_input_ids[1] = tensor([   0, 6295,  268, 1167,  464,  334,  861,  291,  268,  406,  464,  822,
         286,  728, 1223, 1863,  267,    2,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1])
2025-10-04 20:13:10.402 | INFO     | __main__:collate_padded:174 - en_attention_mask[1] = tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
2025-10-04 20:13:10.402 | INFO     | __main__:test_dataloaders:211 - === Train Loader Batch Shapes ===
2025-10-04 20:13:10.403 | INFO     | __main__:test_dataloaders:213 - pt_input_ids         (32, 29)
2025-10-04 20:13:10.403 | INFO     | __main__:test_dataloaders:213 - pt_attention_mask    (32, 29)
2025-10-04 20:13:10.403 | INFO     | __main__:test_dataloaders:213 - en_input_ids         (32, 28)
2025-10-04 20:13:10.403 | INFO     | __main__:test_dataloaders:213 - en_attention_mask    (32, 28)
2025-10-04 20:13:10.403 | INFO     | __main__:test_dataloaders:217 - 
=== Validation Loader Example ===
2025-10-04 20:13:10.403 | INFO     | __main__:collate_padded:170 - en_ids_list[1] = [0, 72, 341, 352, 2051, 2993, 286, 6266, 377, 2]
2025-10-04 20:13:10.404 | INFO     | __main__:collate_padded:173 - en_input_ids[1] = tensor([   0,   72,  341,  352, 2051, 2993,  286, 6266,  377,    2,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1])
2025-10-04 20:13:10.404 | INFO     | __main__:collate_padded:174 - en_attention_mask[1] = tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0])
2025-10-04 20:13:10.404 | INFO     | __main__:test_dataloaders:219 - pt_input_ids:     
2025-10-04 20:13:10.404 | INFO     | __main__:test_dataloaders:220 - pt_attention_mask:

âœ… DataLoaderæµ‹è¯•å®Œæˆï¼
(train_transformers) root@iv-ydg6wcq3ggay8n6dmn75:/data2/workspace/yszhang/train_transformers/run_by_steps/run_by_steps# 
(train_transformers) root@iv-ydg6wcq3ggay8n6dmn75:/data2/workspace/yszhang/train_transformers/run_by_steps/run_by_steps# 
"""
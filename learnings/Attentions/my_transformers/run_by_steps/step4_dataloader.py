# -*- coding: utf-8 -*-
"""
Step 4: æ„å»ºå’Œæµ‹è¯•DataLoader
æ„å»ºè®­ç»ƒå’ŒéªŒè¯ DataLoaderï¼ˆç­‰ä»· TF çš„ filter_by_max_length + padded_batchï¼‰
"""

import os
import torch
from datasets import load_dataset
from tokenizers import ByteLevelBPETokenizer
from transformers import PreTrainedTokenizerFast
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from loguru import logger

def load_translation_dataset(train_path: str, val_path: str, delimiter: str = "\t"):
    """åŠ è½½æ•°æ®é›†"""
    dataset = load_dataset(
        "csv",
        data_files={
            "train": train_path,
            "validation": val_path
        },
        column_names=["pt", "en"],
        delimiter=delimiter
    )
    return dataset["train"], dataset["validation"]

def train_and_load_tokenizers(
    train_dataset,
    pt_key="pt",
    en_key="en",
    vocab_size=2 ** 13,
    min_freq=2,
    special_tokens=None,
    save_dir_pt="tok_pt",
    save_dir_en="tok_en",
    max_length=1024
):
    """è®­ç»ƒå¹¶åŠ è½½tokenizer"""
    if special_tokens is None:
        special_tokens = ["<s>", "<pad>", "</s>", "<unk>", "<mask>"]

    def iter_lang(ds, key):
        for ex in ds:
            txt = ex[key]
            if isinstance(txt, bytes):
                txt = txt.decode("utf-8")
            yield txt

    # åˆå§‹åŒ– tokenizer
    pt_bbpe = ByteLevelBPETokenizer(add_prefix_space=True)
    en_bbpe = ByteLevelBPETokenizer(add_prefix_space=True)

    # è®­ç»ƒ tokenizer
    pt_bbpe.train_from_iterator(
        iter_lang(train_dataset, pt_key),
        vocab_size=vocab_size,
        min_frequency=min_freq,
        special_tokens=special_tokens,
    )
    
    en_bbpe.train_from_iterator(
        iter_lang(train_dataset, en_key),
        vocab_size=vocab_size,
        min_frequency=min_freq,
        special_tokens=special_tokens,
    )

    # ä¿å­˜
    Path(save_dir_pt).mkdir(exist_ok=True)
    Path(save_dir_en).mkdir(exist_ok=True)
    pt_bbpe.save_model(save_dir_pt)
    en_bbpe.save_model(save_dir_en)
    pt_bbpe._tokenizer.save(f"{save_dir_pt}/tokenizer.json")
    en_bbpe._tokenizer.save(f"{save_dir_en}/tokenizer.json")

    # åŠ è½½
    pt_tokenizer = PreTrainedTokenizerFast(tokenizer_file=f"{save_dir_pt}/tokenizer.json")
    en_tokenizer = PreTrainedTokenizerFast(tokenizer_file=f"{save_dir_en}/tokenizer.json")

    # è®¾ç½®ç‰¹æ®Šç¬¦å·
    for tok in (pt_tokenizer, en_tokenizer):
        tok.pad_token = "<pad>"
        tok.unk_token = "<unk>"
        tok.bos_token = "<s>"
        tok.eos_token = "</s>"
        tok.mask_token = "<mask>"
        tok.model_max_length = max_length
        tok.padding_side = "right"

    return pt_tokenizer, en_tokenizer

def build_dataloaders(
    train_dataset,
    val_dataset,
    pt_tokenizer,
    en_tokenizer,
    batch_size: int = 64,
    max_length: int = 48,
    num_workers: int = 0,
    shuffle_train: bool = True,
):
    """
    æ„å»ºè®­ç»ƒå’ŒéªŒè¯ DataLoaderï¼ˆç­‰ä»· TF çš„ filter_by_max_length + padded_batchï¼‰

    å‚æ•°:
        train_dataset: HuggingFace Dataset (è®­ç»ƒé›†)
        val_dataset: HuggingFace Dataset (éªŒè¯é›†)
        pt_tokenizer: è‘¡è¯­ tokenizer (æºè¯­è¨€)
        en_tokenizer: è‹±è¯­ tokenizer (ç›®æ ‡è¯­è¨€)
        batch_size: æ‰¹å¤§å°
        max_length: æ ·æœ¬æœ€å¤§é•¿åº¦ï¼ˆè¶…è¿‡åˆ™è¿‡æ»¤ï¼‰
        num_workers: DataLoader worker æ•°é‡
        shuffle_train: æ˜¯å¦æ‰“ä¹±è®­ç»ƒé›†

    è¿”å›:
        train_loader, val_loader
    """

    # 1) å°å·¥å…·ï¼šç¼–ç  + æ·»åŠ  BOS/EOS
    def encode_with_bos_eos(tokenizer, text: str):
        ids = tokenizer.encode(text, add_special_tokens=False)
        bos_id = tokenizer.bos_token_id
        eos_id = tokenizer.eos_token_id
        if bos_id is None or eos_id is None:
            raise ValueError("è¯·ç¡®ä¿ tokenizer è®¾ç½®äº† bos_token/eos_token")
        return [bos_id] + ids + [eos_id]

    # 2) æ„é€ å·²è¿‡æ»¤çš„æ ·æœ¬å¯¹
    def build_filtered_pairs(hf_split, pt_tok, en_tok, max_len: int):
        pairs, kept, skipped = [], 0, 0
        # pairs[1] = ([0, 88, 1290, 304, 740, 4916, 304, 6351, 430, 290, 335, 430, 390, 2], [0, 72, 341, 352, 2051, 2993, 286, 6266, 377, 2])
        for ex in hf_split:
            """ex = {'pt': 'os meus alunos tÃªm problemas , problemas sociais , emocionais e econÃ³micos , que vocÃªs nem podem imaginar .', 
                     'en': 'my students have problems : social , emotional and economic problems you could never imagine .'}
            """
            pt_ids = encode_with_bos_eos(pt_tok, ex["pt"])
            en_ids = encode_with_bos_eos(en_tok, ex["en"])
            if len(pt_ids) <= max_len and len(en_ids) <= max_len:
                pairs.append((pt_ids, en_ids));
                kept += 1
            else:
                skipped += 1
        logger.info(f"[filter] kept={kept}, skipped={skipped}, max_length={max_len}")
        return pairs

    train_pairs = build_filtered_pairs(train_dataset, pt_tokenizer, en_tokenizer, max_length)
    val_pairs = build_filtered_pairs(val_dataset, pt_tokenizer, en_tokenizer, max_length)

    # 3) Dataset ç±»
    class PairsDataset(Dataset):
        def __init__(self, pairs):
            self.pairs = pairs

        def __len__(self):
            return len(self.pairs)

        def __getitem__(self, idx):
            pt_ids, en_ids = self.pairs[idx]
            return {"pt_input_ids": pt_ids, "en_input_ids": en_ids}

    # 4) Collate å‡½æ•°ï¼ˆåŠ¨æ€ paddingï¼‰
    def collate_padded(batch, pad_id_pt: int, pad_id_en: int):
        def pad_block(seqs, pad_value):
            max_len = max(len(s) for s in seqs)
            out = torch.full((len(seqs), max_len), pad_value, dtype=torch.long)
            attn = torch.zeros((len(seqs), max_len), dtype=torch.long)
            for i, s in enumerate(seqs):
                L = len(s)
                out[i, :L] = torch.tensor(s, dtype=torch.long)
                attn[i, :L] = 1
            return out, attn

        pt_ids_list = [ex["pt_input_ids"] for ex in batch]
        en_ids_list = [ex["en_input_ids"] for ex in batch]
        pt_input_ids, pt_attention_mask = pad_block(pt_ids_list, pt_tokenizer.pad_token_id)
        en_input_ids, en_attention_mask = pad_block(en_ids_list, en_tokenizer.pad_token_id)

        return {
            "pt_input_ids": pt_input_ids,
            "pt_attention_mask": pt_attention_mask,
            "en_input_ids": en_input_ids,
            "en_attention_mask": en_attention_mask,
        }

    # 5) DataLoader
    train_loader = DataLoader(
        PairsDataset(train_pairs),
        batch_size=batch_size,
        shuffle=shuffle_train,
        collate_fn=lambda b: collate_padded(b, pt_tokenizer.pad_token_id, en_tokenizer.pad_token_id),
        num_workers=num_workers,
    )
    val_loader = DataLoader(
        PairsDataset(val_pairs),
        batch_size=batch_size,
        shuffle=False,
        collate_fn=lambda b: collate_padded(b, pt_tokenizer.pad_token_id, en_tokenizer.pad_token_id),
        num_workers=num_workers,
    )

    return train_loader, val_loader

def test_dataloaders(train_loader, val_loader, show_val: bool = True):
    """
    æ£€æŸ¥ DataLoader çš„ batch è¾“å‡ºï¼Œæ‰“å°å¼ é‡å½¢çŠ¶å’Œä¸€ä¸ªç¤ºä¾‹

    å‚æ•°:
        train_loader: è®­ç»ƒ DataLoader
        val_loader: éªŒè¯ DataLoader
        show_val: æ˜¯å¦å±•ç¤ºéªŒè¯é›†çš„ä¸€ä¸ªæ ·æœ¬ï¼ˆé»˜è®¤ Trueï¼‰
    """
    # 1. æ‹¿ä¸€ä¸ªè®­ç»ƒ batch çœ‹ shape
    batch = next(iter(train_loader))
    logger.info("=== Train Loader Batch Shapes ===")
    for k, v in batch.items():
        logger.info(f"{k:20s} {tuple(v.shape)}")

    # 2. éªŒè¯é›†æ ·æœ¬
    if show_val:
        logger.info("\n=== Validation Loader Example ===")
        for i in val_loader:
            logger.info("pt_input_ids:     ", i["pt_input_ids"][0])
            logger.info("pt_attention_mask:", i["pt_attention_mask"][0])
            break

if __name__ == "__main__":
    print("=" * 60)
    print("Step 4: æ„å»ºå’Œæµ‹è¯•DataLoader")
    print("=" * 60)
    
    # æ•°æ®æ–‡ä»¶åœ°å€
    train_path = "/data2/workspace/yszhang/train_transformers/tensorflow_datasets/ted_pt_en_train.csv"
    val_path = "/data2/workspace/yszhang/train_transformers/tensorflow_datasets/ted_pt_en_test.csv"
    
    # æ„å»ºè¯è¡¨å‚æ•°
    vocab_size = 2 ** 13  # è¯è¡¨å¤§å°
    min_freq = 2  # æœ€å°è¯é¢‘
    special_tokens = ["<s>", "<pad>", "</s>", "<unk>", "<mask>"]  # ç‰¹æ®Šç¬¦å·
    max_length = 30  # æœ€å¤§åºåˆ—é•¿åº¦
    
    # DataLoaderå‚æ•°
    batch_size = 32  # æ‰¹å¤„ç†æ•°
    
    print(f"ğŸ”§ DataLoaderå‚æ•°:")
    print(f"   æ‰¹å¤§å°: {batch_size}")
    print(f"   æœ€å¤§åºåˆ—é•¿åº¦: {max_length}")
    
    try:
        # 1. åŠ è½½æ•°æ®é›†
        print(f"\nğŸ“ åŠ è½½æ•°æ®é›†...")
        train_dataset, val_dataset = load_translation_dataset(
            train_path=train_path,
            val_path=val_path
        )
        
        # 2. æ„å»º Tokenizer
        print(f"\nğŸ”¨ æ„å»º Tokenizer...")
        pt_tokenizer, en_tokenizer = train_and_load_tokenizers(
            train_dataset=train_dataset,
            pt_key="pt",
            en_key="en",
            vocab_size=vocab_size,
            min_freq=min_freq,
            special_tokens=special_tokens,
            save_dir_pt="tok_pt",
            save_dir_en="tok_en",
            max_length=max_length
        )
        
        # 3. æ„å»º DataLoader
        print(f"\nğŸ”¨ æ„å»º DataLoader...")
        train_loader, val_loader = build_dataloaders(
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            pt_tokenizer=pt_tokenizer,
            en_tokenizer=en_tokenizer,
            batch_size=batch_size,
            max_length=max_length,
            num_workers=0,
            shuffle_train=True
        )
        
        print(f"\nâœ… DataLoaderæ„å»ºå®Œæˆï¼")
        print(f"ğŸ“Š è®­ç»ƒé›†æ‰¹æ¬¡æ•°: {len(train_loader)}")
        print(f"ğŸ“Š éªŒè¯é›†æ‰¹æ¬¡æ•°: {len(val_loader)}")
        
        # 4. æµ‹è¯• DataLoader
        print(f"\nğŸ§ª æµ‹è¯• DataLoader...")
        test_dataloaders(train_loader, val_loader)
        
        print(f"\nâœ… DataLoaderæµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ DataLoaderæ„å»ºå¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")

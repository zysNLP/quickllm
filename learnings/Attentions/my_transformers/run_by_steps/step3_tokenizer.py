# -*- coding: utf-8 -*-
"""
Step 3: è®­ç»ƒå’Œæµ‹è¯•Tokenizer
è®­ç»ƒå¹¶åŠ è½½è‘¡è„ç‰™è¯­å’Œè‹±è¯­çš„ ByteLevel BPE Tokenizer
"""

import os
from pathlib import Path
from datasets import load_dataset
from tokenizers import ByteLevelBPETokenizer
from transformers import PreTrainedTokenizerFast
from loguru import logger

def load_translation_dataset(train_path: str, val_path: str, delimiter: str = "\t"):
    """åŠ è½½æ•°æ®é›†"""
    dataset = load_dataset(
        "csv",
        data_files={
            "train": train_path,
            "validation": val_path
        },
        column_names=["pt", "en"],
        delimiter=delimiter
    )
    return dataset["train"], dataset["validation"]

def train_and_load_tokenizers(
    train_dataset,
    pt_key="pt",
    en_key="en",
    vocab_size=2 ** 13,
    min_freq=2,
    special_tokens=["<s>", "<pad>", "</s>", "<unk>", "<mask>"],
    save_dir_pt="tok_pt",
    save_dir_en="tok_en",
    max_length=1024
):
    """
    è®­ç»ƒå¹¶åŠ è½½è‘¡è„ç‰™è¯­å’Œè‹±è¯­çš„ ByteLevel BPE Tokenizer

    å‚æ•°:
        train_dataset: æ•°æ®é›† (éœ€åŒ…å« pt_key å’Œ en_key ä¸¤åˆ—)
        pt_key: æºè¯­è¨€å­—æ®µå (é»˜è®¤ "pt")
        en_key: ç›®æ ‡è¯­è¨€å­—æ®µå (é»˜è®¤ "en")
        vocab_size: è¯è¡¨å¤§å°
        min_freq: æœ€å°è¯é¢‘
        special_tokens: ç‰¹æ®Šç¬¦å·
        save_dir_pt: è‘¡è¯­ tokenizer ä¿å­˜è·¯å¾„
        save_dir_en: è‹±è¯­ tokenizer ä¿å­˜è·¯å¾„
        max_length: æ¨¡å‹æœ€å¤§åºåˆ—é•¿åº¦

    è¿”å›:
        pt_tokenizer, en_tokenizer
    """

    def iter_lang(ds, key):
        for ex in ds:
            txt = ex[key]
            if isinstance(txt, bytes):
                txt = txt.decode("utf-8")
            yield txt

    # åˆå§‹åŒ– tokenizer
    pt_bbpe = ByteLevelBPETokenizer(add_prefix_space=True)
    en_bbpe = ByteLevelBPETokenizer(add_prefix_space=True)

    # è®­ç»ƒ tokenizer
    logger.info("å¼€å§‹è®­ç»ƒè‘¡è„ç‰™è¯­ tokenizer...")
    pt_bbpe.train_from_iterator(
        iter_lang(train_dataset, pt_key),
        vocab_size=vocab_size,
        min_frequency=min_freq,
        special_tokens=special_tokens,
    )
    
    logger.info("å¼€å§‹è®­ç»ƒè‹±è¯­ tokenizer...")
    en_bbpe.train_from_iterator(
        iter_lang(train_dataset, en_key),
        vocab_size=vocab_size,
        min_frequency=min_freq,
        special_tokens=special_tokens,
    )

    # ä¿å­˜ vocab/merges + tokenizer.json
    Path(save_dir_pt).mkdir(exist_ok=True)
    Path(save_dir_en).mkdir(exist_ok=True)
    pt_bbpe.save_model(save_dir_pt)
    en_bbpe.save_model(save_dir_en)
    pt_bbpe._tokenizer.save(f"{save_dir_pt}/tokenizer.json")
    en_bbpe._tokenizer.save(f"{save_dir_en}/tokenizer.json")

    # ç”¨ PreTrainedTokenizerFast åŠ è½½
    pt_tokenizer = PreTrainedTokenizerFast(tokenizer_file=f"{save_dir_pt}/tokenizer.json")
    en_tokenizer = PreTrainedTokenizerFast(tokenizer_file=f"{save_dir_en}/tokenizer.json")

    # è®¾ç½®ç‰¹æ®Šç¬¦å·
    for tok in (pt_tokenizer, en_tokenizer):
        tok.pad_token = "<pad>"
        tok.unk_token = "<unk>"
        tok.bos_token = "<s>"
        tok.eos_token = "</s>"
        tok.mask_token = "<mask>"
        tok.model_max_length = max_length
        tok.padding_side = "right"

    logger.info(f"pt vocab size: {len(pt_tokenizer)}")
    logger.info(f"en vocab size: {len(en_tokenizer)}")

    return pt_tokenizer, en_tokenizer

def test_tokenizers(en_tokenizer, pt_tokenizer,
                    en_sample: str = "Transformer is awesome.",
                    pt_sample: str = "Transformers sÃ£o incrÃ­veis."):
    """
    æµ‹è¯•è‹±æ–‡å’Œè‘¡è„ç‰™è¯­çš„ tokenizer ç¼–ç /è§£ç æ˜¯å¦æ­£ç¡®ï¼Œ
    å¹¶æ‰“å° token IDsã€å•ä¸ª ID å¯¹åº”çš„ token ç»“æœã€‚
    """

    # --- English ---
    logger.info("=== English Tokenizer Test ===")
    en_ids = en_tokenizer.encode(en_sample, add_special_tokens=False)
    logger.info(f"[EN] Tokenized IDs: {en_ids}")

    en_decoded = en_tokenizer.decode(
        en_ids,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )
    logger.info(f"[EN] Decoded string: {en_decoded}")
    assert en_decoded == en_sample, "EN decode != original input!"

    logger.info("[EN] id --> decoded([id])  |  id --> token(str)")
    for tid in en_ids:
        single_decoded = en_tokenizer.decode(
            [tid],
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )
        token_str = en_tokenizer.convert_ids_to_tokens(tid)
        logger.info(f"{tid:>6} --> {single_decoded!r}  |  {tid:>6} --> {token_str!r}")

    logger.info("\n" + "-" * 60 + "\n")

    # --- Portuguese ---
    logger.info("=== Portuguese Tokenizer Test ===")
    pt_ids = pt_tokenizer.encode(pt_sample, add_special_tokens=False)
    logger.info(f"[PT] Tokenized IDs: {pt_ids}")

    pt_decoded = pt_tokenizer.decode(
        pt_ids,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )
    logger.info(f"[PT] Decoded string: {pt_decoded}")
    assert pt_decoded == pt_sample, "PT decode != original input!"

    logger.info("[PT] id --> decoded([id])  |  id --> token(str)")
    for tid in pt_ids:
        single_decoded = pt_tokenizer.decode(
            [tid],
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )
        token_str = pt_tokenizer.convert_ids_to_tokens(tid)
        logger.info(f"{tid:>6} --> {single_decoded!r}  |  {tid:>6} --> {token_str!r}")

if __name__ == "__main__":
    print("=" * 60)
    print("Step 3: è®­ç»ƒå’Œæµ‹è¯•Tokenizer")
    print("=" * 60)
    
    # æ•°æ®æ–‡ä»¶åœ°å€
    train_path = "/data2/workspace/yszhang/train_transformers/tensorflow_datasets/ted_pt_en_train.csv"
    val_path = "/data2/workspace/yszhang/train_transformers/tensorflow_datasets/ted_pt_en_test.csv"
    
    # æ„å»ºè¯è¡¨å‚æ•°
    vocab_size = 2 ** 13  # è¯è¡¨å¤§å°
    min_freq = 2  # æœ€å°è¯é¢‘
    special_tokens = ["<s>", "<pad>", "</s>", "<unk>", "<mask>"]  # ç‰¹æ®Šç¬¦å·
    max_length = 30  # æœ€å¤§åºåˆ—é•¿åº¦
    
    print(f"ğŸ”§ Tokenizerå‚æ•°:")
    print(f"   è¯è¡¨å¤§å°: {vocab_size}")
    print(f"   æœ€å°è¯é¢‘: {min_freq}")
    print(f"   æœ€å¤§åºåˆ—é•¿åº¦: {max_length}")
    print(f"   ç‰¹æ®Šç¬¦å·: {special_tokens}")
    
    try:
        # 1. åŠ è½½æ•°æ®é›†
        print(f"\nğŸ“ åŠ è½½æ•°æ®é›†...")
        train_dataset, val_dataset = load_translation_dataset(
            train_path=train_path,
            val_path=val_path
        )
        
        # 2. æ„å»º Tokenizer
        print(f"\nğŸ”¨ å¼€å§‹æ„å»º Tokenizer...")
        pt_tokenizer, en_tokenizer = train_and_load_tokenizers(
            train_dataset=train_dataset,  # æ•°æ®é›†
            pt_key="pt",  # è‘¡è¯­åˆ—å
            en_key="en",  # è‹±è¯­åˆ—å
            vocab_size=vocab_size,  # è¯è¡¨å¤§å°
            min_freq=min_freq,  # æœ€å°è¯é¢‘
            special_tokens=special_tokens,  # ç‰¹æ®Šç¬¦å·
            save_dir_pt="tok_pt",  # ä¿å­˜ç›®å½• (pt)
            save_dir_en="tok_en",  # ä¿å­˜ç›®å½• (en)
            max_length=max_length  # æœ€å¤§åºåˆ—é•¿åº¦
        )
        
        print(f"\nâœ… Tokenizeræ„å»ºå®Œæˆï¼")
        print(f"ğŸ“Š è‘¡è„ç‰™è¯­è¯è¡¨å¤§å°: {len(pt_tokenizer)}")
        print(f"ğŸ“Š è‹±è¯­è¯è¡¨å¤§å°: {len(en_tokenizer)}")
        
        # 3. æµ‹è¯• Tokenizer
        print(f"\nğŸ§ª æµ‹è¯• Tokenizer...")
        test_tokenizers(en_tokenizer=en_tokenizer, pt_tokenizer=pt_tokenizer)
        
        print(f"\nâœ… Tokenizeræµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ Tokenizeræ„å»ºå¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")

# -*- coding: utf-8 -*-
"""
Step 7: ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œå¹¶å¯è§†åŒ–å­¦ä¹ ç‡æ›²çº¿
"""

import os
import torch
import torch.optim as optim
from torch.optim.lr_scheduler import _LRScheduler
from transformers import get_cosine_schedule_with_warmup
import matplotlib.pyplot as plt

# è®¾ç½®GPUè®¾å¤‡
os.environ["CUDA_VISIBLE_DEVICES"] = "2"

class CustomizedSchedule(_LRScheduler):
    """
    Noam / Transformer LR:
      lr = d_model**(-0.5) * min(step**(-0.5), step * warmup_steps**(-1.5))
    """

    def __init__(self, optimizer, d_model, warmup_steps=4000, last_epoch=-1):
        self.d_model = float(d_model)
        self.warmup_steps = float(warmup_steps)
        super().__init__(optimizer, last_epoch)

    def get_lr(self):
        step = max(1, self.last_epoch + 1)  # ç¡®ä¿ä» 1 å¼€å§‹
        scale = self.d_model ** -0.5
        arg1 = step ** -0.5
        arg2 = step * (self.warmup_steps ** -1.5)
        lr = scale * min(arg1, arg2)
        return [lr for _ in self.base_lrs]

def plot_customized_lr_curve(optimizer, scheduler, total_steps: int, label: str = None):
    """
    ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿ï¼ˆæ”¯æŒä¼ å…¥å·²æœ‰ optimizer å’Œ schedulerï¼‰

    Args:
        optimizer (torch.optim.Optimizer): ä¼˜åŒ–å™¨
        scheduler (torch.optim.lr_scheduler._LRScheduler): å­¦ä¹ ç‡è°ƒåº¦å™¨
        total_steps (int): æ€»è®­ç»ƒæ­¥æ•°
        label (str): å›¾ä¾‹æ ‡ç­¾ï¼Œé»˜è®¤ä½¿ç”¨ scheduler é…ç½®
    """
    lrs = []
    for step in range(total_steps):
        scheduler.step()
        lr = scheduler.get_last_lr()[0]
        lrs.append(lr)

    # ç»˜åˆ¶æ›²çº¿
    plt.figure(figsize=(8, 4))
    plt.plot(range(1, total_steps + 1), lrs, label=label or "LR Curve")
    plt.ylabel("Learning Rate")
    plt.xlabel("Train Step")
    plt.title("Learning Rate Schedule")
    plt.legend()
    plt.grid(True)
    plt.show()

def create_optimizer_and_scheduler(model, learning_rate=5e-4, betas=(0.9, 0.98), 
                                 eps=1e-8, weight_decay=2e-5, epochs=600, 
                                 train_loader_length=1000, d_model=128):
    """
    åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    
    Args:
        model: æ¨¡å‹
        learning_rate: å­¦ä¹ ç‡
        betas: Adamä¼˜åŒ–å™¨çš„betaå‚æ•°
        eps: Adamä¼˜åŒ–å™¨çš„epsilonå‚æ•°
        weight_decay: æƒé‡è¡°å‡
        epochs: è®­ç»ƒè½®æ•°
        train_loader_length: è®­ç»ƒé›†æ‰¹æ¬¡æ•°
        d_model: æ¨¡å‹ç»´åº¦
    
    Returns:
        optimizer, scheduler
    """
    # è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°
    num_training_steps = train_loader_length * epochs
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        betas=betas,
        eps=eps,
        weight_decay=weight_decay
    )
    
    # è®¡ç®—warmupæ­¥æ•°
    warmup_steps = int(0.1 * num_training_steps)  # 10% æ­¥æ•°ç”¨ä½œ warmup
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=num_training_steps,
        num_cycles=0.5,
    )
    
    return optimizer, scheduler, num_training_steps, warmup_steps

def analyze_optimizer_and_scheduler(optimizer, scheduler, num_training_steps):
    """
    åˆ†æä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çš„é…ç½®
    """
    print(f"ğŸ“Š ä¼˜åŒ–å™¨é…ç½®:")
    print(f"   ä¼˜åŒ–å™¨ç±»å‹: {type(optimizer).__name__}")
    print(f"   å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']}")
    print(f"   Betaå‚æ•°: {optimizer.param_groups[0]['betas']}")
    print(f"   Epsilon: {optimizer.param_groups[0]['eps']}")
    print(f"   æƒé‡è¡°å‡: {optimizer.param_groups[0]['weight_decay']}")
    
    print(f"\nğŸ“Š å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®:")
    print(f"   è°ƒåº¦å™¨ç±»å‹: {type(scheduler).__name__}")
    print(f"   æ€»è®­ç»ƒæ­¥æ•°: {num_training_steps}")
    print(f"   Warmupæ­¥æ•°: {scheduler.num_warmup_steps}")
    print(f"   è®­ç»ƒæ­¥æ•°: {scheduler.num_training_steps}")
    print(f"   å‘¨æœŸæ•°: {scheduler.num_cycles}")
    
    # åˆ†æå­¦ä¹ ç‡å˜åŒ–
    print(f"\nğŸ“ˆ å­¦ä¹ ç‡å˜åŒ–åˆ†æ:")
    print(f"   åˆå§‹å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.6f}")
    
    # è®¡ç®—warmupç»“æŸæ—¶çš„å­¦ä¹ ç‡
    warmup_end_lr = scheduler.get_last_lr()[0]
    for _ in range(scheduler.num_warmup_steps):
        scheduler.step()
    warmup_end_lr = scheduler.get_last_lr()[0]
    print(f"   Warmupç»“æŸå­¦ä¹ ç‡: {warmup_end_lr:.6f}")
    
    # è®¡ç®—æœ€ç»ˆå­¦ä¹ ç‡
    for _ in range(scheduler.num_training_steps - scheduler.num_warmup_steps):
        scheduler.step()
    final_lr = scheduler.get_last_lr()[0]
    print(f"   æœ€ç»ˆå­¦ä¹ ç‡: {final_lr:.6f}")

if __name__ == "__main__":
    print("=" * 60)
    print("Step 7: ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨")
    print("=" * 60)
    
    # è®­ç»ƒå‚æ•°
    learning_rate = 5e-4
    betas = (0.9, 0.98)
    eps = 1e-8
    weight_decay = 2e-5
    epochs = 600
    train_loader_length = 1000  # å‡è®¾è®­ç»ƒé›†æœ‰1000ä¸ªbatch
    d_model = 128
    
    print(f"ğŸ”§ è®­ç»ƒå‚æ•°:")
    print(f"   å­¦ä¹ ç‡: {learning_rate}")
    print(f"   Betaå‚æ•°: {betas}")
    print(f"   Epsilon: {eps}")
    print(f"   æƒé‡è¡°å‡: {weight_decay}")
    print(f"   è®­ç»ƒè½®æ•°: {epochs}")
    print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {train_loader_length}")
    print(f"   æ¨¡å‹ç»´åº¦: {d_model}")
    
    try:
        # 1. åˆ›å»ºç¤ºä¾‹æ¨¡å‹ï¼ˆç”¨äºæµ‹è¯•ä¼˜åŒ–å™¨ï¼‰
        print(f"\nğŸ”¨ åˆ›å»ºç¤ºä¾‹æ¨¡å‹...")
        model = torch.nn.Linear(100, 10)  # ç®€å•çš„çº¿æ€§æ¨¡å‹ç”¨äºæµ‹è¯•
        
        # 2. åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        print(f"\nğŸ”¨ åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨...")
        optimizer, scheduler, num_training_steps, warmup_steps = create_optimizer_and_scheduler(
            model=model,
            learning_rate=learning_rate,
            betas=betas,
            eps=eps,
            weight_decay=weight_decay,
            epochs=epochs,
            train_loader_length=train_loader_length,
            d_model=d_model
        )
        
        print(f"âœ… ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨åˆ›å»ºå®Œæˆï¼")
        
        # 3. åˆ†æé…ç½®
        print(f"\nğŸ“Š åˆ†æä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨é…ç½®...")
        analyze_optimizer_and_scheduler(optimizer, scheduler, num_training_steps)
        
        # 4. å¯è§†åŒ–å­¦ä¹ ç‡æ›²çº¿
        print(f"\nğŸ“ˆ å¯è§†åŒ–å­¦ä¹ ç‡æ›²çº¿...")
        
        # é‡æ–°åˆ›å»ºè°ƒåº¦å™¨ç”¨äºå¯è§†åŒ–ï¼ˆå› ä¸ºä¸Šé¢çš„å·²ç»è¢«stepäº†ï¼‰
        scheduler_viz = get_cosine_schedule_with_warmup(
            optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=num_training_steps,
            num_cycles=0.5,
        )
        
        plot_customized_lr_curve(
            optimizer, 
            scheduler_viz, 
            total_steps=num_training_steps,
            label=f"d_model={d_model}, warmup={warmup_steps}"
        )
        
        print(f"\nâœ… ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨è®¾ç½®å®Œæˆï¼")
        
        # 5. å±•ç¤ºä¸åŒè°ƒåº¦å™¨çš„å¯¹æ¯”
        print(f"\nğŸ”„ å¯¹æ¯”ä¸åŒå­¦ä¹ ç‡è°ƒåº¦å™¨...")
        
        # åˆ›å»ºä¸åŒçš„è°ƒåº¦å™¨è¿›è¡Œå¯¹æ¯”
        optimizers = []
        schedulers = []
        labels = []
        
        # AdamW + Cosine with Warmup
        opt1 = optim.AdamW(model.parameters(), lr=learning_rate, betas=betas, eps=eps, weight_decay=weight_decay)
        sched1 = get_cosine_schedule_with_warmup(opt1, warmup_steps, num_training_steps, num_cycles=0.5)
        optimizers.append(opt1)
        schedulers.append(sched1)
        labels.append("Cosine with Warmup")
        
        # AdamW + Linear with Warmup
        opt2 = optim.AdamW(model.parameters(), lr=learning_rate, betas=betas, eps=eps, weight_decay=weight_decay)
        sched2 = get_cosine_schedule_with_warmup(opt2, warmup_steps, num_training_steps, num_cycles=1.0)
        optimizers.append(opt2)
        schedulers.append(sched2)
        labels.append("Cosine (1 cycle)")
        
        # ç»˜åˆ¶å¯¹æ¯”å›¾
        plt.figure(figsize=(12, 6))
        for i, (opt, sched, label) in enumerate(zip(optimizers, schedulers, labels)):
            lrs = []
            for step in range(min(1000, num_training_steps)):  # åªæ˜¾ç¤ºå‰1000æ­¥
                sched.step()
                lr = sched.get_last_lr()[0]
                lrs.append(lr)
            
            plt.plot(range(1, len(lrs) + 1), lrs, label=label, linewidth=2)
        
        plt.ylabel("Learning Rate")
        plt.xlabel("Train Step")
        plt.title("Learning Rate Schedule Comparison")
        plt.legend()
        plt.grid(True)
        plt.show()
        
        print(f"\nâœ… å­¦ä¹ ç‡è°ƒåº¦å™¨å¯¹æ¯”å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨è®¾ç½®å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥å‚æ•°è®¾ç½®æ˜¯å¦æ­£ç¡®")

好的，一句话概括：

**为了防止模型在训练时“作弊”，偷看未来的信息。**

---

为了让你更好地理解，这里有两个简单的比喻：

1.  **像考试时遮住答案**：你在做一道填空题时，不能先把后面的答案看了再回来写。Mask就像用手遮住了试卷上你还没做的部分，强迫你只根据已掌握的知识来答题。

2.  **像教孩子说话**：你让孩子接着说完“今天天气真...”，他应该猜“好”或“不错”，而不是直接告诉他后面的词是“好，我们出去玩吧”。Mask确保了模型在预测下一个词时，只能利用它前面已经生成的词。

在Transformer中，这个“遮罩”通常是通过一个**上三角矩阵**（矩阵右上角全是负无穷 `-inf`）来实现的，这样在计算注意力权重时，后面的词就会被屏蔽掉，权重为0。
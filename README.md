## ä¸€. ç®€ä»‹

LLMå­¦ä¹ èµ„æºåº“ã€‚ä¸€ä¸ªä½¿ç”¨pytorchå’Œéƒ¨åˆ†Tensorflow2å®ç°çš„é¡¹ç›®ï¼Œå¯ä»¥ **<u>*æœ¬åœ°è¿è¡Œå’Œè°ƒè¯•*</u>** çš„å¤§æ¨¡å‹LLMç›¸å…³çš„åº”ç”¨å’Œä»£ç 

å†æ¬¡å¼ºè°ƒï¼šå¼ºè°ƒæœ¬åœ°è°ƒè¯•ä¹ æƒ¯ï¼Œå¼ºè°ƒä»£ç è·³è½¬ï¼Œå¼ºè°ƒå¿«é€ŸæŒæ¡LLMç›¸å…³å·¥ä½œ

```python
# ä¸‹æ¬¡æ›´æ–°ï¼š1.åŸºäºPyTorchçš„Mistral-AIçš„MOEæ¨¡å‹å¾®è°ƒï¼›2.å„ç±»TensorRT-LLMå’Œæ¨ç†åŠ é€Ÿçš„ç›¸å…³å·¥ä½œ
```

### æœ€æ–°æ›´æ–°ï¼š

- 20231219ï¼šğŸ¯feat(basic_language_model_moe*):MOEæ–°å¢2ç»´å’Œ3ç»´æ•°æ®çš„è®­ç»ƒè¿‡ç¨‹
- 20231218ï¼šğŸ¯feat(quickllm/clients/vllm.py):æ·»åŠ vllmçš„apiè¯·æ±‚æ¨¡å—
- 20231214ï¼šğŸ¯feat(README): å¢åŠ Qwen-TensorRT-LLMçš„é“¾æ¥å’Œè¯´æ˜

- 20231213ï¼šğŸ¯feat(quickllm/clients/triton_client*):æ·»åŠ ChatGLMç³»åˆ—tritonæœåŠ¡åŸºç¡€ä»£ç 
- 20231213ï¼šğŸ¯feat(basic_language_model_moe.py):æ·»åŠ PyTorchå®ç°MOEæ¨¡å‹åˆå§‹è°ƒè¯•ä»£ç 
- 20231212ï¼šğŸ¯feat(quickllm/layers/lora.py):æ·»åŠ TF2å®ç°çš„LORAå±‚ï¼ˆåˆæµ‹ç‰ˆï¼‰
- 20231201ï¼šğŸ¯feat: å¢åŠ Tensorflow2çš„ROPEå‡½æ•°å®ç°
- 

**è°ƒè¯•æ–¹å¼ï¼š**

â€‹		**æ ¼å¼1. åªæœ‰ä¸€ä¸ªè„šæœ¬æ–‡ä»¶ï¼Œå°†examplesä¸­çš„pyè„šæœ¬å¤åˆ¶åˆ°quickllmæ–‡ä»¶å¤¹çš„åŒçº§ç›®å½•ï¼ˆå½“å‰é¡¹ç›®æ ¹ç›®å½•ï¼‰**

â€‹		**æ ¼å¼2. å¦‚æœè°ƒè¯•è„šæœ¬æ˜¯ä¸ªæ–‡ä»¶å¤¹ï¼Œæ¨èå°†ä¾èµ–æ”¹æˆç›¸å¯¹è·¯å¾„æˆ–è€…æ ¼å¼1çš„å½¢å¼ï¼›æˆ–è€…åœ¨éœ€è¦ä½¿ç”¨from quickllmå¯¼åŒ…è°ƒç”¨çš„è„šæœ¬ä»£ç ä¸­æ·»åŠ çˆ¶çº§ç›®å½•**

```python
import sys
sys.path.append('/path/to/directory of quickllm')  # quickllmæ–‡ä»¶å¤¹çš„çˆ¶çº§ç›®å½•
```

**æ ¸å¿ƒåŠŸèƒ½**ï¼š chatglmã€chatglm2ã€llamaã€llama2ã€ baichuanã€ziyaã€bloomç­‰å¼€æºå¤§æ¨¡å‹æƒé‡è¿›è¡Œæ¨ç†å’Œå¾®è°ƒã€promptåº”ç”¨



### äºŒã€å¿«é€Ÿå¯åŠ¨ï¼ˆä»¥MOEæ ¸å¿ƒä»£ç åŸç†ä¸ºä¾‹ï¼‰

```python
# -*- coding: utf-8 -*- 
# @Time : 2023/12/13 02:09 
# @Author : ys 
# @File : basic_language_model_moe.py

import torch
from torch import nn
from quickllm.layers.moe import MoE


if __name__ == "__main__":

    moe = MoE(
        dim=512,  # è¾“å…¥å¼ é‡çš„ç»´åº¦
        num_experts=16,  # ä¸“å®¶æ•°é‡ï¼Œå¯ä»¥å¢åŠ è¯¥å‚æ•°è€Œä¸å¢åŠ è®¡ç®—é‡
        hidden_dim=512 * 4,  # æ¯ä¸ªä¸“å®¶ç½‘ç»œä¸­çš„éšè—å±‚ç»´åº¦ï¼Œé»˜è®¤ä¸º 4 å€è¾“å…¥ç»´åº¦
        activation=nn.LeakyReLU,  # ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ï¼Œé»˜è®¤ä¸º GELU
        second_policy_train='random',  # ä½¿ç”¨çš„ç¬¬äºŒåä¸“å®¶çš„è®­ç»ƒç­–ç•¥
        second_policy_eval='random',  # ä½¿ç”¨çš„ç¬¬äºŒåä¸“å®¶çš„éªŒè¯ç­–ç•¥
        second_threshold_train=0.2,  # è®­ç»ƒæ—¶ä½¿ç”¨çš„ç¬¬äºŒåä¸“å®¶é˜ˆå€¼
        second_threshold_eval=0.2,  # æµ‹è¯•æ—¶ä½¿ç”¨çš„ç¬¬äºŒåä¸“å®¶é˜ˆå€¼
        capacity_factor_train=1.25,  # æ¯ä¸ªä¸“å®¶ç½‘ç»œåœ¨å•ä¸ªæ‰¹æ¬¡ä¸­çš„å›ºå®šå®¹é‡ï¼Œéœ€è¦é¢å¤–çš„å®¹é‡ä»¥é˜²é—¨æ§ä¸å¹³è¡¡
        capacity_factor_eval=2.,  # capacity_factor_* åº”è®¾ç½®ä¸º >=1 çš„å€¼
        loss_coef=1e-2  # è¾…åŠ©ä¸“å®¶å¹³è¡¡è¾…åŠ©æŸå¤±çš„ä¹˜æ•°
    )
    inputs = torch.randn(4, 1024, 512)
    out, aux_loss = moe(inputs)  # (4, 1024, 512), (1,)
    print(out.shape, aux_loss.shape)
```



## ä¸‰.ä½¿ç”¨æ–¹å¼(ä»¥chatglm2ä¸ºä¾‹)

**åŸºæœ¬æµç¨‹ï¼š**

```shell
pip install -r requirements.txt -i https://pypi.douban.com/simple
```

â€‹	**1. å®šä¹‰configå‚æ•°å’Œé…ç½®ã€åŠ è½½æ•°æ®é›†ï¼ˆå…¶ä»–å‚æ•°åˆ—è¡¨å‚è€ƒç¬¬ä¸‰éƒ¨åˆ†ï¼‰ï¼›**

â€‹	chatglm2å‚æ•°ä¸‹è½½ï¼šhttps://huggingface.co/THUDM/chatglm2-6bï¼›

â€‹	chatglm2æ•°æ®ä¸‹è½½ï¼šhttps://cloud.tsinghua.edu.cn/f/b3f119/a008264b1cabd1/?dl=1

â€‹	**2.ç¼–å†™åŠ è½½promptã€å®šä¹‰æ¨¡å‹ï¼ˆè®­ç»ƒå’ŒéªŒè¯å‡½æ•°ï¼‰**

â€‹	**3.è½½å…¥æ•°æ®ï¼Œå¯åŠ¨è®­ç»ƒå’ŒéªŒè¯ï¼Œè°ƒè¯•ä»£ç ï¼Œè°ƒè¯•å®Œæˆï¼ä¿å­˜ä¿®æ”¹è„šæœ¬åˆ°examplesï¼Œå¤„ç†ä¸‹ä¸€ä¸ª**

**å¿«é€Ÿå¯åŠ¨ï¼š** å°†examples/basic/glm/basic_language_model_chatglm2.pyå¤åˆ¶åˆ°æ ¹ç›®å½•ä¸‹ï¼Œæ·»åŠ æ–­ç‚¹ï¼Œå¯åŠ¨è°ƒè¯•ï¼

```python
# -*- coding: utf-8 -*- 
"""
    @Project ï¼šquickllm 
    @File    ï¼šquickly_start.py
    @Author  ï¼šys
    @Time    ï¼š2023/12/12 12:12
    å®˜æ–¹é¡¹ç›®ï¼šhttps://github.com/THUDM/ChatGLM2-6B
"""

import os
import torch
from loguru import logging
from transformers import AutoTokenizer

from quickllm.models import build_transformer_model
from quickllm.generation import SeqGeneration


class ExpertModel:

    def __init__(self):
        self.prompt = "è¯·ä»¥ä¸€ä½åŒ»ç–—é¢†åŸŸçŸ¥è¯†å›¾è°±ä¸“å®¶çš„è§’è‰²å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š"
        self.choice = 'default'  # default, int4, 32k
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'

        # æ¥è‡ªhttps://huggingface.co/THUDM/chatglm2-6bï¼›
        self.dir_path = "/path/to/my/pretrain_ckpt/glm/chatglm2-6B"
        self.checkpoint_path = [os.path.join(dir_path, i) for i in os.listdir(dir_path) if i.endswith('.bin')]
        # æ¥è‡ªé¡¹ç›®ä¸­çš„ï¼šexamples/basic/glm/chatglm2-6B/quickllm_config.json
        self.config_path = dir_path + '/quickllm_config.json'

    def build_prompt(self, history):
        for query, response in history:
            self.prompt += f"\n\nç”¨æˆ·ï¼š{query}"
            self.prompt += f"\n\nChatGLM-6Bï¼š{response}"
        return self.prompt

    def build_model(self):
        tokenizer = AutoTokenizer.from_pretrained(self.dir_path.replace('/', '\\'), trust_remote_code=True)
        if self.choice in {'default', '32k'}:
            encoder = build_transformer_model(config_path=self.config_path,
                                              checkpoint_path=self.checkpoint_path).half().to(device)
        else:
            encoder = build_transformer_model(config_path=self.config_path,
                                              checkpoint_path=self.checkpoint_path).to(device)

        model = SeqGeneration(encoder, tokenizer, start_id=None, end_id=tokenizer.eos_token_id, mode='random_sample',
                              maxlen=2048, default_rtype='logits', use_states=True)
        return model

    def chat(self, query, history):
        prompt = ""
        for i, (old_query, response) in enumerate(history):
            prompt += "[Round {}]\n\né—®ï¼š{}\n\nç­”ï¼š{}\n".format(i + 1, old_query, response)
        prompt += "[Round {}]\n\né—®ï¼š{}\n\nç­”ï¼š".format(len(history) + 1, query)

        for response in self.build_model().stream_generate(prompt, topk=50, topp=0.7, temperature=0.95):
            new_history = history + [(query, response)]
            yield response, new_history

    def main(self):
        history = []
        logging.info("----æ¬¢è¿ä½¿ç”¨ChatGLM2-6Bæ¨¡å‹ï¼Œä¿®æ”¹promptè¾“å…¥å†…å®¹è¿›è¡Œå¯¹è¯ï¼Œclearæ¸…ç©ºå¯¹è¯å†å²ï¼Œstopç»ˆæ­¢ç¨‹åº")
        while True:
            query = input("\nQuestionï¼š")
            if query.strip() == "stop":
                break
            if query.strip() == "clear":
                history = []
                print("----å·²æ¸…ç©ºå†å²å¯¹è¯----")
                continue
            for response, history in self.chat(query, history=history):
                print(build_prompt(history), flush=True)

            print(build_prompt(history), flush=True)
            torch.cuda.empty_cache()


if __name__ == '__main__':
    expert_bot = ExpertModel()
    expert_bot.main()
```



**å¿«é€Ÿå¾®è°ƒ**ï¼š å°†examples/llm/task_chatglm2_lora.pyæ–‡ä»¶è½¬ç§»è‡³æ ¹ç›®å½•ä¸‹ï¼Œæ·»åŠ æ–­ç‚¹ï¼Œå¯åŠ¨è°ƒè¯•ï¼

**å¿«é€Ÿéƒ¨ç½²ï¼š** å°†examples/serving/basic_simple_web_serving_simbert.pyæ–‡ä»¶è½¬ç§»è‡³æ ¹ç›®å½•ä¸‹ï¼Œæ·»åŠ æ–­ç‚¹ï¼Œå¯åŠ¨è°ƒè¯•ï¼



## ä¸‰. é¢„è®­ç»ƒæƒé‡
- è‹¥æ— è¯´æ˜åˆ™ä½¿ç”¨æƒé‡è‡ªå¸¦çš„`pytorch_model.bin`å’Œ`config.json`

| æ¨¡å‹åˆ†ç±»| æ¨¡å‹åç§° | æƒé‡æ¥æº| æƒé‡é“¾æ¥ | å¤‡æ³¨(è‹¥æœ‰)|
| ----- | ----- | ----- | ----- | ----- |
| chatglm   |chatglm-6b | THUDM | [github](https://github.com/THUDM/ChatGLM-6B), [v0.1.0](https://huggingface.co/THUDM/chatglm-6b/tree/v0.1.0), [v1.1.0](https://huggingface.co/THUDM/chatglm-6b/tree/v1.1.0), [int8](https://huggingface.co/THUDM/chatglm-6b-int8), [int4](https://huggingface.co/THUDM/chatglm-6b-int4) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/glm) |
|       |chatglm2-6b | THUDM | [github](https://github.com/THUDM/ChatGLM2-6B), [v2](https://huggingface.co/THUDM/chatglm2-6b), [int4](https://huggingface.co/THUDM/chatglm2-6b-int4), [32k](https://huggingface.co/THUDM/chatglm2-6b-32k) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/glm) |
|       |chatglm3-6b | THUDM | [github](https://github.com/THUDM/ChatGLM3), [v3](https://huggingface.co/THUDM/chatglm3-6b), [32k](https://huggingface.co/THUDM/chatglm3-6b-32k) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/glm) |
| bert| bert-base-chinese| è°·æ­Œbertçš„torchç‰ˆ | [torch](https://huggingface.co/bert-base-chinese) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/bert/google@bert-base-chinese/bert4torch_config.json) |
|     | chinese_L-12_H-768_A-12| è°·æ­Œ | [github](https://github.com/google-research/bert), [tf](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip) | [è½¬æ¢å‘½ä»¤](https://huggingface.co/docs/transformers/v4.28.1/en/converting_tensorflow_models), [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/bert/google@chinese_L-12_H-768_A-12/bert4torch_config.json) |
|     | chinese-bert-wwm-ext| HFL | [tf/torch](https://github.com/ymcui/Chinese-BERT-wwm)ï¼Œ[torch](https://huggingface.co/hfl/chinese-bert-wwm-ext)| |
|     | bert-base-multilingual-cased| huggingface | [torch](https://huggingface.co/bert-base-multilingual-cased) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/bert/google@bert-base-chinese/bert4torch_config.json) |
|     | macbert | HFL| [tf/torch](https://github.com/ymcui/MacBERT)ï¼Œ[torch](https://huggingface.co/hfl/chinese-macbert-base) | |
|     | wobert| è¿½ä¸€ç§‘æŠ€| [tf](https://github.com/ZhuiyiTechnology/WoBERT)ï¼Œ[torch_base](https://huggingface.co/junnyu/wobert_chinese_base)ï¼Œ[torch_plus_base](https://huggingface.co/junnyu/wobert_chinese_plus_base) | |
|     | guwenbert| ethanyt |[torch](https://huggingface.co/ethanyt/guwenbert-base) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/bert/ethanyt@guwenbert-base/bert4torch_config.json)|
|roberta|chinese-roberta-wwm-ext | HFL | [tf/torch](https://github.com/ymcui/Chinese-BERT-wwm)ï¼Œ[torch](https://huggingface.co/hfl/chinese-roberta-wwm-ext) | |
|     |roberta-small/tiny| è¿½ä¸€ç§‘æŠ€ & UER| [tf](https://github.com/ZhuiyiTechnology/pretrained-models)ï¼Œ[torch](https://huggingface.co/uer) | [è½¬æ¢è„šæœ¬](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/roberta/convert_roberta-small.py) |
|     |roberta-base-english| huggingface | [torch](https://huggingface.co/roberta-base) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/roberta/huggingface@roberta-base-english/bert4torch_config.json) |
| albert|albert| brightmart| [tf](https://github.com/brightmart/albert_zh)ï¼Œ[torch](https://huggingface.co/voidful)ï¼Œ[torch](https://github.com/lonePatient/albert_pytorch) | |
| nezha|NEZHA | åä¸º| [tf](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-TensorFlow)ï¼Œ[torch](https://github.com/lonePatient/NeZha_Chinese_PyTorch) | |
| xlnet|chinese-xlnet | HFL | [tf/torch](https://github.com/ymcui/Chinese-XLNet) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/xlnet/hfl@chinese-xlnet-base)|
|deberta| Erlangshen-DeBERTa-v2| IDEA | [torch](https://huggingface.co/IDEA-CCNL/Erlangshen-DeBERTa-v2-320M-Chinese/tree/main) | |
| electra|Chinese-ELECTRA | HFL | [tf](https://github.com/ymcui/Chinese-ELECTRA)ï¼Œ[torch](https://huggingface.co/hfl/chinese-electra-base-discriminator) | |
| ernie|ernie | ç™¾åº¦æ–‡å¿ƒ| [paddle](https://github.com/PaddlePaddle/ERNIE)ï¼Œ[torch](https://huggingface.co/nghuyong)| |
| roformer|roformer| è¿½ä¸€ç§‘æŠ€| [tf](https://github.com/ZhuiyiTechnology/roformer)ï¼Œ[torch](https://huggingface.co/junnyu/roformer_chinese_base) | |
|         |roformer_v2 | è¿½ä¸€ç§‘æŠ€| [tf](https://github.com/ZhuiyiTechnology/roformer-v2)ï¼Œ[torch](https://huggingface.co/junnyu/roformer_v2_chinese_char_base)| |
| simbert|simbert | è¿½ä¸€ç§‘æŠ€| [tf](https://github.com/ZhuiyiTechnology/simbert)ï¼Œ[torch_base](https://huggingface.co/peterchou/simbert-chinese-base/tree/main) | [è½¬æ¢è„šæœ¬](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/simbert/convert_simbert.py) |
|        |simbert_v2/roformer-sim | è¿½ä¸€ç§‘æŠ€| [tf](https://github.com/ZhuiyiTechnology/roformer-sim)ï¼Œ[torch](https://huggingface.co/junnyu/roformer_chinese_sim_char_base)| |
| gau|GAU-alpha | è¿½ä¸€ç§‘æŠ€| [tf](https://github.com/ZhuiyiTechnology/GAU-alpha)| [è½¬æ¢è„šæœ¬](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/gau/convert_GAU_alpha.py) |
| gpt |CDial-GPT| thu-coai| [torch](https://github.com/thu-coai/CDial-GPT) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/gpt/thu-coai@CDial-GPT-LCCC-base/bert4torch_config.json) |
| gpt2| cmp_lm(26äº¿)|æ¸…å | [torch](https://github.com/TsinghuaAI/CPM-1-Generate)| [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/gpt2/cpm@cpm_lm_2.6b) |
|     | gpt2-chinese-cluecorpussmall|UER | [torch](https://huggingface.co/uer/gpt2-chinese-cluecorpussmall) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/gpt2/uer@gpt2-chinese-cluecorpussmall)|
|     | gpt2-ml|imcaspar | [tf](https://github.com/imcaspar/gpt2-ml)ï¼Œ[torch](https://github.com/ghosthamlet/gpt2-ml-torch) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/gpt2/imcaspar@gpt2-ml_15g_corpus_torch) |
| bart| bart_base_chinese|å¤æ—¦fnlp| [torch](https://github.com/fastnlp/CPT), [v1.0](https://huggingface.co/fnlp/bart-base-chinese/tree/v1.0), [v2.0](https://huggingface.co/fnlp/bart-base-chinese/tree/v2.0)| [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/bart/fnlp@bart-base-chinese/bert4torch_config.json) |
| t5  | t5| UER | [torch](https://huggingface.co/uer/t5-base-chinese-cluecorpussmall)| [config_base](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/t5/uer@t5-base-chinese-cluecorpussmall), [config_small](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/t5/uer@t5-small-chinese-cluecorpussmall)|
|     | mt5 | è°·æ­Œ| [torch](https://huggingface.co/google/mt5-base)| [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/t5/google@mt5_torch_base)|
|     | t5_pegasus| è¿½ä¸€ç§‘æŠ€| [tf](https://github.com/ZhuiyiTechnology/t5-pegasus) | [config_base](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/t5/sushen@chinese_t5_pegasus_base_torch), [config_small](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/t5/sushen@chinese_t5_pegasus_small_torch)|
|     | chatyuan v1&v2| clue-ai | [torch](https://github.com/clue-ai/ChatYuan) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/t5/ClueAI@ClueAI-ChatYuan-large-v1)|
|     | PromptCLUE| clue-ai | [torch](https://github.com/clue-ai/PromptCLUE) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/t5/ClueAI@ClueAI-ChatYuan-large-v1)|
| llama | llama | facebook| [github](https://github.com/facebookresearch/llama) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/llama)|
|       | llama-2 | facebook| [github](https://github.com/facebookresearch/llama), [7b](https://huggingface.co/meta-llama/Llama-2-7b-hf), [7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf), [13b](https://huggingface.co/meta-llama/Llama-2-13b-hf), [13b-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/llama)|
|       | chinese_llama_alpaca|HFL|[github](https://github.com/ymcui/Chinese-LLaMA-Alpaca) |[config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/llama)|
|       | Belle_llama| LianjiaTech| [github](https://github.com/LianjiaTech/BELLE), [7B-2M-enc](https://huggingface.co/BelleGroup/BELLE-LLaMA-7B-2M-enc) | [åˆæˆè¯´æ˜](https://github.com/LianjiaTech/BELLE/tree/main/models)ã€[config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/llama)|
|       | Ziya | IDEA-CCNL | [v1](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1), [v1.1](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1.1), [pretrain-v1](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-Pretrain-v1) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/llama) |
|       | Baichuan | baichuan-inc | [github](https://github.com/baichuan-inc/Baichuan), [7B](https://huggingface.co/baichuan-inc/Baichuan-7B), [13B-Base](https://huggingface.co/baichuan-inc/Baichuan-13B-Base), [13B-Chat](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/llama) |
|       | Baichuan2 | baichuan-inc | [github](https://github.com/baichuan-inc/Baichuan2), [7B-Base](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base), [7B-Chat](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat), [13B-Base](https://huggingface.co/baichuan-inc/Baichuan2-13B-Base), [13B-Chat](https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/llama) |
|       | vicuna | lmsys| [7b-v1.5](https://huggingface.co/lmsys/vicuna-7b-v1.5) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/llama)|
|       | Yi | 01-ai| [github](https://github.com/01-ai/Yi), [6B](https://huggingface.co/01-ai/Yi-6B), [6B-200K](https://huggingface.co/01-ai/Yi-6B-200K) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/llama)|
| bloom |bloom | bigscience | [bloom-560m](https://huggingface.co/bigscience/bloom-560m), [bloomz-560m](https://huggingface.co/bigscience/bloomz-560m) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/bloom) |
| Qwen  |Qwen | é˜¿é‡Œäº‘ | [github](https://github.com/QwenLM/Qwen-7B), [7B](https://huggingface.co/Qwen/Qwen-7B), [7B-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/Qwen) |
| InternLM|InternLM | ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ | [github](https://github.com/InternLM/InternLM), [7B-Chat](https://huggingface.co/internlm/internlm-chat-7b), [7B](https://huggingface.co/internlm/internlm-7b) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/internlm) |
| Falcon|Falcon | tiiuae | [hf](https://huggingface.co/tiiuae), [RW-1B](https://huggingface.co/tiiuae/falcon-rw-1b), [7B](https://huggingface.co/tiiuae/falcon-7b), [7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/falcon) |
| embedding| text2vec-base-chinese |shibing624| [torch](https://huggingface.co/shibing624/text2vec-base-chinese) | |
|          | m3e |moka-ai| [torch](https://huggingface.co/moka-ai) |[config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/embedding/moka-ai@m3e-base/bert4torch_config.json)|
|          | bge |BAAI| [torch](huggingface.co) |[config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/basic/embedding/moka-ai@m3e-base/bert4torch_config.json)|

## äº”. é¸£è°¢

- æ„Ÿè°¢Tongjiliboçš„[bert4torch](https://github.com/Tongjilibo/bert4torch)ï¼Œæœ¬å®ç°é‡ç‚¹å‚è€ƒäº†è¿™ä¸ªé¡¹ç›®ï¼Œè¿›è¡Œäº†ä¼˜åŒ–å’Œæ›´æ–°ï¼›é¡¹ç›®ä¼šæŒç»­è·Ÿè¿›bert4torchçš„æœ€æ–°å®ç°

- æ„Ÿè°¢è‹ç¥å®ç°çš„[bert4keras](https://github.com/bojone/bert4keras)ï¼Œæœ‰äº›åœ°æ–¹å‚è€ƒäº†bert4kerasçš„æºç ï¼Œåœ¨æ­¤è¡·å¿ƒæ„Ÿè°¢å¤§ä½¬çš„æ— ç§å¥‰çŒ®ï¼›å¤§ä½¬çš„ç§‘å­¦ç©ºé—´

  ```bibtex
  @misc{bert4torch,
    title={bert4torch},
    author={Bo Li},
    year={2022},
    howpublished={\url{https://github.com/Tongjilibo/bert4torch}},
  }
  ```

## å…­. å¼•ç”¨

```bibtex
@misc{quickllm,
  title={quickllm},
  author={NLPå°è®²å ‚},
  year={2022},
  howpublished={\url{https://github.com/zysNLP/quickllm}},
}
```

## 6. å…¶ä»–

å…³æ³¨å…¬ä¼—å·ã€ŠNLPå°è®²å ‚ã€‹ï¼Œæ›´å¤šé«˜æ•ˆå†…å®¹åŠæ—¶è®¢é˜…ï¼Œæœ€æ–°æ–‡ç« å’Œ[è§†é¢‘](https://edu.csdn.net/course/detail/39082)åŒæ­¥ï¼Œ[Bç«™å…³æ³¨](https://www.bilibili.com/video/BV1hG411e7Ng/?spm_id_from=333.999.0.0&vd_source=9a2f107418c10b543b13cbd8e1f9e98d)ï¼š

[ã€Šæµ…è°ˆMOEçš„ä»£ç åŸç†ï¼ˆä¸€ï¼‰ï¼Œæ˜¯å¦è¶³å¤Ÿå¯¹æ ‡self-attentionï¼Ÿã€‹](https://mp.weixin.qq.com/s/mbXePBZXIiN3aa8sszPzHQ)å‚è€ƒå€Ÿé‰´ï¼š[Mistral Transformers](https://github.com/mistralai/mistral-src)ï¼Œ[Mixture of Expert](https://github.com/lucidrains/mixture-of-experts.git)

ã€ŠTritonå¤æ‚åˆç®€å•ï¼šæŠŠéƒ¨ç½²åˆ‡æˆåšåšçš„è–„ç‰‡ã€‚ã€‚ã€‹å‚è€ƒå€Ÿé‰´ï¼š[NGC Tritoné•œåƒ](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver)ï¼Œ[Triton Inference Server GitHubå®˜ç½‘](https://github.com/triton-inference-server/server)

ã€ŠTensorRT-LLMï¼šå¤§æ¨¡å‹æ¨ç†åŠ é€Ÿå¿…å¤‡ã€‹å‚è€ƒå€Ÿé‰´[Qwen-TensorRTåŸç†](https://developer.nvidia.com/zh-cn/blog/qwen-model-support-nvidia-tensorrt-llm)ï¼Œ[Qwen-TensorRTä»£ç ](https://github.com/Tlntin/Qwen-TensorRT-LLM/tree/main?tab=readme-ov-file)


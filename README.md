## ä¸€. ç®€ä»‹

LLMå­¦ä¹ èµ„æºåº“ã€‚ä½¿ç”¨pytorchå’Œéƒ¨åˆ†Tensorflow2å®ç°ï¼Œå¯ä»¥ **<u>*æœ¬åœ°è¿è¡Œå’Œè°ƒè¯•*</u>** çš„å¤§æ¨¡å‹LLMç›¸å…³çš„åº”ç”¨

#### **æ ¸å¿ƒåŠŸèƒ½**ï¼š Mixtral-7x8Bã€MOEã€ChatGLM3ã€LLaMa2ã€ BaChuanã€Qwen-TensorRTç­‰

å†æ¬¡å¼ºè°ƒï¼šå¼ºè°ƒæœ¬åœ°è°ƒè¯•ã€ä»£ç è·³è½¬ã€å¿«é€ŸæŒæ¡LLMï¼basic_llm\*å’Œbasic_run\*åˆ†åˆ«æ˜¯è°ƒè¯•å’Œè¿è¡Œæ¨¡å¼

```python
# ä¸‹æ¬¡æ›´æ–°ï¼š1.åŸºäºPyTorchçš„Mistral-8x7Bå’ŒMixtral-8x7B-Instruct-v0.1çš„MOEæ¨¡å‹å¾®è°ƒï¼›2.å„ç±»TensorRT-LLMå’Œæ¨ç†åŠ é€Ÿçš„ç›¸å…³å·¥ä½œ
```

### æœ€æ–°æ›´æ–°ï¼š

- 20240105ï¼šğŸ¯feat(quickllm/projects/*):æ–°å¢åŸºäºNeo4jçŸ¥è¯†å›¾è°±çš„KBQAå¯¹è¯ç³»ç»Ÿé¡¹ç›®
- 20231225ï¼šğŸ¯feat(quickllm/layers/moe_by_transformers.py):æ–°å¢MOE-transformersæ¨¡å—çš„Lossè°ƒè¯•æµç¨‹
- 20231225ï¼šğŸ¯feat(quickllm/layers/moe_by_transformers.py):æ–°å¢transformersåŒ…çš„Mixtral-8x7B / MOEæºä»£ç å’Œè°ƒè¯•è„šæœ¬
- 20231222ï¼šğŸ¯feat(quickllm/layers/multi_query_attention.py):æ·»åŠ è°ƒè¯•transformersåŒ…MQAçš„æ–¹æ³•
- 20231219ï¼šğŸ¯feat(basic_language_model_moe*):MOEæ–°å¢2ç»´å’Œ3ç»´æ•°æ®çš„è®­ç»ƒè¿‡ç¨‹
- 20231218ï¼šğŸ¯feat(quickllm/clients/vllm.py):æ·»åŠ vllmçš„apiè¯·æ±‚æ¨¡å—
- 20231214ï¼šğŸ¯feat(README): å¢åŠ Qwen-TensorRT-LLMçš„é“¾æ¥å’Œè¯´æ˜
- 20231213ï¼šğŸ¯feat(quickllm/clients/triton_client*):æ·»åŠ ChatGLMç³»åˆ—tritonæœåŠ¡åŸºç¡€ä»£ç 

### **è°ƒè¯•æ–¹å¼!!!ï¼š**

â€‹		**æ ¼å¼1. åªæœ‰ä¸€ä¸ªè„šæœ¬æ–‡ä»¶ï¼Œå°†examplesä¸­çš„pyè„šæœ¬å¤åˆ¶åˆ°quickllmæ–‡ä»¶å¤¹çš„åŒçº§ç›®å½•ï¼ˆå½“å‰é¡¹ç›®æ ¹ç›®å½•ï¼‰**

â€‹		**æ ¼å¼2. å¦‚æœè°ƒè¯•è„šæœ¬æ˜¯ä¸ªæ–‡ä»¶å¤¹ï¼Œæ¨èå°†ä¾èµ–æ”¹æˆç›¸å¯¹è·¯å¾„-æ ¼å¼1çš„å½¢å¼ï¼›æˆ–è€…åœ¨éœ€è¦ä½¿ç”¨from quickllmå¯¼åŒ…è°ƒç”¨çš„è„šæœ¬ä»£ç ä¸­æ·»åŠ çˆ¶çº§ç›®å½•**

```python
import sys
sys.path.append('/path/to/directory of quickllm')  # quickllmæ–‡ä»¶å¤¹çš„çˆ¶çº§ç›®å½•
```



## äºŒã€å¿«é€Ÿè°ƒè¯•Mixtral-7x8Båœ¨transformersæ¨¡å—ä¸­çš„MOEæ ¸å¿ƒä»£ç 

Time to debug Mixtral-7x8B-instruct-v0.1 models one line by one line immediately!

```python
# -*- coding: utf-8 -*-
"""
    @Project ï¼šquickllm
    @File    ï¼šbasic_llm_moe_transformers.py
    @Author  ï¼šys
    @Time    ï¼š2023/12/21 18:10
    Mixtral-8x7B æ¨¡å‹ä¸­çš„moeéƒ¨åˆ†ï¼Œä»¥ä¸‹ä»£ç æ¥è‡ªå®˜æ–¹transformersåº“
"""

import torch
torch.manual_seed(123)

from quickllm.layers.moe_by_transformers import MixtralConfig
from quickllm.layers.moe_by_transformers import MixtralSparseMoeBlock

config = MixtralConfig()
moe = MixtralSparseMoeBlock(config)

hidden_states = torch.randn(4, 71, 4096)
hidden_states, router_logits = moe(hidden_states)

print(hidden_states.shape, router_logits.shape)
```



## ä¸‰.å¿«é€Ÿfinetuneæ–¹å¼(ä»¥chatglm2ä¸ºä¾‹)

**å®‰è£…ä¾èµ–ï¼š**

```shell
pip install -r requirements.txt -i https://pypi.douban.com/simple
```

**å¿«é€Ÿå¾®è°ƒ**ï¼š å°†examples/llm/task_chatglm2_lora.pyæ–‡ä»¶è½¬ç§»è‡³æ ¹ç›®å½•ä¸‹ï¼Œæ·»åŠ æ–­ç‚¹ï¼Œå¯åŠ¨è°ƒè¯•ï¼

â€‹	**1. å®šä¹‰configå‚æ•°å’Œé…ç½®ã€åŠ è½½æ•°æ®é›†ï¼ˆå…¶ä»–å‚æ•°åˆ—è¡¨å‚è€ƒç¬¬ä¸‰éƒ¨åˆ†ï¼‰ï¼›**

â€‹	chatglm2å‚æ•°ä¸‹è½½ï¼šhttps://huggingface.co/THUDM/chatglm2-6bï¼›

â€‹	chatglm2æ•°æ®ä¸‹è½½ï¼šhttps://cloud.tsinghua.edu.cn/f/b3f119/a008264b1cabd1/?dl=1

â€‹	**2.ç¼–å†™åŠ è½½promptã€å®šä¹‰æ¨¡å‹ï¼ˆè®­ç»ƒå’ŒéªŒè¯å‡½æ•°ï¼‰**

â€‹	**3.è½½å…¥æ•°æ®ï¼Œå¯åŠ¨è®­ç»ƒå’ŒéªŒè¯ï¼Œè°ƒè¯•ä»£ç ï¼Œè°ƒè¯•å®Œæˆï¼ä¿å­˜ä¿®æ”¹è„šæœ¬åˆ°examplesï¼Œå¤„ç†ä¸‹ä¸€ä¸ª**

**å¿«é€Ÿå¯åŠ¨ï¼š** å°†examples/basic/glm/basic_language_model_chatglm2.pyå¤åˆ¶åˆ°æ ¹ç›®å½•ä¸‹ï¼Œæ·»åŠ æ–­ç‚¹ï¼Œå¯åŠ¨è°ƒè¯•ï¼

```python
# -*- coding: utf-8 -*- 
"""
    @Project ï¼šquickllm 
    @File    ï¼šquickly_start.py
    @Author  ï¼šys
    @Time    ï¼š2023/12/12 12:12
    å®˜æ–¹é¡¹ç›®ï¼šhttps://github.com/THUDM/ChatGLM2-6B
"""

import os
import torch
from loguru import logging
from transformers import AutoTokenizer

from quickllm.models import build_transformer_model
from quickllm.generation import SeqGeneration


class ExpertModel:

    def __init__(self):
        self.prompt = "è¯·ä»¥ä¸€ä½åŒ»ç–—é¢†åŸŸçŸ¥è¯†å›¾è°±ä¸“å®¶çš„è§’è‰²å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š"
        self.choice = 'default'  # default, int4, 32k
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'

        # æ¥è‡ªhttps://huggingface.co/THUDM/chatglm2-6bï¼›
        self.dir_path = "/path/to/my/pretrain_ckpt/glm/chatglm2-6B"
        self.checkpoint_path = [os.path.join(dir_path, i) for i in os.listdir(dir_path) if i.endswith('.bin')]
        # æ¥è‡ªé¡¹ç›®ä¸­çš„ï¼šexamples/basic/glm/chatglm2-6B/quickllm_config.json
        self.config_path = dir_path + '/quickllm_config.json'

    def build_prompt(self, history):
        for query, response in history:
            self.prompt += f"\n\nç”¨æˆ·ï¼š{query}"
            self.prompt += f"\n\nChatGLM-6Bï¼š{response}"
        return self.prompt

    def build_model(self):
        tokenizer = AutoTokenizer.from_pretrained(self.dir_path.replace('/', '\\'), trust_remote_code=True)
        if self.choice in {'default', '32k'}:
            encoder = build_transformer_model(config_path=self.config_path,
                                              checkpoint_path=self.checkpoint_path).half().to(device)
        else:
            encoder = build_transformer_model(config_path=self.config_path,
                                              checkpoint_path=self.checkpoint_path).to(device)

        model = SeqGeneration(encoder, tokenizer, start_id=None, end_id=tokenizer.eos_token_id, mode='random_sample',
                              maxlen=2048, default_rtype='logits', use_states=True)
        return model

    def chat(self, query, history):
        prompt = ""
        for i, (old_query, response) in enumerate(history):
            prompt += "[Round {}]\n\né—®ï¼š{}\n\nç­”ï¼š{}\n".format(i + 1, old_query, response)
        prompt += "[Round {}]\n\né—®ï¼š{}\n\nç­”ï¼š".format(len(history) + 1, query)

        for response in self.build_model().stream_generate(prompt, topk=50, topp=0.7, temperature=0.95):
            new_history = history + [(query, response)]
            yield response, new_history

    def main(self):
        history = []
        logging.info("----æ¬¢è¿ä½¿ç”¨ChatGLM2-6Bæ¨¡å‹ï¼Œä¿®æ”¹promptè¾“å…¥å†…å®¹è¿›è¡Œå¯¹è¯ï¼Œclearæ¸…ç©ºå¯¹è¯å†å²ï¼Œstopç»ˆæ­¢ç¨‹åº")
        while True:
            query = input("\nQuestionï¼š")
            if query.strip() == "stop":
                break
            if query.strip() == "clear":
                history = []
                print("----å·²æ¸…ç©ºå†å²å¯¹è¯----")
                continue
            for response, history in self.chat(query, history=history):
                print(build_prompt(history), flush=True)

            print(build_prompt(history), flush=True)
            torch.cuda.empty_cache()


if __name__ == '__main__':
    expert_bot = ExpertModel()
    expert_bot.main()
```

**å¿«é€Ÿéƒ¨ç½²ï¼š** å°†examples/serving/basic_simple_web_serving_simbert.pyæ–‡ä»¶è½¬ç§»è‡³æ ¹ç›®å½•ä¸‹ï¼Œæ·»åŠ æ–­ç‚¹ï¼Œå¯åŠ¨è°ƒè¯•ï¼



## ä¸‰. é¢„è®­ç»ƒæƒé‡
- è‹¥æ— è¯´æ˜åˆ™ä½¿ç”¨[huggingfaceå®˜ç½‘](https://huggingface.co/models)ä¸­å¯¹åº”æ¨¡å‹çš„`pytorch_model.bin`å’Œå¯¹åº”config.json

  

## å››.é¸£è°¢

- æ„Ÿè°¢Tongjiliboçš„[bert4torch](https://github.com/Tongjilibo/bert4torch)ï¼Œæœ¬å®ç°é‡ç‚¹å‚è€ƒäº†è¿™ä¸ªé¡¹ç›®ï¼Œè¿›è¡Œäº†ä¼˜åŒ–å’Œæ›´æ–°ï¼›é¡¹ç›®ä¼šæŒç»­è·Ÿè¿›bert4torchçš„æœ€æ–°å®ç°

- æ„Ÿè°¢è‹ç¥å®ç°çš„[bert4keras](https://github.com/bojone/bert4keras)ï¼Œæœ‰äº›åœ°æ–¹å‚è€ƒäº†bert4kerasçš„æºç ï¼Œåœ¨æ­¤è¡·å¿ƒæ„Ÿè°¢å¤§ä½¬çš„æ— ç§å¥‰çŒ®ï¼›å¤§ä½¬çš„ç§‘å­¦ç©ºé—´

  ```bibtex
  @misc{bert4torch,
    title={bert4torch},
    author={Bo Li},
    year={2022},
    howpublished={\url{https://github.com/Tongjilibo/bert4torch}},
  }
  ```

## äº”. å¼•ç”¨

```bibtex
@misc{quickllm,
  title={quickllm},
  author={NLPå°è®²å ‚},
  year={2022},
  howpublished={\url{https://github.com/zysNLP/quickllm}},
}
```



## å…­. å…¶ä»–

å…³æ³¨å…¬ä¼—å·ã€ŠNLPå°è®²å ‚ã€‹ï¼Œæ›´å¤šé«˜æ•ˆå†…å®¹åŠæ—¶è®¢é˜…ï¼Œæœ€æ–°æ–‡ç« å’Œ[è§†é¢‘](https://edu.csdn.net/course/detail/39082)åŒæ­¥ï¼Œ[Bç«™å…³æ³¨](https://www.bilibili.com/video/BV1hG411e7Ng/?spm_id_from=333.999.0.0&vd_source=9a2f107418c10b543b13cbd8e1f9e98d)ï¼š

ã€ŠMixtral-8x7B-Instruct-v0.1çš„finetuneå¾®è°ƒå®æˆ˜ã€‹ï¼šå‚è€ƒå€Ÿé‰´[Aurora](https://github.com/WangRongsheng/Aurora)ï¼Œ[Firefly](https://github.com/yangjianxin1/Firefly)

[ã€Šæµ…è°ˆMOEçš„ä»£ç åŸç†ï¼ˆä¸€ï¼‰ï¼Œæ˜¯å¦è¶³å¤Ÿå¯¹æ ‡self-attentionï¼Ÿã€‹](https://mp.weixin.qq.com/s/mbXePBZXIiN3aa8sszPzHQ)å‚è€ƒå€Ÿé‰´ï¼š[Mistral Transformers](https://github.com/mistralai/mistral-src)ï¼Œ[Mixture of Expert](https://github.com/lucidrains/mixture-of-experts.git)

ã€ŠTritonå¤æ‚åˆç®€å•ï¼šæŠŠéƒ¨ç½²åˆ‡æˆåšåšçš„è–„ç‰‡ã€‚ã€‚ã€‹å‚è€ƒå€Ÿé‰´ï¼š[NGC Tritoné•œåƒ](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver)ï¼Œ[Triton Inference Server GitHubå®˜ç½‘](https://github.com/triton-inference-server/server)

ã€ŠTensorRT-LLMï¼šå¤§æ¨¡å‹æ¨ç†åŠ é€Ÿå¿…å¤‡ã€‹å‚è€ƒå€Ÿé‰´ï¼š[Qwen-TensorRTåŸç†](https://developer.nvidia.com/zh-cn/blog/qwen-model-support-nvidia-tensorrt-llm)ï¼Œ[Qwen-TensorRTä»£ç ](https://github.com/Tlntin/Qwen-TensorRT-LLM/tree/main?tab=readme-ov-file)

ã€ŠLORAè®ºæ–‡è§£è¯»ï¼šå¤§æ¨¡å‹çš„ä½ç§©é€‚åº”ã€‹å‚è€ƒå€Ÿé‰´ï¼š[LORAè®ºæ–‡](https://arxiv.org/pdf/2106.09685.pdf)ï¼Œ[LORAè®ºæ–‡è§£è¯»](https://zhuanlan.zhihu.com/p/624576869)
